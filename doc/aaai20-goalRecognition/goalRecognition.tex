\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai20}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}\urlstyle{rm}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{0}

% Addiotional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{calc,backgrounds,positioning,fit}


\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition


\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{mydefinition}[mytheorem]{Definition}

%PDF Info Is Required:
\pdfinfo{}

\title{Goal Recognition as Planning with Unknown Action Models}

%\author{\#}
% Commented for blind submission
\author{Diego Aineto \and Sergio Jim\'enez\and Eva Onaindia\\
{\scriptsize Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\scriptsize Universitat Polit\`ecnica de Val\`encia.}\\
{\scriptsize Camino de Vera s/n. 46022 Valencia, Spain}\\
{\scriptsize \{dieaigar,serjice,onaindia\}@dsic.upv.es}
\And Miquel Ram\'irez\\
{\scriptsize School of Computing and Information Systems}\\
{\scriptsize The University of Melbourne}\\
{\scriptsize Melbourne, Victoria. Australia}\\
{\scriptsize miquel.ramirez@nimelb.edu.au}}


\begin{document}
\maketitle
\begin{abstract}
{\em Plan recognition as planning} assumes that {\em observers} must have correct and complete knowledge of the action model of the observed agents. We relax this assumption formulating a novel setup for classical planning where action models are unknown but the state variables and the action parameters are however known. The experimental results demonstrate that this novel classical planning setup allows us to solve standard goal recognition benchmarks, still using an off-the-shelf classical planner, but without knowing beforehand the precise action model of the observed agents. 
\end{abstract}



\section{Introduction}
\label{sec:introduction}
{\em Goal recognition} is a particular classification task in which each class represents a different goal of the observed agents and classification examples are observations of the agents pursuing one of those goals. While there exist diverse approaches for {\em goal recognition}, {\em plan recognition as planning}~\cite{ramirez2009plan,ramirez2010probabilistic} is one of the most popular and it is currently at the core of various model-based activity recognition tasks such as, {\em goal recognition design}~\cite{KerenGK14}, {\em deceptive planning}~\cite{masters2017deceptive}, {\em planning for transparency}~\cite{macnally2018action} or {\em counter-planning}~\cite{PozancoEFB18}.

{\em Plan recognition as planning} leverages the action model of the observed agents, a single plan observation, and an off-the-shelf classical planner to estimate the most likely goal of the observed agents~\cite{ramirez2012plan}. In this paper we show how to relax the assumption of {\em knowing the action model of the observed agents}, which can become a too strong requirement.  We formulate a novel set up for classical planning where no action model is given (instead, only the state variables and the action parameters are known beforehand) and we show that this formulation neatly fits into the {\em plan recognition as planning} approach. The experimental results demonstrate that we can solve standard goal recognition benchmarks, still using an off-the-shelf classical planner, but without requiring having at hand a model of the {\em preconditions} and {\em effects} of the actions of the observed agents.



\section{Background}
\label{sec:background}
This section formalizes the {\em classical planning} model that we follow in this work, the kind of {\em observations} of plan executions that input the {\em goal recognition} task, and the {\em plan recognition as planning} approach for the {\em goal recognition} task.  

\subsection{Classical planning with conditional effects}
We denote as $F$ ({\em fluents}) the set of  propositional state variables. A partial assignment of values to fluents is represented by $L$ ({\em literals}). To implicitly represent the unobserved literals of a state we adopt the \emph{open world assumption} (i.e. what is not known to be true in a state is {\em unknown}). Hence, a state $s$ includes positive literals ($f$) and negative literals ($\neg f$) and it is defined as a full assignment of values to fluents; $|s|=|F|$. We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents.

A {\em planning action} $a$ comprises a set of preconditions $\pre(a)\in\mathcal{L}(F)$ and a set of effects $\eff(a)\in\mathcal{L}(F)$. The semantics of an action $a$ is specified with two functions: $\rho(s,a)$ denoting whether $a$ is {\em applicable} in a state $s$ and $\theta(s,a)$ denoting the {\em successor state} that results from applying $a$ in $s$. Then $\rho(s,a)$ holds, i.e.~the action preconditions hold in the given state iff $\pre(a)\subseteq s$. The result of executing an applicable action $a$ in a state $s$ is a new state $\theta(s,a)=\{s\setminus \neg\eff(a)\cup\eff(a)\}$, where $\neg\eff(a)$ is the complement of $\eff(a)$, which is subtracted from $s$ to ensure that $\theta(s,a)$ remains a well-defined state. The subset of effects of an action $a$ that assign a positive value to a fluent is called {\em positive effects}, and denoted by $\eff^+(a)\in \eff(a)$, while $\eff^-(a)\in \eff(a)$ denotes the {\em negative effects}.

A planning action $a$ can also define a set of {\em conditional effects} $cond(a)$. Formally a conditional effect $C\rhd E\in cond(a)$ is composed of two sets of literals: the {\em condition}, $C\in\mathcal{L}(F)$, and the {\em effect}, $E\in\mathcal{L}(F)$. The {\em triggered effects} resulting from the action application (conditional effects whose conditions hold in $s$) is defined as $\eff_c(s,a)=\bigcup_{C\rhd E\in\cond(a),C\subseteq s} E$.

A {\em planning problem} is a tuple $P=\tup{F,A,I,G}$, where $A$ is a set of actions, $I$ is the initial state and $G\in\mathcal{L}(F)$ is the set of goal conditions over the state variables. A {\em plan} $\pi$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$, with $|\pi|=n$ denoting its {\em plan length}. The execution of $\pi$ in $I$ induces a {\em trajectory} $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, it holds $\rho(s_{i-1},a_i)$ and $s_i=\theta(s_{i-1},a_i)$. A plan $\pi$ solves $P$ iff the induced trajectory reaches a final state $s_n$ such that $G \subseteq s_n$.

\subsection{The observation model}
Given a planning problem $P=\tup{F,A,I,G}$, a plan $\pi$ that solves $P$, and the corresponding trajectory $\tau$ induced by the execution of $\pi$ in $I$, $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$; there exist as many possible observations of $\tau$ as combinations of observable actions and observable fluents of the states in $\tau$. We refer to the set of all possible combinations of observable elements of $\tau$ as $Obs(\tau)$.

In this work we define {\em an observation} $\mathcal{O}\in Obs(\tau)$ as a sequence of {\em partially observed states}, $\mathcal{O}=\tup{s_0^o,s_1^o \ldots , s_m^o}$, except for the initial state $s_0^o=I$ which is fully observed. A {\em partially observed state} is one in which $|s_i^o| < |F|$, {\small $1\leq i\leq m\leq n$}; i.e., a state in which at least the value of a fluent in $F$ is {\em unknown}. It may be also the case that $|s_i^o| = 0$ when an intermediate state is fully unknown. This observation model allow us to distinguish between {\em observable state variables}, whose value may be read from sensors, and {\em hidden} (or {\em latent}) {\em state variables}, that cannot be observed. 

The observation model can also include {\em observed actions} as fluents indicating the applied action in a given state. This means that a sequence of observed actions $\tup{a_1^o, \ldots, a_l^o}$ is a sub-sequence of $\pi=\tup{a_1, \ldots, a_n}$ such that $a_i^o \in s_{i-1}^o$, {\small $0\leq i \leq l$}. Consequently, the number of fluents that represent observed actions, $l$, can range from $0$ (in a fully unobservable action sequence) to $|\pi|=n$ (in a fully observed action sequence).

Given $\mathcal{O} \in Obs(\tau)$, the number of observed states of $\mathcal{O}=\tup{s_0^o,s_1^o \ldots , s_m^o}$ ranges from 2 (at least the initial and final state) to $|\pi|+1$. The number of fluents of the full observable state $s_0^o$ will be $|F|$, or $|F|+1$ in case the fluent of the applied action in $s_0$ is also observed. Every observable intermediate state will comprise a number of fluents between $[1,|F|+1]$, where a single fluent may represent a sensing fluent of the state or the observation of the applied action.

\subsection{Goal recognition as planning}
{\em Goal recognition} is a specific classification task in which each class represents a different possible goal $G\in G[\cdot]$ (where $G[\cdot]$ is the set of {\em recognizable} goals) and there is a single classification example $\mathcal{O}$ (that represents the observation of a plan execution where agents act to achieve a goal $G\in G[\cdot]$).

According to the {\em naive Bayes classifier}, the {\em solution} to the {\em goal recognition} task is the subset of goals in $G[\cdot]$ that maximizes this expression.
\begin{align}
argmax_{G\in G[\cdot]} P(\mathcal{O}|G) P(G).
\end{align}

The {\em plan recognition as planning} approach for goal recognition shows that the $P(\mathcal{O}|G)$ likelihood can be estimated leveraging the action model of the observed agents and off-the-shelf classical planners~\cite{ramirez2012plan}. Given $P=\tup{F,A,I,G[\cdot]}$ then $P(\mathcal{O}|G)$ is estimated computing, for each goal $G\in G[\cdot]$, the cost difference of the solution plans to two classical planning problems:
\begin{itemize}
\item $P^{\top}_G$, the classical planning problem built constraining $P=\tup{F,A,I,G}$ to achieve $G\in G[\cdot]$ through a plan $\pi^\top$ {\em consistent} with the input observation $\mathcal{O}$.
\item $P^{\bot}_G$, the classical planning problem that constrains solutions of $P=\tup{F,A,I,G}$ to plans $\pi^\bot$, that achieve $G\in G[\cdot]$, but that are {\em inconsistent} with $\mathcal{O}$.
\end{itemize}

The higher the value of the $\Delta(\pi^\top,\pi^\bot)$ cost difference, the higher the probability of the observed agents to aim goal $G\in G[\cdot]$. {\em Plan recognition as planning} uses the {\em sigmoid function} to map the previous cost difference into a likelihood:

\begin{align}
P(\mathcal{O}|G) = \frac{1}{1+e^{-\beta \Delta(\pi^\top,\pi^\bot)}}
\end{align}

This expression is derived from the assumption that while the observed agents are not perfectly rational, they are more likely to follow cheaper plans, according to a {\em Logistic} distribution. The larger the value of $\beta$, the more rational the agents, and the less likely that they will follow suboptimal plans. Recent work on {\em goal recognition} exploit the structure of action {\em preconditions} and {\em effects} to compute fast estimates of the $P(\mathcal{O}|G)$ likelihood~\cite{pereira2017landmark}.

To compute the target probability distribution $P(G|\mathcal{O})$ the $P(\mathcal{O}|G)$ likelihoods can be plugged into the {\em Bayes rule}. In this case the $P(\mathcal{O})$ probabilities are obtained by normalization (goal probabilities must add up to 1 when summed over all possible goals).



\section{Goal recognition as planning with unknown action models}
\label{sec:recognition}
This section formalizes the goal recognition task addressed in this paper, defines a setup for classical planning with unknown action models, and shows how to leverage this setup to solve goal recognition tasks when the action model of the observed agents is unknown.

\subsection{Goal recognition with unknown action models}
We define the task of {\em goal recognition with unknown domain models} as a $\tup{P,\mathcal{O}}$ pair:
\begin{itemize}
\item $P=\tup{F,A[\cdot],I,G[\cdot]}$ is a classical planning problem where $G[\cdot]$ is the set of {\em recognizable} goals and $A[\cdot]$ is a set of actions s.t., for each $a\in A[\cdot]$, the semantics of $a$ is unknown (i.e. the functions $\rho$ and/or $\theta$ of $a$ are undefined).
\item $\mathcal{O}$ is the observation of the execution of an unknown plan $\pi$ to reach goal $G\in G[\cdot]$ starting from the given initial state $I=s_0^o$.
\end{itemize}
The parameters of the actions in $A[\cdot]$ are known. This means that they are either given in $P$ or deducible from $\mathcal{O}$.

The {\em solution} to the {\em goal recognition with unknown domain models} task is again the subset of goals in $G[\cdot]$ that maximizes expression (1). With this regard this new task is defined exactly as the original goal recognition task~\cite{ramirez2012plan} except that the {\em preconditions} and {\em effects} of the input set of {\em actions} are now unknown.

\subsection{Planning with unknown domain models}
Our approach to estimate the $P(\mathcal{O}|G)$ likelihoods is to compute solution plans to the classical planning problems $P^{\top}_G$ and $P^{\bot}_G$, as in the {\em plan recognition as planning} approach for goal recognition. The only difference of our approach is that we solve $P^{\top}_G$ while we build a reasonable model for the actions in $A[\cdot]$ leveraging the available input knowledge $\tup{P,\mathcal{O}}$. To do so, we formulate {\em planning with unknown domain models}, a setup for classical planning where no action model is given. The state variables, action parameters and a single plan observation are known though. This setup is closely related to the learning of planning action models~\cite{SternJ17} since it can be understood as {\em one-shot learning} (i.e. learning from a single example).

A {\em classical planning with unknown domain models} is defined as a tuple $P=\tup{F,A[\cdot],I,G}$, where $A[\cdot]$ is a set of actions s.t., the semantics of each action $a\in A[\cdot]$ is unknown (i.e. the functions $\rho$ and/or $\theta$ of $a$ are undefined).

A {\em solution plan} is a sequence of actions $\pi=\tup{a_1, \ldots, a_n}$ whose execution on $I$ induces a trajectory $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and {\em there exists} at least one possible action model (e.g. one possible definition of the $\rho$ and $\theta$ functions within the given state variables) satisfying the following constraints:
\begin{itemize}
\item For every {\small $1\leq i\leq n$} then $\rho(s_{i-1},a_i)={\tt\small True}$ .
\item For every {\small $1\leq i\leq n$} then $s_i=\theta(s_{i-1},a_i)$.
\item Goals are met at the final state $G\subseteq s_n$. 
\end{itemize}

Further constraints can be defined to build more accurate models for the $\rho(s,a)$ and $\theta(s,a)$ functions of the $A[\cdot]$ actions:
\begin{itemize}
\item {\em Observations}. The states trajectories induced by plans computable with the actions in $A[\cdot]$ must be {\em consistent} with the observed states in $\mathcal{O}$. Observations $\mathcal{O}$ act as ordered {\em landmarks} for the planning problem $P$~\cite{hoffmann2004ordered} since all the fluents of the sets in $\mathcal{O}$ must be achieved by any plan that solves $P$ and in the same order as defined in the observation $\mathcal{O}$.
\item {\em Domain-specific knowledge}. {\em State invariants} is a useful type of state constraints for computing more compact state representations of a given planning problem~\cite{helmert2009concise} and for making {\em satisfiability planning} or {\em backward search} more efficient~\cite{rintanen2014madagascar,alcazar2015reminder}. {\em State invariants} reduce the space of the possible preconditions and effects of the actions in $A[\cdot]$. For instance, in the {\em blocksworld} one can argue that {\small\tt on(?x,?x)} will not appear in the preconditions/effects of an action because, in this specific domain, a block cannot be on top of itself.
\item {\em Partially-specified action models}. In some contexts some portions of the preconditions and effects of the actions in $A[\cdot]$ can be known~\cite{ZhuoNK13,sreedharan2018handling,pereira2018heuristic}.
\end{itemize}

\subsection{Estimating $P(\mathcal{O}|G)$ by planning with unknown action models}
Our formalization of {\em classical planning with unknown domain models} can be used to estimate the $P(\mathcal{O}|G)$ likelihoods following the {\em plan recognition as planning} approach for goal recognition:
\begin{enumerate}
\item Compute an action model $A$ that can solve {\em $P^{\top}_G$}, now the classical planning problem {\em with unknown action model} that constrains solutions of $\tup{F,A[\cdot],I,G}$ to plans $\pi^\top$ {\em consistent} with the input observation $\mathcal{O}$, and take $cost(\pi^\top)$ from that solution.
\item Solve $P^{\bot}_G$, the classical planning problem that constrains $\tup{F,A,I,G}$ to achieve $G\in G[\cdot]$ through a plan $\pi^\bot$ {\em inconsistent} with $\mathcal{O}$, and take $cost(\pi^\bot)$ from that solution.
\item Compute the $\Delta(\pi^\top,\pi^\bot)$ cost difference and plug it into equation (2) to get the $P(\mathcal{O}|G)$ likelihoods.
\end{enumerate}


\begin{figure}
  \begin{footnotesize}
$\forall x_1,x_2\ \neg ontable(x_1)\vee\neg on(x_1,x_2)$.\\
$\forall x_1,x_2\ \neg clear(x_1)\vee\neg on(x_2,x_1)$.\\
$\forall x_1,x_2,x_3\ \neg on(x_1,x_2)\vee\neg on(x_1,x_3)\ such\ that\ x_2\neq x_3$.\\
$\forall x_1,x_2,x_3\ \neg on(x_2,x_1)\vee\neg on(x_3,x_1)\ such\ that\ x_2\neq x_3$.\\
\end{footnotesize}
 \caption{\small {\em Schematic mutexes} for the {\em blocksworld} domain.}
\label{fig:strongest-invariant}
\end{figure}

\subsection{Completing observations with {\em schematic mutexes}}
The estimate of the $P(\mathcal{O}|G)$ likelihoods can be optimizing by adding a pre-processing step:
\begin{itemize}
\item[$0.$]  Complete the input observation $\mathcal{O}$ using the domain-specific knowledge if available.
\end{itemize}

Next we provide the details os this pre-process. A {\em mutex} (mutually exclusive) is a state invariant that takes the form of a binary clause and indicates a pair of different properties that cannot be simultaneously true~\cite{kautz:mutex:IJCAI1999}. For instance in a three-block {\em blocksworld}, $\neg on(block_A,block_B)\vee \neg on(block_A,block_C)$ is a {\em mutex} because $block_A$ can only be on top of a single block.

Recently, some works point at extracting \emph{lifted} invariants, also called {\em schematic} invariants~\cite{rintanen:schematicInvariants:AAAI2017}, that hold for any possible state and any possible set of objects. Invariant
templates obtained by inspecting the lifted representation of the domain have also been exploited for deriving \emph{lifted mutex}~\cite{BernardiniFS18}. In this work we exploit domain-specific knowledge that is given as {\em schematic mutex}. We pay special attention to {\em schematic mutex} because they identify mutually exclusive properties of a given type of objects~\cite{fox:TIM:JAIR1998} and because they enable (1) an effective completion of a partially observed state and (2) an effective pruning of inconsistent \strips\ action models.

We define a schematic mutex as a $\tup{p,q}$ pair of predicates in $\Psi$ (the initial state is a full assignment of values to fluents so the predicates $\Psi$ are extractable from $I$) that satisfy the formulae $\neg p\vee \neg q$, considering that their corresponding parameters are variables universally quantified. For instance, $holding(v_1)$ and $clear(v_1)$ from the {\em blocksworld} are {\em schematic mutex} while $clear(v_1)$ and $ontable(v_1)$ are not because $\forall v_1, \neg clear(v_1)\vee\neg ontable(v_1)$ does not hold for every possible state. Figure~\ref{fig:strongest-invariant} shows an example of four clauses that define schematic mutexes for the {\em blocksworld} domain.

Let $\Omega$ be the set of objects that appear in $F$ as the values of the arguments of the predicates $\Psi$, and $\phi=\tup{p,q}$ a schematic mutex. There exist many possible instantiations of $\phi$ of the type $\tup{p(\omega),q(\omega')}$ with objects of $\Omega$, where $\omega\subseteq\Omega^{|args(p)|}$ and $\omega'\subseteq\Omega^{|args(q)|}$. Let us now assume that the instantiation $p(\omega) \in s_j^o$, {\small $(1\leq j\leq m)$}, being $s_j^o$ a partially observed state of $\mathcal{O}$. Then, two situations may occur: (a) $\neg q(\omega') \in s_j^o$, in which case the expression $\neg p(\omega) \vee \neg q(\omega')$ holds in $s_j^o$; or (b) $\neg q(\omega') \notin s_j^o$, in which case the literal has not been observed in $s_j^o$ and so we can safely complete the state with $\neg q(\omega')$ (the same applies inversely, when $q(\omega') \in s_j^o$ but $\neg p(\omega) \notin s_j^o$). In other words, if we find that one component of a schematic mutex is positively observed in a state and the other component is not observable in such state, we can complete the state with the missing negative literal. For instance, if the literal {\tt\small holding(blockA)} is observed in a particular state and $\Phi$ contains the schematic mutex $\neg holding(v_1)\vee\neg clear(v_1)$, we extend the state observation with literal {\tt\small $\neg$clear(blockA)} (despite this particular literal being initially unknown).



\section{Classical planning with unknown \strips\ models}
\label{sec:planning}
This section shows that, when the unknown precondition and effects of the actions in $A[\cdot]$ follow the \strips\ model, we can solve the task of {\em planning with unknown domain models} (and hence estimate the $P(\mathcal{O}|G)$ likelyhood) with an off-the-shelf classical planner.

\begin{figure}
	\begin{scriptsize}
		\begin{tabular}{c@{\hskip .2in} |@{\hskip .1in} c}
	{\bf Encoding} & {\bf Meaning}\\\hline
$\neg pre_{p,\xi} \wedge \neg eff_{p,\xi} $& $p$ belongs neither to the preconditions nor effects of $\xi$ \\
             & ($p \notin pre(\xi) \wedge p \notin add(\xi) \wedge p \notin del(\xi)$)\\\\
$pre_{p,\xi} \wedge \neg eff_{p,\xi} $& $p$ is only a precondition of $\xi$\\
               &  ($p \in pre(\xi) \wedge p \notin add(\xi) \wedge p \notin del(\xi)$) \\\\
$\neg pre_{p,\xi} \wedge eff_{p,\xi} $& $p$ is a positive effect of $\xi$ \\
               &  ($p \notin pre(\xi) \wedge p \in add(\xi) \wedge p \notin del(\xi)$) \\\\
$pre_{p,\xi} \wedge eff_{p,\xi} $& $p$ is a negative effect of $\xi$ \\
               &  ($p \in pre(\xi) \wedge p \notin add(\xi) \wedge p \in del(\xi)$)
		\end{tabular}
	\end{scriptsize}
	\caption{\small Combinations of the propositional encoding and their meaning}
	\label{fig:combinations}
\end{figure}

\begin{figure}
  \begin{scriptsize}
  \begin{verbatim}
(:action stack
   :parameters (?v1 ?v2)
   :precondition (and (holding ?v1) (clear ?v2))
    :effect (and (not (holding ?v1)) (not (clear ?v2))
                 (clear ?v1) (handempty) (on ?v1 ?v2)))


(pre_holding_v1_stack) (pre_clear_v2_stack)
(eff_holding_v1_stack) (eff_clear_v2_stack)
(eff_clear_v1_stack) (eff_handempty_stack) (eff_on_v1_v2_stack)
  \end{verbatim}
  \end{scriptsize}
 \caption{\small PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema.}
\label{fig:propositional}
\end{figure}


\subsection{A propositional encoding for \strips\ actions models}
A \strips\ \emph{action model} is defined as $\xi=\tup{name(\xi),pars(\xi),pre(\xi),add(\xi),del(\xi)}$, where $name(\xi)$ and parameters, $pars(\xi)$, define the header of $\xi$; and $pre(\xi)$, $del(\xi)$ and $add(\xi)$) are sets of fluents that represent the {\em preconditions}, {\em negative effects} and {\em positive effects}, respectively, of the actions induced from the action model $\xi$.

Let $\Psi$ be the set of {\em predicates} that shape the fluents $F$. The set of propositions that can appear in $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of a given $\xi$, denoted as ${\mathcal I}_{\xi,\Psi}$, are FOL interpretations of $\Psi$ over the parameters $pars(\xi)$.

For instance, in a four-operator {\em blocksworld}~\cite{slaney2001blocks}, the ${\mathcal I}_{\xi,\Psi}$ set contains five elements for the {\small \tt pickup($v_1$)} model, ${\mathcal I}_{pickup,\Psi}$={\small\tt\{handempty, holding($v_1$),clear($v_1$),ontable($v_1$), on($v_1,v_1$)\}} and eleven elements for the model of {\small \tt stack($v_1$,$v_2$)}, ${\mathcal I}_{stack,\Psi}$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$),clear($v_2$),ontable($v_1$),ontable($v_2$), on($v_1,v_1$),on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. 

An action model $\xi$ must be consistent with the \strips\ constraints: $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$. {\em Typing constraints} are also a type of syntactic constraint that reduce the size of ${\mathcal I}_{\xi,\Psi}$~\cite{mcdermott1998pddl}. Considering only these syntactic constraints, the size of the space of possible \strips\ models is given by $2^{2\times|{\mathcal I}_{\Psi,\xi}|}$ because one element in $\mathcal{I}_{\xi,\Psi}$ can appear both in the preconditions and effects of $\xi$. Given $p \in \mathcal{I}_{\Psi,\xi}$, the belonging of $p$ to the preconditions, positive effects or negative effects of $\xi$ is handled with a propositional encoding that uses fluents of two types, $pre_{p,\xi}$ and $eff_{p,\xi}$. The four possible combinations of these two fluents are summarized in Figure \ref{fig:combinations}. 

To illustrate better this encoding, Figure~\ref{fig:propositional} shows the PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema with $pre_{p,stack}$ and $eff_{p,stack}$ fluents ($p\in{\mathcal I}_{\Psi,stack}$).


\subsection{A classical planning compilation for planning with unknown domain models}
The approach for learning \strips\ action models presented in~\cite{aineto2018learning}, which we will use as our baseline learning system (hereafter BLS, for short), is a compilation scheme that transforms the problem of learning the preconditions and effects of action models into a planning task $P'$. A \strips\ \emph{action model} $\xi$ is defined as $\xi=\tup{name(\xi),pars(\xi),pre(\xi),add(\xi),del(\xi)}$, where $name(\xi)$ and parameters, $pars(\xi)$, define the header of $\xi$; and $pre(\xi)$, $del(\xi)$ and $add(\xi)$) are sets of fluents that represent the {\em preconditions}, {\em negative effects} and {\em positive effects}, respectively, of the actions induced from the action model $\xi$.

The BLS receives as input an empty domain model, which only contains the headers of the action models, and a set of observations of plan executions, and creates a propositional encoding of the planning task $P'$. Let $\Psi$ be the set of {\em predicates}\footnote{The initial state of an observation is a full assignment of values to fluents, $|s_0|=|F|$, and so the predicates $\Psi$ are extractable from the observed state $s_0$.} that shape the variables $F$. The set of propositions of $P'$ that can appear in $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of a given $\xi$, denoted as ${\mathcal I}_{\xi,\Psi}$, are FOL interpretations of $\Psi$ over the parameters $pars(\xi)$. For instance, in a four-operator {\em blocksworld}~\cite{slaney2001blocks}, the ${\mathcal I}_{\xi,\Psi}$ set contains five elements for the {\small \tt pickup($v_1$)} model, ${\mathcal I}_{pickup,\Psi}$={\small\tt\{handempty, holding($v_1$),clear($v_1$),ontable($v_1$), on($v_1,v_1$)\}} and eleven elements for the model of {\small \tt stack($v_1$,$v_2$)}, ${\mathcal I}_{stack,\Psi}$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$),clear($v_2$),ontable($v_1$),ontable($v_2$), \\
on($v_1,v_1$),on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. Hence, solving $P'$ consists in determining which elements of ${\mathcal I}_{\xi,\Psi}$ will shape the preconditions, positive and negative effects of each action model $\xi$.

The decision as to whether or not an element of ${\mathcal I}_{\xi,\Psi}$ will be part of $pre(\xi)$, $del(\xi)$ or $add(\xi)$ is given by the plan that solves $P'$. Specifically, two different sets of actions are included in the definition of $P'$: \emph{insert actions}, which insert preconditions and effects on an action model; and \emph{apply actions}, which validate the application of the learned action models in the input observations. Roughly speaking, in the \emph{blocksworld} domain, the insert actions of a plan that solves $P'$ will look like {\tt{\footnotesize(insert\_pre\_stack\_holding\_v1)}},\\
{\tt{\footnotesize(insert\_eff\_stack\_clear\_v1),\\
(insert\_eff\_stack\_clear\_v2)}}, where the second action denotes a positive effect and the third one a negative effect both to be inserted in the model of {\tt{\small stack}}; and the second set of actions of the plan that solves $P'$ will be like {\tt{\small (apply\_unstack blockB blockA),(validate\_1),(apply\_putdown blockB),(validate\_2)}}, where the {\tt {\small validate}} actions denote the points at which the states generated through the {\tt {\small apply}} actions must be validated with the observations of plan executions.

In a nutshell, the output of the BLS compilation is a plan that completes the empty input domain model by specifying the preconditions and effects of each action model such that the validation of the completed model over the input observations is successful.

\subsection{Observations}
This require the compilation to include actions for {\em validating} partially observed states $s_j^o\in\mathcal{O}$. These actions are also part of the postfix of the solution plan $\pi_\Lambda$ and they are aimed at checking that the observation $\mathcal{O}$ follows after the execution of the apply actions.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{j}})=&s_j^o\cup\{test_{j-1}\},\\
\cond(\mathsf{validate_{j}})=&\{\emptyset\}\rhd\{\neg test_{j-1}, test_j\}.
\end{align*}
\end{small}

There will be a validate action in $\pi_\Lambda$ for every observed state in $\mathcal{O}$. The position of the validate actions in $\pi_\Lambda$ will be determined by the planner by checking that the state resulting after the execution of an apply action comprises the observed state $s_j^o\in\mathcal{O}$.


\begin{figure}
	\begin{footnotesize}
		\begin{tabular}{lll}
			{\bf ID} & {\bf Action} & {\bf New conditional effect}\\\hline
			1&${\tt (insert\_pre)_{\xi,p}}$&$\{pre\_\xi\_q\}\rhd\{invalid\}$\\
			2&${\tt (insert\_eff)_{\xi,p}}$&$\{pre\_\xi\_q\wedge eff\_\xi\_q\wedge pre\_\xi\_p\}\rhd\{invalid\}$\\
			3&${\tt (insert\_eff)_{\xi,p}}$&$\{\neg pre\_\xi\_q\wedge eff\_\xi\_q\wedge \neg pre\_\xi\_p\}\rhd\{invalid\}$\\
			4&${\tt (apply)_{\xi,\omega}}$&$\{\neg pre\_\xi\_p \wedge eff\_\xi\_p \wedge $\\
			&&$q(\omega)\wedge \neg pre\_\xi\_q\}\rhd\{invalid\}$\\
			5&${\tt (apply)_{\xi,\omega}}$&$\{\neg pre\_\xi\_p \wedge eff\_\xi\_p \wedge $\\
			&&$q(\omega)\wedge \neg eff\_\xi\_q\}\rhd\{invalid\}$
		\end{tabular}
	\end{footnotesize}
	\caption{\small Summary of the new conditional effects added to the classical planning compilation for the learning of \strips\ action models.}
	\label{fig:ceffects}
\end{figure}


\subsection{Schematic mutexes}
We could extend the classical planning compilation for the learning of \strips\ action models~\cite{aineto2018learning} to check the consistency of the {\em state-constraints} in $\Phi$ at every state traversed by a solution to the compiled problem. Unfortunately, checking arbitrary $\phi$ formulae is too expensive for current classical planners.

Instead, our approach is to define a mechanism to check {\em state-constraints} in the form of {\em schematic mutex}. To implement this checking mechanism we add new conditional effects to the {\em insert} and {\em apply} actions of the classical planning compilation. Figure~\ref{fig:ceffects} summarizes the new conditional effects added to the compilation and next, we describe them in detail:

Our approach to learning action models consistent with the schematic mutexes in $\Phi$ is to ensure that newly generated states induced by the learned actions do not introduce any inconsistency. This is implemented by adding new conditional effects to the ${\tt \small insert}$ and ${\tt\small apply}$ actions of the BLS compilation. Figure~\ref{fig:ceffects} summarizes the new conditional effects added to the compilation and next, we describe them in detail:

\begin{enumerate}
	\item[1-3] For every schematic mutex $\tup{p,q}$, where both $p$ and $q$ belong to ${\mathcal I}_{\xi,\Psi}$, one conditional effect is added to the ${\tt \small (insert\_pre)_{\xi,p}}$ actions to prevent the insertion of two preconditions that are schematic mutex. Likewise, two conditional effects are added to the ${\tt \small (insert\_eff)_{\xi,p}}$ actions, one to prevent the insertion of two positive effects that are schematic mutex and another one to prevent two mutex negative effects.
	\item[4-5] For every schematic mutex $\tup{p,q}$, where both $p$ and $q$ belong to ${\mathcal I}_{\xi,\Psi}$, two conditional effects are added to the ${\tt \small (apply)_{\xi,\omega}}$ actions to prevent positive effects that are inconsistent with an input observation (in ${\tt \small (apply)_{\xi,\omega}}$ actions the variables in $pars(\xi)$ are bounded to the objects in $\omega$ that appear in the same position).
\end{enumerate}

In theory, conditional effects of the type $4$-$5$ are sufficient to guarantee that all the states traversed by a plan produced by the compilation are {\em consistent} with the input set of schematic mutexes $\Phi$ (obviously provided that the input initial state $s_0^o$ is a valid state). In practice we include also conditional effects of the type $1$-$3$ because they prune {\em invalid} action models at an earlier stage of the planning process (these effects extend the ${\tt \small insert}$ actions that always appear first in the solution plans).

The goals of the planning task $P'$ generated by the original BLS compilation are extended with the $\neg invalid$ literal to validate that only states consistent with the state constraints defined in $\Phi$ are traversed by solution plans. Remarkably, the $\neg invalid$ literal allows us also to define ${\tt \small (apply)_{\xi,\omega}}$ actions more compactly than in the original compilation. Disjunctions are no longer required to code the possible preconditions of an action schema since they can now be encoded with conditional effects of the type $\{pre\_\xi\_p\wedge \neg p(\omega)\}\rhd\{invalid\}$.


\subsection{Partially specified action models}
Our compilation approach is also flexible to this particular scenario. The known preconditions and effects are encoded setting the corresponding fluents $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ to true in the initial state. Further, the corresponding insert actions, $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertEff_{p,\xi}}$, become unnecessary and are removed from $A_{\Lambda}$, making the classical planning task $P_{\Lambda}$ easier to be solved.

For example, suppose that the preconditions of the {\em blocksworld} action schema {\tt stack} are known, then the initial state $I$ is extended with literals, {\small\tt(pre\_holding\_v1\_stack)} and {\small\tt(pre\_clear\_v2\_stack)} and the associated actions $\mathsf{insertPre_{holding_v1,stack}}$ and $\mathsf{insertPre_{clear_v2,stack}}$ can be safely removed from the $A_{\Lambda}$ action set without altering the {\em soundness} and {\em completeness} of the $P_{\Lambda}$ compilation.
















\section{Evaluation}
\label{sec:evaluation}

\section{Related Work}
\label{sec:evaluation}
The problem of {\em classical planning with unknown domain models} has been previously addressed~\cite{SternJ17}. In this work we evidence the relevance of this task for addressing {\em goal recognition} when the action model of the observed agent is not available.   

The paper also showed that {\em goal recognition}, when the domain model is unknown, is closely related to the learning of planning action models. With this regard, the classical planning compilation for learning \strips\ action models~\cite{aineto2018learning} is very appealing because it allows to produce a \strips\ action model from minimal input knowledge (a single initial state and goals pair), and to refine this model if more input knowledge is available (e.g. observation constraints). Most of the existing approaches for learning action models aim maximizing an statistical consistency of the learned model with respect to the input observations so require large amounts of input knowledge and do not produce action models that are guaranteed to be {\em logically consistent} with the given input knowledge.

Our approach for {\em planning with an unknown domain model} is related to {\em goal recognition design}~\cite{KerenGK14}. The reason is that we are encoding the space of propositional schemes as state variables of the planning problem (the initial state encodes the {\em empty} action model with no preconditions and no effects) and provide actions to modify the value of this state variables as in {\em goal recognition design}. The aims of {\em goal recognition design} are however different. {\em Goal recognition design} applied to {\em goal recognition with unknown domain models} would compute the action model, in the space of possible models, that allows to reveal any of the possible goals as early as possible.



\section{Conclusions}
\label{sec:conclusions}
Classical planners tend to preffer shorter solution plans, so our compilation may introduce a bias to $P=\tup{F,A[\cdot],I,G}$ problems preferring solutions that are referred to action models with a shorter number of {\em preconditions}/{\em effects}. In more detail, all $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ fluents are false at the initial state of our $P'=\tup{F',A',I,G}$ compilation so classical planners tend to solve $P'$ with plans that require a shorter number of {\em insert} actions.

This bias could be eliminated defining a cost function for the actions in $A'$ (e.g. {\em insert} actions has {\em zero cost} while $\mathsf{apply_{\xi,\omega}}$ actions has a {\em positive constant cost}). In practice we use a different approach to disregard the cost of {\em insert} actions because classical planners are not proficiency optimizing {\em plan cost} with zero-cost actions. Instead, our approach is to use a SAT-based planner~\cite{rintanen2014madagascar} because it can apply all actions for inserting preconditions in a single planning step (these actions do not interact). Further, the actions for inserting action effects are also applied in a single planning step so the plan horizon for programming any action model is always bound to 2, which significantly reduces the planning horizon.

Our compilation for {\em planning with unknown domain models} can then be understood as an extension of the SATPLAN approach for classical planning~\cite{kautz1992planning} with two additional initial layers: a first layer for inserting the action preconditions and a second one for inserting the action effects. These two extra layers are followed by the typical $N$ layers of the SATPLAN encoding (extended however to apply the action models that are determined by the previous two initial layers, the $\mathsf{apply_{\xi,\omega}}$ actions). Regarding again the example of Figure~\ref{fig:plan-lplan}, this means that steps [00-04] are applied in paralel in the first SATPLAN layer, steps [05-13] are applied in paralel in the second layer and each step [14-17] is applied sequentially and correponds to a differerent SATPLAN layer (so just six layers are necesary to compute the example plan of Figure~\ref{fig:plan-lplan}).

The SAT-based planning approach is also convenient for the task of {\em goal recognition as planning with unknown domain models} because its ability to deal with classical planning problems populated with dead-ends and because symmetries in the insertion of preconditions/effects into an action model do not affect to the planning performance.


\subsection*{Acknowledgments}
This work is supported by the Spanish MINECO project TIN2017-88476-C2-1-R. D. Aineto is partially supported by the {\it FPU16/03184} and S. Jim\'enez by the {\it RYC15/18009}. M. Ram\'irez research is partially funded by DST Group Joint \& Operations Analysis Division.

\bibliography{planlearnbibliography}
\bibliographystyle{aaai}

\end{document}
