\def\year{2020}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai20}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}\urlstyle{rm}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{0}

% Addiotional packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{calc,backgrounds,positioning,fit}


\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition


\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{mydefinition}[mytheorem]{Definition}


%PDF Info Is Required:
\pdfinfo{}


\title{Goal Recognition as Planning with Unknown Domain Models}


%\author{\#}
% Commented for blind submission
\author{Diego Aineto \and Sergio Jim\'enez\and Eva Onaindia\\
{\scriptsize Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\scriptsize Universitat Polit\`ecnica de Val\`encia.}\\
{\scriptsize Camino de Vera s/n. 46022 Valencia, Spain}\\
{\scriptsize \{dieaigar,serjice,onaindia\}@dsic.upv.es}
\And Miquel Ram\'irez\\
{\scriptsize School of Computing and Information Systems}\\
{\scriptsize The University of Melbourne}\\
{\scriptsize Melbourne, Victoria. Australia}\\
{\scriptsize miquel.ramirez@nimelb.edu.au}}




\begin{document}
\maketitle
\begin{abstract}
The {\em plan recognition as planning} approach assumes that {\em observers} must have both correct and complete knowledge of the action model of the observed agents. This paper shows that this assumption can be relaxed formulating a novel setup for classical planning where action models are unknown but the state variables, action parameters and a single plan observation are however known. The experimental results demonstrate that this novel classical planning setup allows us to solve standard goal recognition benchmarks, still using an off-the-shelf classical planner, but without knowing beforehand the precise action model of the observed agents. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}
{\em Goal recognition} is a particular classification task in which each class represents a different goal for the observed agents and classification examples are observations of the agents pursuing one of those goals. While there exist diverse approaches for {\em goal recognition}, {\em plan recognition as planning}~\cite{ramirez2009plan,ramirez2010probabilistic} is one of the most popular and it is currently at the core of various model-based activity recognition tasks such as, {\em goal recognition design}~\cite{KerenGK14}, {\em deceptive planning}~\cite{masters2017deceptive}, {\em planning for transparency}~\cite{macnally2018action} or {\em counter-planning}~\cite{PozancoEFB18}.

{\em Plan recognition as planning} leverages the action model of the observed agents, a single plan observation, and an off-the-shelf classical planner to estimate the most likely goal of the observed agents~\cite{ramirez2012plan}. In this paper we show how to relax the assumption of {\em knowing the action model of the observed agents}, which can become a too strong requirement when applying {\em plan recognition as planning} on real-world problems.  We formulate a novel set up for classical planning where no action model is given (instead, only the state variables, a single plan observation and the action parameters are known beforehand) and we show that this formulation neatly fits into the {\em plan recognition as planning} approach. The experimental results demonstrate that we can solve standard goal recognition benchmarks, still using an off-the-shelf classical planner, but without requiring having at hand a model of the {\em preconditions} and {\em effects} of the actions of the observed agents 



\section{Background}
\label{sec:background}
This section formalizes the {\em classical planning} model that we follow in this work, the kind of {\em observations} that input the {\em goal recognition} task, and the {\em plan recognition as planning} approach for {\em goal recognition}.  

\subsection{Classical planning with conditional effects}
We denote as $F$ ({\em fluents}) the set of  propositional state variables. A partial assignment of values to fluents is represented by $L$ ({\em literals}). We adopt the \emph{open world assumption} (what is not known to be true in a state is unknown) to implicitly represent the unobserved literals of a state. Hence, a state $s$ includes positive literals ($f$) and negative literals ($\neg f$) and it is defined as a full assignment of values to fluents; $|s|=|F|$. We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents.

A {\em planning action} $a$ comprises a set of preconditions $\pre(a)\in\mathcal{L}(F)$ and a set of effects $\eff(a)\in\mathcal{L}(F)$. The semantics of an action $a$ is specified with two functions: $\rho(s,a)$ denoting whether $a$ is {\em applicable} in a state $s$ and $\theta(s,a)$ denoting the {\em successor state} that results from applying $a$ in a state $s$. Then, $\rho(s,a)$ holds iff $\pre(a)\subseteq s$, i.e.~ the action preconditions hold in the given state. The result of executing an applicable action $a$ in a state $s$ is a new state $\theta(s,a)=\{s\setminus \neg\eff(a)\cup\eff(a)\}$, where $\neg\eff(a)$ is the complement of $\eff(a)$, which is subtracted from $s$ so as to ensure that $\theta(s,a)$ remains a well-defined state. The subset of effects of an action $a$ that assign a positive value to a fluent is called {\em positive effects} and denoted by $\eff^+(a)\in \eff(a)$ while $\eff^-(a)\in \eff(a)$ denotes the {\em negative effects}.

Planning actions can also define {\em conditional effects}. Formally the conditional effects of an action $a_c$ is composed of two sets of literals: $C\in\mathcal{L}(F)$, the {\em condition}, and $E\in\mathcal{L}(F)$, the {\em effect}. The {\em triggered effects} resulting from the action application (conditional effects whose conditions hold in $s$) is defined as $\eff_c(s,a)=\bigcup_{C\rhd E\in\cond(a_c),C\subseteq s} E$.

A {\em planning problem} is a tuple $P=\tup{F,A,I,G}$, where $A$ is a set of actions, $I$ is the initial state and $G\in\mathcal{L}(F)$ is the set of goal conditions over the state variables. A {\em plan} $\pi$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$, with $|\pi|=n$ denoting its {\em plan length}. The execution of $\pi$ in $I$ induces a {\em trajectory} $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, it holds $\rho(s_{i-1},a_i)$ and $s_i=\theta(s_{i-1},a_i)$. A plan $\pi$ solves $P$ iff the induced trajectory reaches a final state $s_n$ such that $G \subseteq s_n$.


\subsection{The observation model}
Given a planning problem $P=\tup{F,A,I,G}$, a plan $\pi$ that solves $P$, and the corresponding trajectory $\tau$ induced by the execution of $\pi$ in $I$, $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$; there exist as many observations of $\tau$ as combinations of observable actions and observable fluents of the states of $\tau$. We refer to the set of all possible combinations of observable elements of $\tau$ as $Obs(\tau)$.

We define {\em an observation} $\mathcal{O}\in Obs(\tau)$ as a sequence of possibly {\em partially observable states}, $\mathcal{O}=\tup{s_0^o,s_1^o \ldots , s_m^o}$, except for the initial state $s_0^o=I$ which is fully observable. A {\em partially observable state} is one in which $|s_i^o| < |F|$, {\small $1\leq i\leq m\leq n$}; i.e., a state in which at least a fluent of $F$ is not observable. It may be also the case that $|s_i^o| = 0$ when an intermediate state is fully unobservable. 

The observation model can also include {\em observed actions} as fluents indicating the applied action in a given state. This means that a sequence of observed actions $\tup{a_1^o, \ldots, a_l^o}$ is a sub-sequence of $\pi=\tup{a_1, \ldots, a_n}$ such that $a_i^o \in s_{i-1}^o$, {\small $0\leq i \leq l$}. Consequently, the number of fluents that represent observed actions, $l$, can range from $0$ (in a fully unobservable action sequence) to $|\pi|=n$ (in a fully observed action sequence).

Given $\mathcal{O} \in Obs(\tau)$, the number of observed states of $\mathcal{O}=\tup{s_0^o,s_1^o \ldots , s_m^o}$ ranges from 2 (at least the initial and final state, as explained above) to $|\pi|+1$. The number of fluents of the full observable state $s_0^o$ will be $|F|$, or $|F|+1$ in case the fluent of the applied action in $s_0$ is also observed. Every observable intermediate state will comprise a number of fluents between $[1,|F|+1]$, where a single fluent may represent a sensing fluent of the state or the observation of the applied action.

This observation model can distinguish between {\em observable state variables}, whose value may be read from sensors, and {\em hidden} (or {\em latent}) {\em state variables}, that cannot be observed. Given a subset of fluents $\Gamma\subseteq F$ we say that $\mathcal{O}$ is a $\Gamma$-observation of the execution of $\pi$ on $P$ iff for every observed state $s_i^o$, ${\small 1\leq i\leq m}$, $s_i^o$ only contains fluents in $\Gamma$.


\subsection{Goal recognition with classical planning}
{\em Goal recognition} is a specific classification task in which each class represents a different possible goal $G\in G[\cdot]$ (where $G[\cdot]$ represents the full set of {\em recognizable} goals) and there is a single classification example $\mathcal{O}(\tau)$ (that represents the observation of a plan where agents act to achieve a goal $G\in G[\cdot]$).

Following the {\em naive Bayes classifier}, the {\em solution} to the {\em goal recognition} task is the subset of goals in $G[\cdot]$ that maximizes this expression.
\begin{align}
argmax_{G\in G[\cdot]} P(\mathcal{O}|G) P(G).
\end{align}

The {\em plan recognition as planning} approach shows that the $P(\mathcal{O}|G)$ likelihood can be estimated leveraging the action model of the observed agents and an off-the-shelf classical planner~\cite{ramirez2012plan}. Given $P=\tup{F,A,I,G[\cdot]}$ then $P(\mathcal{O}|G)$ is estimated computing, for each goal $G\in G[\cdot]$, the cost difference of the solution plans to two classical planning problems:
\begin{itemize}
\item $P^{\top}_G$, the classical planning problem built constraining $P=\tup{F,A,I,G}$ to achieve the particular goal $G\in G[\cdot]$ through a plan $\pi^\top$ that is {\em consistent} with the input observation $\mathcal{O}$.
\item $P^{\bot}_G$, the classical planning problem that constrains solutions of $P=\tup{F,A,I,G}$ to plans $\pi^\bot$, that achieve $G\in G[\cdot]$, but that are {\em inconsistent} with $\mathcal{O}$.
\end{itemize}

The higher the value of the $\Delta(\pi^\top,\pi^\bot)$ cost difference, the higher the probability of the observed agents to aim goal $G\in G[\cdot]$. {\em Plan recognition as planning} uses the {\em sigmoid function} to map the previous cost difference into a likelihood:

\begin{align}
P(\mathcal{O}|G) = \frac{1}{1+e^{-\beta \Delta(\pi^\top,\pi^\bot)}}
\end{align}

This expression is derived from the assumption that while the observed agents are not perfectly rational, they are more likely to follow cheaper plans, according to a {\em Logistic} distribution. The larger the value of $\beta$, the more rational the agents, and the less likely that they will follow suboptimal plans. Recent work on {\em goal recognition} exploit the structure of action {\em preconditions} and {\em effects} to compute fast estimates of the $P(\mathcal{O}|G)$ likelihood~\cite{pereira2017landmark}.


\section{Goal recognition as planning with unknown domain models}
\label{sec:recognition}

We define the task of {\em goal recognition with unknown domain models} as a $\tup{P,\mathcal{O}}$ pair, where:
\begin{itemize}
\item $P=\tup{F,A[\cdot],I,G[\cdot]}$ is a classical planning problem where $G[\cdot]$ is the set of {\em recognizable} goals and $A[\cdot]$ is a set of actions s.t., for each $a\in A[\cdot]$, the semantics of $a$ is unknown (i.e. the functions $\rho$ and/or $\theta$ of $a$ are undefined).
\item $\mathcal{O}$ is the observation of the execution of an unknown plan $\pi$ that reaches the goals $G\in G[\cdot]$ starting from the initial state $I$ in $P$.
\end{itemize}

The {\em solution} to the {\em goal recognition with unknown domain models} task is again the subset of goals in $G[\cdot]$ that maximizes expression (1). 

\subsection{Estimating the $P(\mathcal{O}|G)$ likelyhood with unknown domain models}
To build an estimate of the $P(\mathcal{O}|G)$ likelyhood our mechanism matches the {\em plan recognition as planning} approach~\cite{ramirez2012plan} except that we compute $cost(\pi^\top)$ using our compilation for {\em classical planning with unknown domain models}.

In more detail, we build the estimate of the $P(\mathcal{O}|G)$ likelyhood following these four steps:
\begin{enumerate}
\item {\em Build $P^{\top}_G$}, the classical planning problem that constrains solutions of the problem $P=\tup{F,A[\cdot],s_0^o,G}$ to plans $\pi^\top$ {\em consistent} with the input observation $\mathcal{O}(\tau)$. Note that $s_0^o\in \mathcal{O}(\tau)$ is the initial state in the given observation.
\item {\em Solve $P^{\top}_G$}, using the proposed compilation for {\em classical planning with unknown domain models}. Extract from this solution (1), $cost(\pi^\top)$ (by counting the number of $\mathsf{apply_{\xi,\omega}}$ actions in the solution) but also (2), the action model $A$ that is determined by the {\em insert} actions used in $\pi^\top$ to achieve the goals $G$.
\item {\em Build $P^{\bot}_G$}, the classical planning problem that constrains $P=\tup{F,A,s_0^o,G}$ to achieve $G\in G[\cdot]$ through a plan $\pi^\bot$ {\em inconsistent} with $\mathcal{O}(\tau)$ (where $A$ is the set of actions extracted in step 2.).
\item {\em Solve $P^{\bot}_G$} with a classical planner and extract $cost(\pi_\bot)$ as the length of the found solution plan.
\item Compute the $\Delta(\pi^\top,\pi^\bot)$ cost difference and plug it into equation (2) to get the $P(\mathcal{O}|G)$ likelihoods.
\end{enumerate}

To compute the target probability distribution $P(G|\mathcal{O})$ plug the $P(\mathcal{O}|G)$ likelihoods into the {\em Bayes rule} from which the goal posterior probabilities are obtained. In this case the $P(\mathcal{O})$ probabilities are obtained by normalization (goal probabilities must add up to 1 when summed over all possible goals).





\section{Planning with unknown domain models}
\label{sec:planning}
This section formulates {\em planning with unknown domain models}, a novel setup for classical planning where no action model is given. The state variables, action parameters and a single plan observation are known though. This setup is closely related to the learning of planning action models~\cite{SternJ17} and can be seen as an extreme learning task where action models must be {\em learned} from a single example that contain only two state observations: the initial state and the goals.

A {\em classical planning with unknown domain models} is defined as a tuple $P=\tup{F,A[\cdot],I,G}$, where $A[\cdot]$ is a set of actions s.t., the semantics of each action $a\in A[\cdot]$ is unknown (i.e. the functions $\rho$ and/or $\theta$ of $a$ are undefined). A {\em solution plan} is a sequence of actions $\pi=\tup{a_1, \ldots, a_n}$ whose execution on $I$ induces a trajectory $\tau=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and {\em there exists} at least one possible action model (e.g. one possible definition of the $\rho$ and $\theta$ functions within the given state variables) satisfying:
\begin{itemize}
\item For every {\small $1\leq i\leq n$} then $\rho(s_{i-1},a_i)={\tt\small True}$ .
\item For every {\small $1\leq i\leq n$} then $s_i=\theta(s_{i-1},a_i)$.
\item The reached final state $s_n$ meets the goals $G \subseteq s_n$. 
\end{itemize}

Next we show that a propositional encoding for the space of STRIPS action models allows us to solve $P=\tup{F,A,I,G[\cdot]}$ problems with off-the-shelf classical planners.


\subsection{A propositional encoding for STRIPS action}
A \strips\ \emph{action model} is defined as $\xi=\tup{name(\xi),pars(\xi),pre(\xi),add(\xi),del(\xi)}$, where $name(\xi)$ and parameters, $pars(\xi)$, define the header of $\xi$; and $pre(\xi)$, $del(\xi)$ and $add(\xi)$) are sets of fluents that represent the {\em preconditions}, {\em negative effects} and {\em positive effects}, respectively, of the actions induced from the action model $\xi$.

Let $\Psi$ be the set of {\em predicates} that shape the fluents $F$ (the initial state of an observation is a full assignment of values to fluents, $|s_0^o|=|F|$, and so the predicates $\Psi$ are extractable from the observed state $s_0^o$). The set of propositions that can appear in $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of a given $\xi$, denoted as ${\mathcal I}_{\xi,\Psi}$, are FOL interpretations of $\Psi$ over the parameters $pars(\xi)$. For instance, in a four-operator {\em blocksworld}~\cite{slaney2001blocks}, the ${\mathcal I}_{\xi,\Psi}$ set contains five elements for the {\small \tt pickup($v_1$)} model, ${\mathcal I}_{pickup,\Psi}$={\small\tt\{handempty, holding($v_1$),clear($v_1$),ontable($v_1$), on($v_1,v_1$)\}} and eleven elements for the model of {\small \tt stack($v_1$,$v_2$)}, ${\mathcal I}_{stack,\Psi}$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$),clear($v_2$),ontable($v_1$),ontable($v_2$), on($v_1,v_1$),on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. Hence, solving a $\Lambda=\tup{\mathcal{M},{\mathcal O}}$ learning task is determining which elements of ${\mathcal I}_{\xi,\Psi}$ will shape the preconditions, positive and negative effects of the corresponding action model.

A valid actoin model $\xi$ must be consistent with the \strips\ constraints: $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$. {\em Typing constraints} are also a type of syntactic constraint that reduce the size of ${\mathcal I}_{\xi,\Psi}$~\cite{mcdermott1998pddl}. Considering only these syntactic constraints, the size of the space of possible \strips\ models is given by $2^{2\times|{\mathcal I}_{\Psi,\xi}|}$ because one element in $\mathcal{I}_{\xi,\Psi}$ can appear both in the preconditions and effects of $\xi$. Given $p \in \mathcal{I}_{\Psi,\xi}$, the belonging of $p$ to the preconditions, positive effects or negative effects of $\xi$ is handled with a propositional encoding that uses fluents of two types, $pre_{p,\xi}$ and $eff_{p,\xi}$. The four possible combinations of these two fluents are summarized in Figure \ref{fig:combinations}. 

\begin{figure}
	\begin{scriptsize}
		\begin{tabular}{c@{\hskip .2in} |@{\hskip .1in} c}
	{\bf Encoding} & {\bf Meaning}\\\hline
$\neg pre_{p,\xi} \wedge \neg eff_{p,\xi} $& $p$ belongs neither to the preconditions nor effects of $\xi$ \\
             & ($p \notin pre(\xi) \wedge p \notin add(\xi) \wedge p \notin del(\xi)$)\\\\
$pre_{p,\xi} \wedge \neg eff_{p,\xi} $& $p$ is only a precondition of $\xi$\\
               &  ($p \in pre(\xi) \wedge p \notin add(\xi) \wedge p \notin del(\xi)$) \\\\
$\neg pre_{p,\xi} \wedge eff_{p,\xi} $& $p$ is a positive effect of $\xi$ \\
               &  ($p \notin pre(\xi) \wedge p \in add(\xi) \wedge p \notin del(\xi)$) \\\\
$pre_{p,\xi} \wedge eff_{p,\xi} $& $p$ is a negative effect of $\xi$ \\
               &  ($p \in pre(\xi) \wedge p \notin add(\xi) \wedge p \in del(\xi)$)
		\end{tabular}
	\end{scriptsize}
	\caption{\small Combinations of the propositional encoding and their meaning}
	\label{fig:combinations}
\end{figure}

To illustrate better this encoding, Figure~\ref{fig:propositional} shows the PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema with $pre_{p,stack}$ and $eff_{p,stack}$ fluents ($p\in{\mathcal I}_{\Psi,stack}$).

\begin{figure}
  \begin{scriptsize}
  \begin{verbatim}
(:action stack
   :parameters (?v1 ?v2)
   :precondition (and (holding ?v1) (clear ?v2))
    :effect (and (not (holding ?v1)) (not (clear ?v2))
                 (clear ?v1) (handempty) (on ?v1 ?v2)))


(pre_holding_v1_stack) (pre_clear_v2_stack)
(eff_holding_v1_stack) (eff_clear_v2_stack)
(eff_clear_v1_stack) (eff_handempty_stack) (eff_on_v1_v2_stack)
  \end{verbatim}
  \end{scriptsize}
 \caption{\small PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema.}
\label{fig:propositional}
\end{figure}

\subsection{A classical planning compilation for planning with unknown domain models}
Inspired by the {\em classical planning compilation for learning \strips\ action models}~\cite{aineto2018learning}, we define here a compilation to address the task of {\em planning with unknown domain models} that uses our compact propositional encoding of STRIPS action models.

Given a classical planning problem with unknown domain models $P=\tup{F,A[\cdot],I,G}$ we create a classical planning problem $P'=\tup{F',A',I,G}$ such that:
\begin{itemize}
\item $F'$ extends $F$ with a fluent $mode_{insert}$, to indicate whether action models are being {\em programmed} or {\em applied}, and the $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ fluents for the propositional encoding of the corresponding space of STRIPS action models. 

\item $A'$ replaces the actions in $A$ with two types of actions.
\begin{enumerate}
\item Actions for {\em inserting} a {\em precondition}, {\em positive}/{\em negative} effect into the action model $\xi$ following the syntactic constraints of \strips\ . 
\begin{itemize}
\item Actions to insert an $e\in{\mathcal I}_{\Psi,\xi}$ {\em precondition} into $\xi$. The precondition is only inserted when neither $pre\_e\_\xi$ nor $eff\_e\_\xi$ exist in $\xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertPre_{p,\xi}})=&\{\neg pre\_e\_\xi, \neg eff\_e\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre\_e\_\xi\}.
\end{align*}
\end{small}

\item Actions to insert an $e\in{\mathcal I}_{\Psi,\xi}$ {\em effect} to the action model $\xi$. 

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertEff_{p,\xi}})=&\{\neg eff\_e\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertEff_{p,\xi}})=&\{\emptyset\}\rhd\{eff\_e\_\xi\}.
\end{align*}
\end{small}
\end{itemize}

\item Actions for {\em applying} an action model $\xi$ built by the {\em insert} actions and bounded to objects $\omega\subseteq\Omega^{|pars(\xi)|}$ (where $\Omega$ is the set of {\em objects} used to induce the fluents $F$ by assigning objects in $\Omega$ to the $\Psi$ predicates and $\Omega^k$ is the $k$-th Cartesian power of $\Omega$). Note that the action parameters, $pars(\xi)$, are bound to the objects in $\omega$ that appear in the same position.
\end{enumerate}
\end{itemize}

\begin{small}
\begin{align*}
\pre(\mathsf{apply_{\xi,\omega}})=&\{pre\_e\_\xi\implies p(\omega)\}_{\forall e\in{\mathcal I}_{\Psi,\xi}},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{pre\_e\_\xi\wedge eff\_e\_\xi\}\rhd\{\neg p(\omega)\}_{\forall e\in{\mathcal I}_{\Psi,\xi}},\\
&\{\neg pre\_e\_\xi \wedge eff\_e\_\xi\}\rhd\{p(\omega)\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}\},\\
&\{\emptyset\}\rhd\{\neg mode_{insert}\}.
\end{align*}
\end{small}

The intuition of the compilation is that the dynamics of the actions for {\em applying} an action model $\xi$ is determined by the values of the corresponding $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ fluents in the current state. For instance when executing {\tt{\small (apply\_stack blockB blockA)}} in a state $s$, its preconditions and effects are activated according to the values of the corresponding $\{pre\_e\_stack, eff\_e\_stack\}_{\forall e\in{\mathcal I}_{\Psi,stack}}$ fluents in $s$. This means that if the current state $s$ holds $\{{\tt\scriptsize (pre\_stack\_holding\_v1),(pre\_stack\_clear\_v2)}\} \subset s$, then it must be checked that positive literals {\tt{\small (holding blockB)}} and {\tt{\small (clear blockA)}} hold in $s$. Otherwise, a different set of precondition literals will be checked for the stack action. The same applies to the effects of {\tt{\small (apply\_stack blockB blockA)}}. Executing {\tt{\small (apply\_stack blockB blockA)}}, will add the literals {\tt{\small (on blockB blockA),(clear blockB),(not(clear blockA)),(handempty)}} and {\tt{\small(not(clear blockB))}} to the successor state only if the $\{pre\_e\_stack, eff\_e\_stack\}_{\forall e\in{\mathcal I}_{\Psi,stack}}$ fluents has been correctly programmed by the corresponding {\em insert} actions.

\begin{figure}[hbt!]
	{\tiny\tt

\begin{tabular}{ll}
		{\bf 00}:(insert\_pre\_stack\_holding\_v1) & {\bf 10}:(insert\_eff\_pickup\_clear\_v1) \\
		01:(insert\_pre\_stack\_clear\_v2) & 11:(insert\_eff\_pickup\_ontable\_v1)\\
                {\bf 02}:(insert\_pre\_pickup\_handempty) & 12:(insert\_eff\_pickup\_handempty)\\
                03:(insert\_pre\_pickup\_clear\_v1) & 13:(insert\_eff\_pickup\_holding\_v1)\\
                04:(insert\_pre\_pickup\_ontable\_v1) & {\bf 14}:(apply\_pickup blockB)\\
                {\bf 05}:(insert\_eff\_stack\_clear\_v1) & 15:(apply\_stack blockB blockC)\\
                06:(insert\_eff\_stack\_clear\_v2) & 16:(apply\_pickup blockA)\\
                07:(insert\_eff\_stack\_handempty) & 17:(apply\_stack blockA blockB) \\
                08:(insert\_eff\_stack\_holding\_v1) & \\
                09:(insert\_eff\_stack\_on\_v1\_v2) &             		 
\end{tabular}
}
	\caption{\small Plan computed when solving the classical planning problem output by our compilation corresponding to a classical planning with unknown domain models.}
	\label{fig:plan-lplan}
\end{figure}

Figure~\ref{fig:plan-lplan} shows a solution plan computed when solving a $P'=\tup{F',A',I,G}$ classical planning problem output by our compilation. In the initial state of that problem three  blocks ({\small\tt blockA}, {\small\tt blockB} and {\small\tt blockC}) are clear and on top of the table, the robot hand is empty. The problem goal is having the three-block tower {\tt blockA} on top of {\tt blockB} and {\tt blockB} on top of {\tt blockC}. The plan shows the {\em insert} actions for the {\tt\small stack} scheme (steps $00-01$ insert the preconditions, steps $05-10$ insert the effects), steps $02-04$ insert the preconditions of the {\tt\small pickup} scheme (while steps $10-13$ insert the effects of this scheme). Finally, steps $14-17$ is the plan postfix that applies the programmed action model to achieve the goals $G$ starting from $I$. Note that another valid solution could be computed for instance, by inserting the same preconditions and effects but into the {\em unstack} and {\em putdown} actions and then applying instead the four step postfix {\tt\small (putdown blockB), (unstack blockB blockC), (putdown blockA), (unstack blockA blockB)}.

\subsection{Compilation properties}
\begin{mylemma}
Soundness. Any classical plan $\pi'$ that solves $P'$ produces a solution to the classical planning problem with unknown domain models $P=\tup{F,A[\cdot],I,G}$.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
Once a given precondition (or effect) is inserted into an action model it can never be removed back and once an action model is applied (via an $\mathsf{apply_{\xi,\omega}}$ action) it cannot longer be {\em programed}. The set of goals $G$ can only be achieved executing an applicable sequence of $\mathsf{apply_{\xi,\omega}}$ actions that, starting in the corresponding initial state reach a state $G \subseteq s_n$ (the fluents of the original problem $F$ can exclusively be modified by the $\mathsf{apply_{\xi,\omega}}$ actions). This means that the action model used by the $\mathsf{apply_{\xi,\omega}}$ actions has to be consistent with the traversed intermediate states. We know that this must be true by the definition of the $\mathsf{apply_{\xi,\omega}}$ so hence, the sub-sequence of $\mathsf{apply_{\xi,\omega}}$ appearing in $\pi'$ to solve $P'$ is a solution plan to $P=\tup{F,A[\cdot],I,G}$. 
\end{small}
\end{proof}

\begin{mylemma}
Completeness. Any plan $\pi$ that solves $P=\tup{F,A[\cdot],I,G}$ is computable solving the corresponding classical planning task $P'$.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
By definition ${\mathcal I}_{\Psi,\xi}$ fully captures the set of elements that can appear in an action model $\xi$ using predicates $\Psi$. Furthermore, the compilation does not discard any possible action model definable within ${\mathcal I}_{\Psi,\xi}$. This means that for every plan $\pi$ that solves $P=\tup{F,A[\cdot],I,G}$, we can build a plan $\pi'$ by selecting the appropriate actions for inserting precondition and effects to the corresponding action model and then selecting the corresponding $\mathsf{apply_{\xi,\omega}}$ actions that transform the initial state $I$ into a state $G \subseteq s_n$.
\end{small}
\end{proof}

The size of the classical planning task $P'$ output by our compilation depends on the arity of the given {\em predicates} $\Psi$ and the number of parameters of the action models, $|pars(\xi)|$. The larger these arities, the larger $|{\mathcal I}_{\Psi,\xi}|$. This term dominates the compilation size because it defines the $\{pre\_e\_\xi, eff\_e\_\xi\}$ fluents, the corresponding set of {\em insert} actions, and the number of conditional effects of the $\mathsf{apply_{\xi,\omega}}$ actions. 

\subsection{The bias of the initially {\em empty} action model}
Classical planners tend to preffer shorter solution plans, so our compilation may introduce a bias to $P=\tup{F,A[\cdot],I,G}$ problems preferring solutions that are referred to action models with a shorter number of {\em preconditions}/{\em effects}. In more detail, all $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ fluents are false at the initial state of our $P'=\tup{F',A',I,G}$ compilation so classical planners tend to solve $P'$ with plans that require a shorter number of {\em insert} actions.

This bias could be eliminated defining a cost function for the actions in $A'$ (e.g. {\em insert} actions has {\em zero cost} while $\mathsf{apply_{\xi,\omega}}$ actions has a {\em positive constant cost}). In practice we use a different approach to disregard the cost of {\em insert} actions because classical planners are not proficiency optimizing {\em plan cost} with zero-cost actions. Instead, our approach is to use a SAT-based planner~\cite{rintanen2014madagascar} because it can apply all actions for inserting preconditions in a single planning step (these actions do not interact). Further, the actions for inserting action effects are also applied in a single planning step so the plan horizon for programming any action model is always bound to 2, which significantly reduces the planning horizon.

Our compilation for {\em planning with unknown domain models} can then be understood as an extension of the SATPLAN approach for classical planning~\cite{kautz1992planning} with two additional initial layers: a first layer for inserting the action preconditions and a second one for inserting the action effects. These two extra layers are followed by the typical $N$ layers of the SATPLAN encoding (extended however to apply the action models that are determined by the previous two initial layers, the $\mathsf{apply_{\xi,\omega}}$ actions). Regarding again the example of Figure~\ref{fig:plan-lplan}, this means that steps [00-04] are applied in paralel in the first SATPLAN layer, steps [05-13] are applied in paralel in the second layer and each step [14-17] is applied sequentially and correponds to a differerent SATPLAN layer (so just six layers are necesary to compute the example plan of Figure~\ref{fig:plan-lplan}).

The SAT-based planning approach is also convenient for the task of {\em goal recognition as planning with unknown domain models} because its ability to deal with classical planning problems populated with dead-ends and because symmetries in the insertion of preconditions/effects into an action model do not affect to the planning performance.

\subsection{Constraining the action model space}
In some contexts it is however reasonable to assume that some portions of the action model are known~\cite{ZhuoNK13,sreedharan2018handling,pereira2018heuristic}. Our compilation approach is also flexible to this particular scenario. The known preconditions and effects are encoded setting the corresponding fluents $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ to true in the initial state. Further, the corresponding insert actions, $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertEff_{p,\xi}}$, become unnecessary and are removed from $A_{\Lambda}$, making the classical planning task $P_{\Lambda}$ easier to be solved.

For example, suppose that the preconditions of the {\em blocksworld} action schema {\tt stack} are known, then the initial state $I$ is extended with literals, {\small\tt(pre\_holding\_v1\_stack)} and {\small\tt(pre\_clear\_v2\_stack)} and the associated actions $\mathsf{insertPre_{holding_v1,stack}}$ and $\mathsf{insertPre_{clear_v2,stack}}$ can be safely removed from the $A_{\Lambda}$ action set without altering the {\em soundness} and {\em completeness} of the $P_{\Lambda}$ compilation.

{\em Domain-specific knowledge} is also helpfull to constrain further the space of possible action schemata. For instance, in the {\em blocksworld} one can argue that {\small\tt on($v_1$,$v_1$)} and {\small\tt on($v_2$,$v_2$)} will not appear in the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ lists of an action schema $\xi$ because, in this specific domain, a block cannot be on top of itself. With this regard, {\it state invariants} can be exploited either as {\em syntactic} constraints (to reduce the space of possible action models) but also as {\em semantic} constraints (to complete partial observations of the states traversed by a plan)~\cite{fox:TIM:JAIR1998}. 






\section{Evaluation}
\label{sec:evaluation}

\section{Related Work}
\label{sec:evaluation}
The problem of {\em classical planning with unknown domain models} has been previously addressed~\cite{SternJ17}. In this work we evidence the relevance of this task for addressing {\em goal recognition} when the action model of the observed agent is not available.   

The paper also showed that {\em goal recognition}, when the domain model is unknown, is closely related to the learning of planning action models. With this regard, the classical planning compilation for learning \strips\ action models~\cite{aineto2018learning} is very appealing because it allows to produce a \strips\ action model from minimal input knowledge (a single initial state and goals pair), and to refine this model if more input knowledge is available (e.g. observation constraints). Most of the existing approaches for learning action models aim maximizing an statistical consistency of the learned model with respect to the input observations so require large amounts of input knowledge and do not produce action models that are guaranteed to be {\em logically consistent} with the given input knowledge.

Our approach for {\em planning with an unknown domain model} is related to {\em goal recognition design}~\cite{KerenGK14}. The reason is that we are encoding the space of propositional schemes as state variables of the planning problem (the initial state encodes the {\em empty} action model with no preconditions and no effects) and provide actions to modify the value of this state variables as in {\em goal recognition design}. The aims of {\em goal recognition design} are however different. {\em Goal recognition design} applied to {\em goal recognition with unknown domain models} would compute the action model, in the space of possible models, that allows to reveal any of the possible goals as early as possible.



\section{Conclusions}
\label{sec:conclusions}



\subsection*{Acknowledgments}
This work is supported by the Spanish MINECO project TIN2017-88476-C2-1-R. D. Aineto is partially supported by the {\it FPU16/03184} and S. Jim\'enez by the {\it RYC15/18009}. M. Ram\'irez research is partially funded by DST Group Joint \& Operations Analysis Division.

\bibliography{planlearnbibliography}
\bibliographystyle{aaai}

\end{document}
