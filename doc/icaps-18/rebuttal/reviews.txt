Dear Sergio,

Thank you for your submission to ICAPS 2018. The ICAPS 2018 review
response period starts now and ends at January 13.

During this time, you will have access to the current state of your
reviews and have the opportunity to submit a response.  Please keep in
mind the following during this process:

* Most papers have a so-called placeholder review, which was
  necessary to give the discussion leaders access to the reviewer
  discussion. Some of these reviews list questions that already came
  up during the discussion and which you may address in your response but
  in all cases the (usually enthusiastic) scores are meaningless and you
  should ignore them. Placeholder reviews are clearly indicated as such in
  the review.

* Almost all papers have three reviews. Some may have four. A very
  low number of papers are missing one review. We hope to get that
  review completed in the next day. We apologize for this.

* The deadline for entering a response is January 13th (at 11:59pm
  UTC-12 i.e. anywhere in the world).

* Responses must be submitted through EasyChair.

* Responses are limited to 1000 words in total. You can only enter
  one response, not one per review.

* You will not be able to change your response after it is submitted.

* The response must focus on any factual errors in the reviews and any
  questions posed by the reviewers. Try to be as concise and as to the
  point as possible.

* The review response period is an opportunity to react to the
  reviews, but not a requirement to do so. Thus, if you feel the reviews
  are accurate and the reviewers have not asked any questions, then you
  do not have to respond.

* The reviews are as submitted by the PC members, without much
  coordination between them. Thus, there may be inconsistencies.
  Furthermore, these are not the final versions of the reviews. The
  reviews can later be updated to take into account the discussions at
  the program committee meeting, and we may find it necessary to solicit
  other outside reviews after the review response period.

* The program committee will read your responses carefully and
  take this information into account during the discussions. On the
  other hand, the program committee may not directly respond to your
  responses in the final versions of the reviews.

The reviews on your paper are attached to this letter. To submit your
response you should log on the EasyChair Web page for ICAPS 2018 and
select your submission on the menu.

----------------------- REVIEW 1 ---------------------
PAPER: 39
TITLE: Learning strips action models with classical planning
AUTHORS: Diego Aineto, Sergio Jimenez and Eva Onaindia

Significance: 2 (modest contribution or average impact)
Soundness: 3 (correct)
Scholarship: 2 (relevant literature cited but could be expanded)
Clarity: 3 (well written)
Reproducibility: 5 (code and domains (whichever apply) are already publicly available)
Overall evaluation: -1 (weak reject)
Reviewer's confidence: 3 (high)
Suitable for a demo?: 2 (maybe)
Nominate for Best Paper Award: 1 (no)
Nominate for Best Student Paper Award (if eligible): 1 (no)
[Applications track ONLY]: Importance and novelty of the application: 6 (N/A (not an Applications track paper))
[Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem: 5 (N/A (not an Applications track paper))
[Applications track ONLY] Maturity: 7 (N/A (not an Applications track paper))
[Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Evaluation on physical platforms/simulators: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Significance of the contribution: 6 (N/A (not a Robotics track paper))

----------- Review -----------
This paper presents an interesting approach to learning STRIPS actions models. Specifically, the paper discussed three settings, each with more information available in the input. In one setting, input data includes pairs of the type (initial state, final state), as well as other information such as the predicates used in the domain. In the second setting, for each pair (initial state, final state), a sequence of actions is also provided in the input. The sequence corresponds to a plan from the initial state to the final state. The preconditions and the effects of the actions are not known apriori, and the task is to discover them. In the third setting, some additional information (partial knowledge about the actions' preconditions and effects) is further given in the input.

The paper shows an interesting approach based on compiling the task into a classical planning problem. For instance, in the compiled task, each original action is initially assummed to require as preconditions all facts in the domain. New actions defined in the compiled problem can remove preconditions of the original actions.

Experiments are presented on several planning domains. Compiled tasks are attacked with Madagascar, a SAT-based planning system. Remarkably, the approach turns out to be fast in the experiments reported in the paper. In addition to the running times, precision and recall data are presented.

It is surprising that, in the experiments, no comparison is shown to other approaches from the literature. Judging on the definition of the problem, it looks like there could be a good overlapping between settings 2 and 3 and the way that other authors have formalized the problem in previous work (for instance, the ARMS system).

A second missing part in the evaluation is a study of how the effectiveness of the method (e.g., precision, recall, running time) is impacted when varying the number of learning examples per domain. The paper presents results with only one value, namely 5 learning examples.

I am looking forward to seeing the authors' response on the two points above.

Large tables, such as Table 1, look flat and hard to read. Consider presenting the data with charts, so that the tendencies seen in the data are really easy to grasp.

As a suggestion, the background section could be shorter, which would make room for additional details regarding the evaluation, for example.


Minor points:

Replace Is with It is in "Is flexible to different amounts".

WLOG should be replaced with the actual words (Without loss of generality).

In the formula of \Omega on page 3, \Omega = { o | o \in \Cup ...}:
isn't it simpler to say \Omega = \Cup ... ? Or am I missing something which should otherwise be obvious?

Put the year between parenthesis in Stern and Juba 2017.

----------------------- REVIEW 2 ---------------------
PAPER: 39
TITLE: Learning strips action models with classical planning
AUTHORS: Diego Aineto, Sergio Jimenez and Eva Onaindia

Significance: 2 (modest contribution or average impact)
Soundness: 3 (correct)
Scholarship: 2 (relevant literature cited but could be expanded)
Clarity: 3 (well written)
Reproducibility: 5 (code and domains (whichever apply) are already publicly available)
Overall evaluation: 2 (accept)
Reviewer's confidence: 4 (expert)
Suitable for a demo?: 1 (no)
Nominate for Best Paper Award: 1 (no)
Nominate for Best Student Paper Award (if eligible): 1 (no)
[Applications track ONLY]: Importance and novelty of the application: 6 (N/A (not an Applications track paper))
[Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem: 5 (N/A (not an Applications track paper))
[Applications track ONLY] Maturity: 7 (N/A (not an Applications track paper))
[Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Evaluation on physical platforms/simulators: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Significance of the contribution: 6 (N/A (not a Robotics track paper))

----------- Review -----------
The paper examines a rather interesting scenario, where a single relational STRIPS-style action description is learnt from a very small number of executions using a classical planning procedure. The approach is tested in a variety of IPC domains (10 classical domains), and appears for now to be quite practical. The results would be _much_ stronger if the authors evaluated how this approach scales as the number of training executions increases, and as the length of those executions increases.

To my knowledge this is the first implementation and experimental study using such an approach. Also, sufficient background literature is covered from the symbolic setting. Coverage is limited otherwise (more below), but that is acceptable in this case.

I quite enjoyed the idea implemented in this work, and evaluate it to be a sufficient contribution to be published. It has become increasingly topical to ponder the question of "Where do planning models come from?", and therefore this work is highly relevant.

Criticism of description of compilation:

 - If I have a set of blocks-world problems, I expect to have objects repeated across problems, and indeed fluent symbols repeated across problems. That detail is not discussed in your compilation.

 - For the Type-II compilation, with \Pi, the details of how you serialise plans and step through each plan should be given.  Can you please report on the serial length of your solution plans, relative to the length of an oracle plan -- i.e. oracle is min edits to get true domain, followed by serial executions of model validation.

 - For Type-I you claim you give "no information about actions". This is not strictly true, as unless I misunderstood, you provide action names and arity -- i.e. you later write "we assume that operator headers are known".

 - Only having a lemma for the Type-I scenario, Lemma 1, is a bit awkward. Are you considering mechanising your proofs?

Criticisms of your motivation for using Madagascar. It is controversial to claim, as you seem to have, that this is the only planner that can handle dead-ends. Moreover, the detail of post-serialisation, etc., are native to CP planning, and do not reflect information about SAT-based v.s. structured v.s. state-based planning. I see no error in regards to your choice of planning system, but only in your reasoning behind that choice. Do you have any data to back up your claims that M is the best planner here?

Criticisms of your statements regarding the limitations of (Michaliski etal., 2013) style ML. Statistical machine learning is perfectly capable of learning highly structure solution objects, of which STRIPS models would be one example. You yourself cite Yoon etal. work from the relational reinforcement learning setting.  I note that you have not cited [0], which would be a recent example coming from the resurgence of connectionism. As with relational reinforcement learning, you treat a scenario where you have the luxury of objects, predicate, propositions/fluents, actions, and so on. You derive one domain model consistent with a small set of example plans. That motivating scenario alone distinguishes you from the statistical folk.

Criticisms of learning from plans. Your objective is to learn a description of the domain. If you only learn from plans, you may not learn about dead-ends, traps, etc.

Criticism of precision and recall metrics: Just because a domain looks similar, or exactly the same as the domain from which executions were generated, that does not make it uniquely correct. For example, we could have a symmetry, so that we derive a perfectly good domain description, however this would look nothing like the domain description from which the actions were generated. I find these syntax-based metrics to be of little to no interest in your setting. How often does a plan generated using your modals fail VAL? How often does an execution fail an execution validator?

minor comments:

"because opens" --> ..._it_ opens...

"so low constrained" --> would "underconstrainted" work here?

"aimed learning operators" --> aimed _at_ learning...

[0] Asai M, Fukunaga A. Classical Planning in Deep Latent Space: Bridging the Subsymbolic-Symbolic Boundary. To appear AAAI-18. A version appeared @  KEPS 2017

----------------------- REVIEW 3 ---------------------
PAPER: 39
TITLE: Learning strips action models with classical planning
AUTHORS: Diego Aineto, Sergio Jimenez and Eva Onaindia

Significance: 3 (substantial contribution or strong impact)
Soundness: 3 (correct)
Scholarship: 3 (excellent coverage of related work)
Clarity: 3 (well written)
Reproducibility: 5 (code and domains (whichever apply) are already publicly available)
Overall evaluation: 3 (strong accept)
Reviewer's confidence: 4 (expert)
Suitable for a demo?: 3 (yes)
Nominate for Best Paper Award: 2 (yes)
Nominate for Best Student Paper Award (if eligible): 2 (yes)
[Applications track ONLY]: Importance and novelty of the application: 6 (N/A (not an Applications track paper))
[Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem: 5 (N/A (not an Applications track paper))
[Applications track ONLY] Maturity: 7 (N/A (not an Applications track paper))
[Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Evaluation on physical platforms/simulators: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Significance of the contribution: 6 (N/A (not a Robotics track paper))

----------- Review -----------
This is only a placeholder review. Please ignore it.

----------------------- REVIEW 4 ---------------------
PAPER: 39
TITLE: Learning strips action models with classical planning
AUTHORS: Diego Aineto, Sergio Jimenez and Eva Onaindia

Significance: 1 (minimal contribution or weak impact)
Soundness: 2 (minor inconsistencies or small fixable errors)
Scholarship: 2 (relevant literature cited but could be expanded)
Clarity: 2 (mostly readable with some room for improvement)
Reproducibility: 4 (authors promise to release code and domains (whichever apply))
Overall evaluation: -1 (weak reject)
Reviewer's confidence: 3 (high)
Suitable for a demo?: 1 (no)
Nominate for Best Paper Award: 1 (no)
Nominate for Best Student Paper Award (if eligible): 1 (no)
[Applications track ONLY]: Importance and novelty of the application: 6 (N/A (not an Applications track paper))
[Applications track ONLY]: Importance of planning/scheduling technology to the solution of the problem: 5 (N/A (not an Applications track paper))
[Applications track ONLY] Maturity: 7 (N/A (not an Applications track paper))
[Robotics track ONLY]: Balance of Robotics and Automated Planning and Scheduling: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Evaluation on physical platforms/simulators: 6 (N/A (not a Robotics track paper))
[Robotics Track ONLY]: Significance of the contribution: 6 (N/A (not a Robotics track paper))

----------- Review -----------
This paper describes a reduction from the problem of learning STRIPS action models to classical STRIPS planning, for various types of training data. Training data can be as little as example initial and goal state pairs or as much as action sequences (plans) connecting such pairs along with partial action models.  The predominant content of the paper is the detailed description of the fairly straightforward reduction.  Substantial empirical results are also provided showing that when plans are provided as training data, correct STRIPS definitions can sometimes be recovered in a wide variety of benchmark STRIPS domains.


It is unsurprising that such a reduction exists, given that STRIPS planning is a general form of discrete search problem, and learning STRIPS action definitions is such a search problem.

While learning within a hypothesis space is indeed a form of search, it seems an oversimplification to focus only on the search, and not discuss the bias of the resulting learner.  Bias, here, seems to be provided by defaulting some of the choices, favoring extra preconditions but fewer effects, but this is done without discussion.

Also, a reduction to STRIPS planning seems maybe like overkill: is the original learning problem as hard as general STRIPS planning?  This is also not discussed.

My main problem with the work is that the presentation is so dominated by rather superficial technical details, while the overall content sheds very little insight on the learning problem being addressed.  While the empirical results cover many domains, it is hard to understand much from them about what is working and what is not working and why.  There is also no baseline learner to compare to, a major weakness. In the end, no case is made that this is a good way to solve the learning problem compared to alternatives.

The work also has several grammatical errors or typos.  Also, on p. 4, I could not see any place where the initial states for labels other than label 0 were used.  Doesn't validation need to reset to the initial state s_1 in order to validate label 1, then to s_2 to validate label 2, etc?

------------------------------------------------------

Best wishes,
Gabi Röger and Sven Koenig
ICAPS 2018 program chairs


