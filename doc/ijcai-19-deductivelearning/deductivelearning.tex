%%%% ijcai19.tex

\typeout{IJCAI-19 Instructions for Authors}

% These are the instructions for authors for IJCAI-19.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'
\usepackage{ijcai19}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

%%%%%%%%%%%%%%%%%% Added for this paper
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}% http://ctan.org/pkg/listings
\lstset{
  basicstyle=\ttfamily,
  mathescape
}
\usepackage{ wasysym }
\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{mydefinition}[mytheorem]{Definition}

%%%%%%%%%%%%%





% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning action models from {\em state-invariants}}

% Single author syntax
%\author{
%    Sarit Kraus
%    \affiliations
%    Department of Computer Science, Bar-Ilan University, Israel \emails
%    pcchair@ijcai19.org
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai19-multiauthor.tex file for detailed instructions
\author{
Diego Aineto$^1$\and
Sergio Jim\'enez$^1$\and
Eva Onaindia$^1$
\affiliations
$^1${\small Departamento de Sistemas Inform\'aticos y Computaci\'on. Universitat Polit\`ecnica de Val\`encia. Valencia, Spain}
\emails
{\scriptsize \{dieaigar,serjice,onaindia\}@dsic.upv.es}}


\begin{document}
\maketitle


\begin{abstract}
This paper addresses the learning of \strips\ action models from {\em state-invariants} (i.e logic formulae that specify constraints about the possible states of a given domain). The benefit of exploiting {\em state-invariants} is two-fold, they constrain the space of possible action models and they can complete learning examples that are only partially observed. Our approach for learning \strips\ action from {\em state-invariants} is a {\em classical planning} compilation that is flexible to different kinds of input knowledge (e.g., partially observations of plan executions including partially observed intermediate states and/or actions). The experimental results demonstrate that, even at unfavorable scenarios where input observations are minimal (just an {\em initial state} and the {\em goals}), {\em state-invariant} are helpful to learn good quality \strips\ action models.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The specification of planning action models is a complex process that limits, too often, the application of {\em model-based planning} to real-world tasks~\cite{kambhampati:modellite:AAAI2007}. The {\em machine learning} of action models relieves this {\em knowledge acquisition bottleneck} of {\em model-based planning} and nowadays, there exists a wide range of effective approaches for learning action models~\cite{arora:amodels:ker2018}. Many of the most successful approaches for learning planning action models are however purely {\em inductive}~\cite{yang2007learning,pasula2007learning,mourao2010learning,zhuo2013action}, linking learning performance exclusively to the {\em amount} and {\em quality} of the input learning examples. 

This paper addresses the learning of action models exploiting a different source of knowledge, {\em deductive} knowledge. Our approach leverages {\em state-invariants}, i.e. logic formulae that specify constraints about the possible states of a given domain, to cushion the negative impact of insufficient learning examples. Given an action model, state-of-the-art planners infer {\em state-invariants} from that model to reduce the search space and make the planning process more efficient~\cite{helmert2009concise}. In this paper we follow the opposite direction and leverage {\em state-invariants} to learn the planning action model. The benefit of learning action models from {\em state-invariants} is two-fold, {\em state-invariants} constrain the space of possible action models and can complete learning examples that are only partially observed.

Our approach for learning \strips\ action models from {\em state-invariants} is compile the learning task into a classical planning task. Our compilation is flexible to different kinds of input knowledge (e.g., partially/fully observations of actions of plan executions as well as partially/fully observed intermediate states) and outputs an action model that is {\em consistent} with the given input knowledge. The experimental results demonstrate that, even at unfavorable scenarios where input observations are minimal (just an {\em initial state} and the {\em goals}), {\em state-invariant} help to learn better \strips\ models. 



\section{Background}
\label{sec:background}
This section formalizes the {\em classical planning model} we follow in this work and introduces the classical planning compilation for the learning of \strips\ action models~\cite{aineto2018learning}. Finally, the section formalizes {\em state-invariants}.

\subsection{Classical planning with conditional effects}
Let $F$ be the set of  propositional state variables ({\em fluents}) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$; i.e. either~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (without loss of generality, we will assume that $L$ does not contain conflicting values). Given $L$, let $\neg L=\{\neg l:l\in L\}$ be its complement. We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents. A {\em state} $s$ is a full assignment of values to fluents; $|s|=|F|$.

A {\em classical planning action} $a\in A$ has: a precondition $\pre(a)\in\mathcal{L}(F)$, a set of effects $\eff(a)\in\mathcal{L}(F)$, and a positive action cost $cost(a)$. The semantics of actions $a\in A$ is specified with two functions: $\rho(s,a)$ denotes whether action $a$ is {\em applicable} in a state $s$ and $\theta(s,a)$ denotes the {\em successor state} that results of applying action $a$ in a state $s$. Then, $\rho(s,a)$ holds iff $\pre(a)\subseteq s$, i.e.~if its precondition holds in $s$. The result of executing an applicable action $a\in A$ in a state $s$ is a new state $\theta(s,a)=(s\setminus \neg\eff(a))\cup\eff(a)$. Subtracting the complement of $\eff(a)$ from $s$ ensures that $\theta(s,a)$ remains a well-defined state. The subset of action effects that assign a positive value to a state fluent is called {\em positive effects} and denoted by $\eff^+(a)\in \eff(a)$ while $\eff^-(a)\in \eff(a)$ denotes the {\em negative effects} of an action $a\in A$.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is the initial state and $G\in\mathcal{L}(F)$ is the set of goal conditions over the state variables. A {\em plan} $\pi$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$, with $|\pi|=n$ denoting its {\em plan length} and $cost(\pi)=\sum_{a\in\pi} cost(a)$ its {\em plan cost}. The execution of $\pi$ on the initial state of $P$ induces a {\em trajectory} $\tau(\pi,P)=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, it holds $\rho(s_{i-1},a_i)$ and $s_i=\theta(s_{i-1},a_i)$. A plan $\pi$ solves $P$ iff the induced {\em trajectory} $\tau(\pi,P)$ reaches a final state $G \subseteq s_n$, where all goal conditions are met. A solution plan is {\em optimal} iff its cost is minimal.

We also define {\em actions with conditional effects} because they are useful to compactly formulate our approach for {\em goal recognition with unknown domain models}. An action $a_c\in A$ with conditional effects is a set of preconditions $\pre(a_c)\in\mathcal{L}(F)$ and a set of {\em conditional effects} $\cond(a_c)$. Each conditional effect $C\rhd E\in\cond(a_c)$ is composed of two sets of literals: $C\in\mathcal{L}(F)$, the {\em condition}, and $E\in\mathcal{L}(F)$, the {\em effect}. An action $a_c$ is applicable in a state $s$ if $\rho(s,a_c)$ is true, and the result of applying action $a_c$ in state $s$ is $\theta(s,a_c)=\{s\setminus\neg\eff_c(s,a)\cup\eff_c(s,a)\}$ where $\eff_c(s,a)$ are the {\em triggered effects} resulting from the action application (conditional effects whose conditions hold in $s$):
\[
\eff_c(s,a)=\bigcup_{C\rhd E\in\cond(a_c),C\subseteq s} E,
\]

\subsection{Learning action models with classical planning}
This work is built on top of the {\em classical planning compilation} for the learning \strips\ action models~\cite{aineto2018learning}. This compilation receives as input an empty model $\mathcal{M}$ (which only contains the action headers), and an observation of a plan execution $\mathcal{O}(\tau)$ (extensible to a set of observations). The compilation outputs a model $\mathcal{M'}$ that specifies the preconditions and effects of each action schema included in $\mathcal{M}$ such that the validation of $\mathcal{O}(\tau)$ following $\mathcal{M'}$ is successful; i.e., it holds $\rho(s_{i-1}^o,a_i)$ for every observed action of $\mathcal{O}(\tau)$ and $s_i^o=\theta(s_{i-1}^o,a_i)$ for every observed state of $\mathcal{O}(\tau)$.

Essentially, a solution plan to the classical planning problem that results from the compialtion is a sequence of: (a) \emph{insert actions} that insert preconditions and effects on the schemata of $\mathcal{M}$ to build $\mathcal{M'}$ and (b) \emph{apply actions} that validate the application of the $\mathcal{M'}$ model in the observation $\mathcal{O(\tau)}$.

To illustrate this, Figure~\ref{fig:plan-lplan} shows a solution to a classical planning problem resulting from the~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning} compilation. In the initial state of that problem three  blocks ({\small\tt blockA}, {\small\tt blockB} and {\small\tt blockC}) are clear and on top of the table, the robot hand is empty. The problem goal is having the three-block tower {\tt blockA} on top of {\tt blockB} and {\tt blockB} on top of {\tt blockC}. The plan shows the {\em insert} actions for the {\tt\small stack} scheme (steps $00-01$ insert the preconditions, steps $05-10$ insert the effects), steps $02-04$ insert the preconditions of the {\tt\small pickup} scheme (while steps $10-13$ insert the effects of this scheme). Finally, steps $14-17$ is the plan postfix that applies the programmed action model to achieve the goals $G$ starting from $I$. 

\begin{figure}[hbt!]
	{\tiny\tt
\begin{tabular}{ll}
		{\bf 00}:(insert\_pre\_stack\_holding\_v1) & {\bf 10}:(insert\_eff\_pickup\_clear\_v1) \\
		01:(insert\_pre\_stack\_clear\_v2) & 11:(insert\_eff\_pickup\_ontable\_v1)\\
                {\bf 02}:(insert\_pre\_pickup\_handempty) & 12:(insert\_eff\_pickup\_handempty)\\
                03:(insert\_pre\_pickup\_clear\_v1) & 13:(insert\_eff\_pickup\_holding\_v1)\\
                04:(insert\_pre\_pickup\_ontable\_v1) & {\bf 14}:(apply\_pickup blockB)\\
                {\bf 05}:(insert\_eff\_stack\_clear\_v1) & 15:(apply\_stack blockB blockC)\\
                06:(insert\_eff\_stack\_clear\_v2) & 16:(apply\_pickup blockA)\\
                07:(insert\_eff\_stack\_handempty) & 17:(apply\_stack blockA blockB) \\
                08:(insert\_eff\_stack\_holding\_v1) &  {\bf 18}:(validate\_1)\\
                09:(insert\_eff\_stack\_on\_v1\_v2) &             		 
\end{tabular}
}
	\caption{\small Plan computed when solving a problem output by the classical planning compilation.}
	\label{fig:plan-lplan}
\end{figure}

\subsection{State-invariants}
The notion of {\em state-constraint} is very general and has been used in different areas of AI and for different purposes.  If we restrict ourselves to planning, {\em state-constraints} are abstractions for compactly specifying sets of states. For example, {\em state-constraints} in planning allow to specify the set of states where a given action is applicable, the set of states where a given {\em derived predicate} holds or the set of states that are considered goal states.

{\em State invariants} is a kind of state-constraints useful for computing more compact state representations~\cite{helmert2009concise} or making {\em satisfiability planning} and {\em backward search} more efficient~\cite{rintanen2014madagascar,alcazar2015reminder}. Given a classical planning problem $P=\tup{F,A,I,G}$, a {\em state invariant} is a formula $\phi$ that holds at the initial state of a given classical planning problem, $I\models \phi$, and at every state $s$, built from $F$, that is reachable from $I$ by applying actions in $A$. For instance, Figure~\ref{fig:strongest-invariant} shows five clauses that define {\em state invariants} for the {\em blocksworld} planning domain~\cite{slaney2001blocks}. 

\begin{figure}[hbt!]
  \begin{footnotesize}
$\forall x_1,x_2\ ontable(x_1)\leftrightarrow\neg on(x_1,x_2)$.\\
$\forall x_1,x_2\ clear(x_1)\leftrightarrow\neg on(x_2,x_1)$.\\
$\forall x_1,x_2,x_3\ \neg on(x_1,x_2)\vee\neg on(x_1,x_3)\ such\ that\ x_2\neq x_3$.\\
$\forall x_1,x_2,x_3\ \neg on(x_2,x_1)\vee\neg on(x_3,x_1)\ such\ that\ x_2\neq x_3$.\\
$\forall x_1,\ldots,x_n\ \neg(on(x_1,x_2)\wedge on(x_2,x_3)\wedge\ldots\wedge on(x_{n-1},x_n)\wedge on(x_n,x_1)).$
\end{footnotesize}
 \caption{\small Example of {\em state-invariants} for the {\em blocksworld} domain.}
\label{fig:strongest-invariant}
\end{figure}

A {\em mutex} (mutually exclusive) is a state invariant that takes the form of a binary clause and indicates a pair of different properties that cannot be simultaneously true~\cite{kautz:mutex:IJCAI1999}. For instance in a three-block {\em blocksworld}, $\neg on(block_A,block_B)\vee \neg on(block_A,block_C)$ is a {\em mutex} because $block_A$ can only be on top of a single block.

A {\em domain invariant} is an instance-independent state-invariant, i.e. holds for any possible initial state and any possible set of objects. Therefore, if a given state $s$ holds $s\nvDash \phi$ such that $\phi$ is a {\em domain invariant}, it means that $s$ is not a valid state. Domain invariants are often compactly defined as {\em lifted invariants} (also called schematic invariants)~\cite{rintanen:schematicInvariants:AAAI2017}.

We define a {\em domain mutex} as a $\tup{p,q}$ predicates pair where both $p\in\Psi$ and $q\in\Psi$ are predicates that shape the set of fluents $F$ of a given planning problem and such that they satisfy the following formulae $p\rightarrow \neg q$ where the predicate variables are universally quantified. For instance, predicates $holding(x)$ and $clear(x)$ from the {\em blocksworld} are {\em domain mutex} while predicates $clear(x)$ and $ontable(x)$ are not ($\forall x\ clear(x)\leftrightarrow\neg ontable(x)$ does not hold for every possible {\em blocksworld} state). We pay special attention to {\em domain mutex} because they identify the {\em properties} of a given type of objects~\cite{fox:TIM:JAIR1998} and because they enable (1) effectively pruning of inconsistent \strips\ action models and (2) effective completion of partially observed states. 



\section{Learning \strips\ action models from {\em state-invariants}}
\label{sec:learning}
First, this section defines the sampling space and the space of possible action models. Then, the section formalizes the task of learning \strips\ action models from {\em state-invariants}.

\subsection{The sampling space}
We define a {\em learning example} $\mathcal{O}=\tup{s_0^o,s_1^o \ldots, s_m^o}$ as a sequence of partial states, except for the initial state $s_0^o$, which is fully observed. A partially observable state $s_i^o$ is one in which $|s_i^o| < |F|$; i.e., a state in which at least a fluent of $F$ is not observable. Note that this definition also comprises the case $|s_i^o| = 0$, when the state is fully unobservable. Each {\em observed} states comprises $[1,|F|]$ fluents. The observation can still miss intermediate states that are {\em unobserved} so transiting between two consecutive observed states in $\mathcal{O}$ may require the execution of more than a single action ($\theta(s_i^o,\tup{a_1,\ldots,a_k})=s_{i+1}^o$, where ${\small k\geq 1}$ is unknown but finite. 

Our sampling space follows the {\em open world} assumption, i.e. what is not observed is considered unknown. {\em State-invariants} are helpful to infer new knowledge that was unobserved in the learning examples. Given a {\em domain mutex} $\tup{p,q}$ and a state observation $s_j^o\in \mathcal{O}(\tau)$, {\small $(1\leq j\leq m)$}, such that literal $p(\omega)\in s_j^o$ is an instantiation of predicate $p$ over some subset of objects $\omega\subseteq\Omega^{|pars(p)|}$ then, the state observation $s_j^o$ can be safely completed adding the new literal $\neg q(\omega)$ (despite $\neg q(\omega)$ was actually unobserved). For instance, if the literal {\tt\small holding(blockA)} is observed in a particular blocksword state and we know the {\em domain mutex} $\forall x\ holding(x)\leftrightarrow\neg clear(x)$ we can safely extend the observation with literal {\tt\small $\neg$clear(blockA)} (despite this literal was actually unobserved). 

\subsection{The space of \strips\ action models}
{\em A \strips\ action schema} $\xi$ is defined by: A list of {\em parameters} $pars(\xi)$, and three sets of predicates (namely $pre(\xi)$, $del(\xi)$ and $add(\xi)$) that shape the kind of fluents that can appear in the {\em preconditions}, {\em negative effects} and {\em positive effects} of the actions induced from that schema. Let be $\Psi$ the set of {\em predicates} that shape the propositional state variables $F$, and a list of {\em parameters}, $pars(\xi)$. The set of elements that can appear in $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of the \strips\ action schema $\xi$ is the set of FOL interpretations of $\Psi$ over the parameters $pars(\xi)$ and is denoted as ${\mathcal I}_{\Psi,\xi}$.

For instance in a four-operator {\em blocksworld}~\cite{slaney2001blocks}, the ${\mathcal I}_{\Psi,\xi}$ set contains only five elements for the {\small \tt pickup($v_1$)} schemata, ${\mathcal I}_{\Psi,pickup}$={\small\tt\{handempty, holding($v_1$), clear($v_1$), ontable($v_1$), on($v_1,v_1$)\}} while it contains eleven elements for the {\small \tt stack($v_1$,$v_2$)} schemata, ${\mathcal I}_{\Psi,stack}$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$), clear($v_2$), ontable($v_1$), ontable($v_2$), on($v_1,v_1$), on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. 

Despite any element of ${\mathcal I}_{\Psi,\xi}$ can {\em a priori} appear in the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of schema $\xi$, in practice the actual space of possible \strips\ schemata is bounded by constraints of two kinds:
\begin{enumerate}
\item {\bf Syntactic constraints}. \strips\ constraints require $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$. Considering exclusively these syntactic constraints, the size of the space of possible \strips\ schemata is given by $2^{2\times|{\mathcal I}_{\Psi,\xi}|}$. {\em Typing constraints} are also of this kind~\cite{mcdermott1998pddl}. 
\item {\bf Observation constraints}. The observation of the actions and states resulting from the execution of a plan depicts {\em semantic knowledge} that constraints further the space of possible action schemata.   
\end{enumerate}

In this work we introduce a novel propositional encoding of the {\em preconditions}, {\em negative}, and {\em positive} effects of a \strips\ action schema $\xi$ that uses only fluents of two kinds {\tt\small pre\_e\_$\xi$} and {\tt\small eff\_e\_$\xi$} (where $e\in{\mathcal I}_{\Psi,\xi}$). This encoding exploits the syntactic constraints of \strips\, so it is more compact that the one previously proposed by~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning} for learning \strips\ action models with classical planning. In more detail, if {\tt\small pre\_e\_$\xi$} holds it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em precondition} in $\xi$. If {\tt\small pre\_e\_$\xi$} and {\tt\small eff\_e\_$\xi$} holds it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em negative effect} in $\xi$ while if $pre\_e\_\xi$ does not hold but {\tt\small eff\_e\_$\xi$} holds, it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em positive effect} in $\xi$. Figure~\ref{fig:propositional} shows the PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema using the {\tt\small pre\_e\_stack} and {\tt\small eff\_e\_stack} fluents ($e\in{\mathcal I}_{\Psi,stack}$).

\begin{figure}
  \begin{tiny}  
  \begin{verbatim}
(:action stack
   :parameters (?v1 ?v2)
   :precondition (and (holding ?v1) (clear ?v2))
   :effect (and (not (holding ?v1)) (not (clear ?v2))
                (clear ?v1) (handempty) (on ?v1 ?v2)))


(pre_holding_v1_stack) (pre_clear_v2_stack)
(eff_holding_v1_stack) (eff_clear_v2_stack)
(eff_clear_v1_stack) (eff_handempty_stack) (eff_on_v1_v2_stack)
  \end{verbatim}           
  \end{tiny}  
 \caption{\small PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema.}
\label{fig:propositional}
\end{figure}

One can also introduce {\em domain-specific knowledge} to constrain further the space of possible schemata. For instance, in the {\em blocksworld} one can argue that {\small\tt on($v_1$,$v_1$)} and {\small\tt on($v_2$,$v_2$)} will not appear in the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ lists of an action schema $\xi$ because, in this specific domain, a block cannot be on top of itself. {\it State invariants} are {\em domain-specific knowledge} that can be seen either as {\em syntactic} or {\em semantic} constraints. On the one hand, {\it state invariants} constrain the space of possible action models but on the other hand, they can complete partial observations of the states traversed by a plan.

\subsection{The learning task}
We define the task of learning a planning action model from {\em state-invariants} as a tuple $\Lambda=\tup{\mathcal{M},{\mathcal O},\Phi}$, where:
\begin{itemize}
\item $\mathcal{M}$ is the {\em initial empty model} that contains only the name, $name(\xi)$, and parameters, $pars(\xi)$, of each action model $\xi$ to be learned.
\item $\mathcal{O}=\tup{s_0^o,s_1^o \ldots, s_m^o}$ is a single learning example. This example can be reduced to its minimal expression $\mathcal{O}^*=\tup{s_0^o, s_m^o}$ meaning that it only contains two state observations, a full initial state $s_0^o$ and a partially observed state $s_m^o$. Note that the set of predicates $\Psi$ is deducible from $\mathcal{O}$ since $s_0^o$ is a fully observed state.
\item $\Phi$ is a set of {\em state-invariants} that define constraints about the set of possible states.
\end{itemize}

$M$ is the {\em space of possible action models} for the $A[\cdot]$ actions (i.e., the set of possible specifications of the $\rho$ and/or $\theta$ functions for each $a\in A[\cdot]$ action). We say that a given model $\mathcal{M}\in M$ is a {\em solution} to the $\Lambda=\tup{\mathcal{M},{\mathcal O},\Phi}$ learning task iff there exists a plan $\pi$ that solves $P=\tup{F,A[\cdot],I,G}$, when the semantics of each action $a\in A[\cdot]$ is given by $\mathcal{M}$, and such that any state traversed by a trajectory $\tau(\pi,P)$ is {\em consistent} with the input set of {\em state-invariants} $\Phi$.

Now, we show how to exploit our compact encoding of \strips\ action models to solve a $\Lambda=\tup{\mathcal{M},{\mathcal O},\Phi}$ learning task with an off-the-shelf classical planner.


\section{Learning action models from {\em state-invariants} with classical planning}
\label{sec:compilation}
Given a $\Lambda=\tup{\mathcal{M},{\mathcal O},\Phi}$ learning task we build and solve a classical planning problem $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I,G_{\Lambda}}$ such that:
\begin{itemize}
\item $F_{\Lambda}$ extends $F$ with a fluent {\small$mode_{inval}$}, to indicate whether an action model is {\em inconsistent} with the input {\em state-invariants} $\Phi$, a fluent {\small$mode_{insert}$}, to indicate whether action models are being programmed, and the fluents for the propositional encoding of the corresponding space of STRIPS action models. As explained, this is a set of fluents of the type $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$.

\item $G_{\Lambda}= G\cup \{\neg mode_{inval}\}$ extends the original goals $G$ with the $\neg mode_{inval}$ literal to validate that only states {\em consistent} with the state constraints $\Phi$ are traversed by $P_{\Lambda}$ solutions.

\item $A_{\Lambda}$ replaces the actions in $A$ with two types of actions.
\begin{enumerate}
\item Actions for {\em inserting} a {\em precondition}, {\em positive} effect or {\em negative} effect in $\xi$ following the syntactic constraints of \strips\ models. 
\begin{itemize}
\item Actions which support the addition of a {\em precondition} $p\in {\mathcal I}_{\Psi,\xi}$ to the action model $\xi$. A precondition $p$ is inserted in $\xi$ when neither $pre_p$, $eff_p$ exist in $\xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertPre_{p,\xi}})=&\{\neg pre\_p\_\xi, \neg eff\_p\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre\_p\_\xi\}.
\end{align*}
\end{small}

\item Actions which support the addition of a {\em negative} or {\em positive} effect $p\in {\mathcal I}_{\Psi,\xi}$ to the action model $\xi$. 

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertEff_{p,\xi}})=&\{\neg eff\_p\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertEff_{p,\xi}})=&\{\emptyset\}\rhd\{eff\_p\_\xi\}.
\end{align*}
\end{small}
\end{itemize}

\item Actions for {\em applying} an action model $\xi$ built by the {\em insert} actions and bounded to objects $\omega\subseteq\Omega^{|pars(\xi)|}$ (where $\Omega$ is the set of {\em objects} used to induce the fluents $F$ by assigning objects in $\Omega$ to the $\Psi$ predicates, and $\Omega^k$ is the $k$-th Cartesian power of $\Omega$). The action parameters, $pars(\xi)$, are bound to the objects in $\omega$ that appear in the same position. These actions validate also that any state traversed by $P_{\Lambda}$ solutions is {\em consistent} with the {\em state-invariants} $\Phi$. The definition $\mathsf{apply_{\xi,\omega}}$ actions is also more compact in our compilation that the one previously proposed by~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning} since are not using disjunctions to code the possible preconditions of an action schema.
\end{enumerate}
\end{itemize}

\begin{small}
\begin{align*}
\pre(\mathsf{apply_{\xi,\omega}})=&\{\neg mode_{inval}\},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{pre\_p\_\xi\wedge eff\_p\_\xi\}\rhd\{\neg p(\omega)\}_{\forall p\in{\mathcal I}_{\Psi,\xi}},\\
&\{\neg pre\_p\_\xi \wedge eff\_p\_\xi\}\rhd\{p(\omega)\}_{\forall p\in{\mathcal I}_{\Psi,\xi}}\},\\
&\{pre\_p\_\xi \wedge \neg p(\omega)\}\rhd\{mode_{inval}\}_{\forall p\in{\mathcal I}_{\Psi,\xi}},\\
&\{\neg\phi\}\rhd\{mode_{inval}\}_{\forall \phi\in\Phi},\\
&\{\emptyset\}\rhd\{\neg mode_{insert}\},
\end{align*}
\end{small}

\subsection{Pruning inconsistent action models with {\em domain mutex}}
Our approach to implement this pruning is extending the conditional effects of the $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertPre_{p,\xi}}$ actions (i.e., the actions that determine a solution model $\mathcal{M}$) with extra conditional effects indicating that the programmed model is {\em invalid} (i.e., inconsistent with a {\em domain mutex} in $\Phi$). Note that this {\em consistency} checking is more effective than the one implemented at the $\mathsf{apply_{\xi,\omega}}$ actions since $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertPre_{p,\xi}}$ actions appear at an earlier stage of the planning process.

Formally, given a {\em domain mutex} $(p,q)$, s.t. both $p$ and $q$ belong to $\in{\mathcal I}_{\Psi,\xi}$, we extend the actions for setting a precondition $p$ in a given action schema $\xi$ as follows:

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertPre_{p,\xi}})=&\{\neg pre_{p}(\xi), \neg eff_{p}(\xi),\\
&mode_{insert},\neg mode_{inval}\},\\
\cond(\mathsf{insertPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre_{p}(\xi)\},\\
&\{pre_{q}(\xi)\}\rhd\{mode_{inval}\}.
\end{align*}
\end{small}
The same procedure is applied for action $insertPre_{q,\xi}$ to ban programming precondition $q$ iff $pre_{p}(\xi)$ precondition is already set. A similar procedure is also applied to $\mathsf{insertEff_{p,\xi}}$ and $\mathsf{insertEff_{q,\xi}}$ actions for banning in this case, two {\em negative effects} (or two {\em positive effects}) that are {\em domain mutex}. Now we show the actions that ban programming a positive (or negative) $p$ effect if its corresponding $q$ effect is already programmed:

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertEff_{p,\xi}})=&\{\neg eff_{p}(\xi), mode_{insert},\neg mode_{inval}\},\\
\cond(\mathsf{insertEff_{p,\xi}})=&\{\emptyset\}\rhd\{eff_{p}(\xi),\\
&\{pre_{q}(\xi),eff_{q}(\xi),pre_{p}(\xi)\}\rhd\{mode_{inval}\},\\
&\{\neg pre_{q}(\xi),eff_{q}(\xi),\neg pre_{p}(\xi)\}\rhd\{mode_{inval}\}.
\end{align*}
\end{small}

\subsection{The bias of the initially {\em empty} action model}
Classical planners tend to preffer shorter solution plans, so our compilation may introduce a bias to $P=\tup{F,A[\cdot],I,G}$ problems preferring solutions that are referred to action models with a shorter number of {\em preconditions}/{\em effects}. In more detail, all $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ fluents are false at the initial state of our $P'=\tup{F',A',I,G}$ compilation so classical planners tend to solve $P'$ with plans that require a shorter number of {\em insert} actions.

This bias could be eliminated defining a cost function for the actions in $A'$ (e.g. {\em insert} actions has {\em zero cost} while $\mathsf{apply_{\xi,\omega}}$ actions has a {\em positive constant cost}). In practice we use a different approach to disregard the cost of {\em insert} actions because classical planners are not proficiency optimizing {\em plan cost} with zero-cost actions. Instead, our approach is to use a SAT-based planner~\cite{rintanen2014madagascar} because it can apply all actions for inserting preconditions in a single planning step (these actions do not interact). Further, the actions for inserting action effects are also applied in a single planning step so the plan horizon for programming any action model is always bound to 2, which significantly reduces the planning horizon.

Our compilation for {\em planning with unknown domain models} can then be understood as an extension of the SATPLAN approach for classical planning~\cite{kautz1992planning} with two additional initial layers: a first layer for inserting the action preconditions and a second one for inserting the action effects. These two extra layers are followed by the typical $N$ layers of the SATPLAN encoding (extended however to apply the action models that are determined by the previous two initial layers, the $\mathsf{apply_{\xi,\omega}}$ actions). Regarding again the example of Figure~\ref{fig:plan-lplan}, this means that steps [00-04] are applied in paralel in the first SATPLAN layer, steps [05-13] are applied in paralel in the second layer and each step [14-17] is applied sequentially and correponds to a differerent SATPLAN layer (so just six layers are necesary to compute the example plan of Figure~\ref{fig:plan-lplan}).

The SAT-based planning approach is also convenient because its ability to deal with classical planning problems populated with dead-ends and because symmetries in the insertion of preconditions/effects into an action model do not affect to the planning performance. 


\subsection{Compilation properties}
\begin{mylemma}
Soundness. Any classical plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$ produces a \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi}$ learning task.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
According to the $P_{\Lambda}$ compilation, once a given precondition or effect is inserted into the action model $\mathcal{M}$ it cannot be removed back. In addition, once the action model $\mathcal{M}$ is applied it cannot be {\em reprogrammed}. In the compiled planning problem $P_{\Lambda}$, the value of the original fluents $F$ can exclusively be modified via $\mathsf{apply_{\xi,\omega}}$ actions.  Therefore, the goals of the original $P$ classical planning task can only be achieved executing an applicable sequence of $\mathsf{apply_{\xi,\omega}}$ actions that, starting in the corresponding initial state $I=s_0$ reach a state $G \subseteq s_n$ validating that every generated intermediate state $s_i$, s.t. $0\leq i\leq n$, is consistent with the input {\em state-invariants}. This is exactly the definition of the solution condition for an action model $\mathcal{M}$ to solve the $\Lambda=\tup{P,\Phi,M}$ learning task. 
\end{small}
\end{proof}

\begin{mylemma}
Completeness. Any \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi,M}$ learning task can be computed with a classical plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
By definition ${\mathcal I}_{\Psi,\xi}$ fully captures the set of elements that can appear in a \strips\ action schema $\xi$ using predicates $\Psi$. In addition the $P_{\Lambda}$ compilation does not discard any possible action model $\mathcal{M}$ definable within ${\mathcal I}_{\Psi,\xi}$ while it can satisfy the domain mutex in $\Phi$. This means that for every \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi,M}$, we can build a plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$ by selecting the appropriate $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertEff_{p,\xi}}$ actions for {\em programming} the precondition and effects of the corresponding action model $\mathcal{M}$ and then, selecting the corresponding $\mathsf{apply_{\xi,\omega}}$ actions that transform the initial state $I$ into a state that satisfies the goals $G$.
\end{small}
\end{proof}

The size of the classical planning task $P_{\Lambda}$ output by our compilation depends on the arity of the given {\em predicates} $\Psi$, that shape the propositional state variables $F$, and the number of parameters of the action models, $|pars(\xi)|$. The larger these arities, the larger $|{\mathcal I}_{\Psi,\xi}|$. The size of the ${\mathcal I}_{\Psi,\xi}$ set is the term that dominates the compilation size because it defines the $pre\_e\_\xi/eff\_e\_\xi$ fluents, the corresponding set of {\em insert} actions, and the number of conditional effects in the $\mathsf{apply_{\xi,\omega}}$ actions. Note that {\em typing} can be used straightforward to constrain the FOL interpretations of $\Psi$ over the parameters $pars(\xi)$ which significantly reduces $|{\mathcal I}_{\Psi,\xi}|$ and hence, the size of the classical planning task output by the compilation.


\section{Learning from observations of plan executions}
\label{sec:observations}
Inductive approaches for the learning of planning action models compute an action model starting from an input set of observations of plan executions. This section provides a formal model for such input observations and shows how to leverage {\em state-invariants} to automatically {\em complete} those input observations. The section ends with the extension of our compilation to exploit the {\em completed} observations for the learning of \strips\ action models.








\section{Evaluation}
\label{sec:evaluation}

\section{Related work}
\label{sec:related}
{\em State-invariants} have been previously used to infer a HTN lanning model~\cite{lotinac2016constructing}.

In {\em Inductive Logic Programming} it is very common to make the hypothesis be consistent with some form deductive knowledge apart from the examples, what is usually called {\em background knowledge}~\cite{muggleton1994inductive}.



\section{Conclusions}
\label{sec:conclusions}
In some contexts it is however reasonable to assume that the action model is not learned from scratch, e.g. because some parts of the action model are known~\cite{ZhuoNK13,sreedharan2018handling,pereira2018heuristic}. Our compilation approach is also flexible to this particular learning scenario. The known preconditions and effects are encoded setting the corresponding fluents $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$ to true in the initial state. Further, the corresponding insert actions, $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertEff_{p,\xi}}$, become unnecessary and are removed from $A_{\Lambda}$, making the classical planning task $P_{\Lambda}$ easier to be solved. For example, suppose that the preconditions of the {\em blocksworld} action schema {\tt stack} are known, then the initial state $I$ is extended with literals, {\small\tt(pre\_holding\_v1\_stack)} and {\small\tt(pre\_clear\_v2\_stack)} and the associated actions $\mathsf{insertPre_{holding_v1,stack}}$ and $\mathsf{insertPre_{clear_v2,stack}}$ can be safely removed from the $A_{\Lambda}$ action set without altering the {\em soundness} and {\em completeness} of the $P_{\Lambda}$ compilation.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{planlearnbibliography}

\end{document}
