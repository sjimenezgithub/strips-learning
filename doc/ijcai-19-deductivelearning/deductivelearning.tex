%%%% ijcai19.tex

\typeout{IJCAI-19 Instructions for Authors}

% These are the instructions for authors for IJCAI-19.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai19.sty is NOT the same than previous years'
\usepackage{ijcai19}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\urlstyle{same}

%%%%%%%%%%%%%%%%%% Added for this paper
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{listings}% http://ctan.org/pkg/listings
\lstset{
  basicstyle=\ttfamily,
  mathescape
}
\usepackage{ wasysym }
\newcommand{\tup}[1]{{\langle #1 \rangle}}
\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{mydefinition}[mytheorem]{Definition}

%%%%%%%%%%%%%





% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning \strips\ action models from {\em state-invariants}}

% Single author syntax
%\author{
%    Sarit Kraus
%    \affiliations
%    Department of Computer Science, Bar-Ilan University, Israel \emails
%    pcchair@ijcai19.org
%}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% Check the ijcai19-multiauthor.tex file for detailed instructions
\author{
Diego Aineto$^1$\and
Sergio Jim\'enez$^1$\and
Eva Onaindia$^1$
\affiliations
$^1${\small Departamento de Sistemas Inform\'aticos y Computaci\'on. Universitat Polit\`ecnica de Val\`encia. Valencia, Spain}
\emails
{\scriptsize \{dieaigar,serjice,onaindia\}@dsic.upv.es}}


\begin{document}
\maketitle


\begin{abstract}
This paper addresses the learning of action models from {\em state-invariants} (i.e logic formulae that specify constraints about the possible states of a given domain). The benefit of exploiting {\em state-invariants} is two-fold, they allow to reduce the space of possible action models and to complete learning examples that are only partially observed. Our approach for the learning of \strips\ action models from {\em state-invariants} is a {\em classical planning} compilation. The compilation is flexible to different kinds of input knowledge (e.g., partially observations of plan executions including partially observed intermediate states and/or actions) and outputs an action model that is {\em consistent} with the given input knowledge. The experimental results show that, even at unfavorable scenarios where input observations are minimal (just an {\em initial state} and the {\em goals}), {\em state-invariant} are helpful to learn good quality \strips\ action models.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

The specification of planning action models is a complex process that limits, too often, the application of {\em model-based planning} systems to real-world tasks~\cite{kambhampati:modellite:AAAI2007}. The {\em machine learning} of action models can relieve the {\em knowledge acquisition bottleneck} of planning and nowadays, there exists a wide range of effective approaches for learning action models~\cite{arora:amodels:ker2018}. Many of the most successful approaches for learning planning action models are however purely {\em inductive}~\cite{yang2007learning,pasula2007learning,mourao2010learning,zhuo2013action}, meaning that their performance is linked to the {\em amount} and {\em quality} of the input learning examples. 

This paper addresses the learning of action models exploiting a different source of knowledge, {\em deductive} knowledge, to cushion the negative impact of insufficient learning examples. In more detail our approach leverages {\em state-invariants}, i.e. logic formulae that specify constraints about the possible states of a given domain. Given an action model, state-of-the-art planners are used to infer {\em state-invariants} from that model to reduce search spaces and make the planning process more efficient~\cite{helmert2009concise}. In this paper we follow the opposite direction and leverage {\em state-invariants} to learn the planning action model. The benefit of learning \strips\ action models from {\em state-invariants} is two-fold, {\em state-invariants} allow us to reduce the space of possible action models and to complete learning examples that are only partially observed.

Our approach for learning \strips\ action models from {\em state-invariants} is compile the learning task into a classical planning task. Our compilation is flexible to different kinds of input knowledge (e.g., partially/fully observations of actions of plan executions as well as partially/fully observed intermediate states) and outputs an action model that is {\em consistent} with the given input knowledge. The experimental results show that, even at unfavorable scenarios where input observations are minimal (just an {\em initial state} and the {\em goals}), {\em state-invariant} help to learn better \strips\ models than with the existing {\em classical planning} compilation by~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning}. 



\section{Background}
\label{sec:background}
This section formalizes the {\em classical planning model} we follow in this work and the kind of {\em knowledge} that can be given as input to the task of learning \strips\ action models.  

\subsection{Classical planning with conditional effects}
Let $F$ be the set of  propositional state variables ({\em fluents}) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$; i.e. either~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (without loss of generality, we will assume that $L$ does not contain conflicting values). Given $L$, let $\neg L=\{\neg l:l\in L\}$ be its complement. We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents. A {\em state} $s$ is a full assignment of values to fluents; $|s|=|F|$.

A {\em classical planning action} $a\in A$ has: a precondition $\pre(a)\in\mathcal{L}(F)$, a set of effects $\eff(a)\in\mathcal{L}(F)$, and a positive action cost $cost(a)$. The semantics of actions $a\in A$ is specified with two functions: $\rho(s,a)$ denotes whether action $a$ is {\em applicable} in a state $s$ and $\theta(s,a)$ denotes the {\em successor state} that results of applying action $a$ in a state $s$. Then, $\rho(s,a)$ holds iff $\pre(a)\subseteq s$, i.e.~if its precondition holds in $s$. The result of executing an applicable action $a\in A$ in a state $s$ is a new state $\theta(s,a)=(s\setminus \neg\eff(a))\cup\eff(a)$. Subtracting the complement of $\eff(a)$ from $s$ ensures that $\theta(s,a)$ remains a well-defined state. The subset of action effects that assign a positive value to a state fluent is called {\em positive effects} and denoted by $\eff^+(a)\in \eff(a)$ while $\eff^-(a)\in \eff(a)$ denotes the {\em negative effects} of an action $a\in A$.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is the initial state and $G\in\mathcal{L}(F)$ is the set of goal conditions over the state variables. A {\em plan} $\pi$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$, with $|\pi|=n$ denoting its {\em plan length} and $cost(\pi)=\sum_{a\in\pi} cost(a)$ its {\em plan cost}. The execution of $\pi$ on the initial state of $P$ induces a {\em trajectory} $\tau(\pi,P)=\tup{s_0, a_1, s_1, \ldots, a_n, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, it holds $\rho(s_{i-1},a_i)$ and $s_i=\theta(s_{i-1},a_i)$. A plan $\pi$ solves $P$ iff the induced {\em trajectory} $\tau(\pi,P)$ reaches a final state $G \subseteq s_n$, where all goal conditions are met. A solution plan is {\em optimal} iff its cost is minimal.

We also define {\em actions with conditional effects} because they are useful to compactly formulate our approach for {\em goal recognition with unknown domain models}. An action $a_c\in A$ with conditional effects is a set of preconditions $\pre(a_c)\in\mathcal{L}(F)$ and a set of {\em conditional effects} $\cond(a_c)$. Each conditional effect $C\rhd E\in\cond(a_c)$ is composed of two sets of literals: $C\in\mathcal{L}(F)$, the {\em condition}, and $E\in\mathcal{L}(F)$, the {\em effect}. An action $a_c$ is applicable in a state $s$ if $\rho(s,a_c)$ is true, and the result of applying action $a_c$ in state $s$ is $\theta(s,a_c)=\{s\setminus\neg\eff_c(s,a)\cup\eff_c(s,a)\}$ where $\eff_c(s,a)$ are the {\em triggered effects} resulting from the action application (conditional effects whose conditions hold in $s$):
\[
\eff_c(s,a)=\bigcup_{C\rhd E\in\cond(a_c),C\subseteq s} E,
\]

\subsection{State-invariants}
The notion of {\em state-constraint} is very general and has been used in different areas of AI and for different purposes.  If we restrict ourselves to planning, {\em state-constraints} are abstractions for compactly specifying sets of states. For instance, {\em state-constraints} in planning allow to specify the set of states where a given action is applicable, the set of states where a given {\em derived predicate} holds or the set of states that are considered goal states.

{\em State invariants} is a kind of state-constraints useful for computing more compact state representations~\cite{helmert2009concise} or making {\em satisfiability planning} and {\em backward search} more efficient~\cite{rintanen2014madagascar,alcazar2015reminder}. Given a classical planning problem $P=\tup{F,A,I,G}$, a {\em state invariant} is a formula $\phi$ that holds at the initial state of a given classical planning problem, $I\models \phi$, and at every state $s$, built from $F$, that is reachable from $I$ by applying actions in $A$. For instance Figure~\ref{fig:strongest-invariant} shows five clauses that define the {\em state invariants} for the {\em blocksworld} planning domain~\cite{slaney2001blocks}. There are infinitely many strongest invariants, but they are all logically equivalent, and computing the strongest invariant is PSPACE-hard (as hard as testing plan existence~\cite{bylander:complexity:AIJ1994}).

\begin{figure}[hbt!]
  \begin{footnotesize}
$\forall x_1,x_2\ ontable(x_1)\leftrightarrow\neg on(x_1,x_2)$.\\
$\forall x_1,x_2\ clear(x_1)\leftrightarrow\neg on(x_2,x_1)$.\\
$\forall x_1,x_2,x_3\ \neg on(x_1,x_2)\vee\neg on(x_1,x_3)\ such\ that\ x_2\neq x_3$.\\
$\forall x_1,x_2,x_3\ \neg on(x_2,x_1)\vee\neg on(x_3,x_1)\ such\ that\ x_2\neq x_3$.\\
$\forall x_1,\ldots,x_n\ \neg(on(x_1,x_2)\wedge on(x_2,x_3)\wedge\ldots\wedge on(x_{n-1},x_n)\wedge on(x_n,x_1)).$
\end{footnotesize}
 \caption{\small Example of {\em state-invariants} for the {\em blocksworld} domain.}
\label{fig:strongest-invariant}
\end{figure}

A {\em mutex} (mutually exclusive) is a state invariant that takes the form of a binary clause and indicates a pair of different properties that cannot be simultaneously true~\cite{kautz:mutex:IJCAI1999}. For instance in a three-block {\em blocksworld}, $\neg on(block_A,block_B)\vee \neg on(block_A,block_C)$ is a {\em mutex} because $block_A$ can only be on top of a single block.

A {\em domain invariant} is an instance-independent invariant, i.e. holds for any possible initial state and set of objects. Therefore, if a given state $s$ holds $s\nvDash \phi$ such that $\phi$ is a {\em domain invariant}, it means that $s$ is not a valid state. Domain invariants are often compactly defined as {\em lifted invariants} (also called schematic invariants)~\cite{rintanen:schematicInvariants:AAAI2017}. For instance, $\forall x:\ (\neg handempty\vee \neg holding(x))$, is a {\em domain mutex} for the {\em blocksworld} because the robot hand is never empty and holding a block at the same time.



\section{Learning \strips\ action models from {\em state-invariants}}
\label{sec:learning}
We define the task of learning a planning action model from {\em state-invariants} as a tuple $\Lambda=\tup{P,\Phi,M}$, where:
\begin{itemize}
\item $P=\tup{F,A[\cdot],I,G}$, is a {\em classical planning problem} where $A[\cdot]$ is a set of actions s.t., the {\em dynamics} of each action $a\in A[\cdot]$ is {\em unknown} (i.e. functions $\rho$ and/or $\theta$ are undefined for $a\in A[\cdot]$).
\item $\Phi$ is a set of {\em state-invariants} that define constraints about the set of possible states in the previous planning problem $P$.
\item $M$ is the {\em space of possible action models} for the $A[\cdot]$ actions (i.e., the set of possible specifications of the $\rho$ and/or $\theta$ functions for each $a\in A[\cdot]$ action).
\end{itemize}

We say that a given model $\mathcal{M}\in M$ is a {\em solution} to the $\Lambda=\tup{P,\Phi,M}$ learning task iff there exists a plan $\pi$ that solves $P=\tup{F,A[\cdot],I,G}$, when the semantics of each action $a\in A[\cdot]$ is given by $\mathcal{M}$, and such that any state traversed by a trajectory $\tau(\pi,P)$ is {\em consistent} with the input set of {\em state-invariants} $\Phi$.

Next, we show that the set $M$, of possible action models, can be compactly encoded as a set of propositional variables and a set of constraints over those variables. Then, we show how to exploit this compact encoding to solve a $\Lambda=\tup{P,\Phi,M}$ learning task with an off-the-shelf classical planner.


\subsection{A propositional encoding for the space of STRIPS action models}
{\em A \strips\ action schema} $\xi$ is defined by four lists: A list of {\em parameters} $pars(\xi)$, and three list of predicates (namely $pre(\xi)$, $del(\xi)$ and $add(\xi)$) that shape the kind of fluents that can appear in the {\em preconditions}, {\em negative effects} and {\em positive effects} of the actions induced from that schema. Let be $\Psi$ the set of {\em predicates} that shape the propositional state variables $F$, and a list of {\em parameters}, $pars(\xi)$. The set of elements that can appear in $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of the \strips\ action schema $\xi$ is the set of FOL interpretations of $\Psi$ over the parameters $pars(\xi)$ and is denoted as ${\mathcal I}_{\Psi,\xi}$.

For instance in a four-operator {\em blocksworld}~\cite{slaney2001blocks}, the ${\mathcal I}_{\Psi,\xi}$ set contains only five elements for the {\small \tt pickup($v_1$)} schemata, ${\mathcal I}_{\Psi,pickup}$={\small\tt\{handempty, holding($v_1$), clear($v_1$), ontable($v_1$), on($v_1,v_1$)\}} while it contains eleven elements for the {\small \tt stack($v_1$,$v_2$)} schemata, ${\mathcal I}_{\Psi,stack}$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$), clear($v_2$), ontable($v_1$), ontable($v_2$), on($v_1,v_1$), on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. 

Despite any element of ${\mathcal I}_{\Psi,\xi}$ can {\em a priori} appear in the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ of schema $\xi$, in practice the actual space of possible \strips\ schemata is bounded by constraints of two kinds:
\begin{enumerate}
\item {\bf Syntactic constraints}. \strips\ constraints require $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$. Considering exclusively these syntactic constraints, the size of the space of possible \strips\ schemata is given by $2^{2\times|{\mathcal I}_{\Psi,\xi}|}$. {\em Typing constraints} are also of this kind~\cite{mcdermott1998pddl}. 
\item {\bf Observation constraints}. The observation of the actions and states resulting from the execution of a plan depicts {\em semantic knowledge} that constraints further the space of possible action schemata.   
\end{enumerate}

In this work we introduce a propositional encoding of the {\em preconditions}, {\em negative}, and {\em positive} effects of a \strips\ action schema $\xi$ using only fluents of two kinds {\tt\small pre\_e\_$\xi$} and {\tt\small eff\_e\_$\xi$} (where $e\in{\mathcal I}_{\Psi,\xi}$). This encoding exploits the syntactic constraints of \strips\, so it is more compact that the one previously proposed by~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning} for learning \strips\ action models with classical planning. In more detail, if {\tt\small pre\_e\_$\xi$} holds it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em precondition} in $\xi$. If {\tt\small pre\_e\_$\xi$} and {\tt\small eff\_e\_$\xi$} holds it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em negative effect} in $\xi$ while if $pre\_e\_\xi$ does not hold but {\tt\small eff\_e\_$\xi$} holds, it means that $e\in{\mathcal I}_{\Psi,\xi}$ is a {\em positive effect} in $\xi$. Figure~\ref{fig:propositional} shows the PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema using the {\tt\small pre\_e\_stack} and {\tt\small eff\_e\_stack} fluents ($e\in{\mathcal I}_{\Psi,stack}$).

\begin{figure}
  \begin{tiny}  
  \begin{verbatim}
(:action stack
   :parameters (?v1 ?v2)
   :precondition (and (holding ?v1) (clear ?v2))
   :effect (and (not (holding ?v1)) (not (clear ?v2))
                (clear ?v1) (handempty) (on ?v1 ?v2)))


(pre_holding_v1_stack) (pre_clear_v2_stack)
(eff_holding_v1_stack) (eff_clear_v2_stack)
(eff_clear_v1_stack) (eff_handempty_stack) (eff_on_v1_v2_stack)
  \end{verbatim}           
  \end{tiny}  
 \caption{\small PDDL encoding of the {\tt\small stack(?v1,?v2)} schema and our propositional representation for this same schema.}
\label{fig:propositional}
\end{figure}

In addition, one can introduce {\em domain-specific knowledge} to constrain further the space of possible schemata. For instance, in the {\em blocksworld} one can argue that {\small\tt on($v_1$,$v_1$)} and {\small\tt on($v_2$,$v_2$)} will not appear in the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ lists of an action schema $\xi$ because, in this specific domain, a block cannot be on top of itself. {\it State invariants} are {\em domain-specific knowledge} and they can be seen as either {\em syntactic} or {\em semantic} constraints because on the one hand, they constrain the space of possible action models but on the other hand, they can be used to complete partial observations of the states traversed by a plan. 


\subsection{Learning \strips\ action models with classical planning}
Our approach for computing an action model $\mathcal{M}\in M$ that solves the $\Lambda=\tup{P,\Phi,M}$ learning task is to build and solve a classical planning problem $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I,G_{\Lambda}}$ such that:
\begin{itemize}
\item $F_{\Lambda}$ extends $F$ with a fluent {\small$mode_{inval}$}, to indicate whether an action model is {\em inconsistent} with the input {\em state-invariants} $\Phi$, a fluent {\small$mode_{insert}$}, to indicate whether action models are being programmed, and the fluents for the propositional encoding of the corresponding space of STRIPS action models. As explained, this is a set of fluents of the type $\{pre\_e\_\xi, eff\_e\_\xi\}_{\forall e\in{\mathcal I}_{\Psi,\xi}}$.

\item $G_{\Lambda}= G\cup \{\neg mode_{inval}\}$ extends the original goals $G$ with the $\neg mode_{inval}$ literal to validate that only states {\em consistent} with the state constraints $\Phi$ are traversed by $P_{\Lambda}$ solutions.

\item $A_{\Lambda}$ replaces the actions in $A$ with two types of actions.
\begin{enumerate}
\item Actions for {\em inserting} a {\em precondition}, {\em positive} effect or {\em negative} effect in $\xi$ following the syntactic constraints of \strips\ models. 
\begin{itemize}
\item Actions which support the addition of a {\em precondition} $p\in {\mathcal I}_{\Psi,\xi}$ to the action model $\xi$. A precondition $p$ is inserted in $\xi$ when neither $pre_p$, $eff_p$ exist in $\xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertPre_{p,\xi}})=&\{\neg pre\_p\_\xi, \neg eff\_p\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre\_p\_\xi\}.
\end{align*}
\end{small}

\item Actions which support the addition of a {\em negative} or {\em positive} effect $p\in {\mathcal I}_{\Psi,\xi}$ to the action model $\xi$. 

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertEff_{p,\xi}})=&\{\neg eff\_p\_\xi, mode_{insert}\},\\
\cond(\mathsf{insertEff_{p,\xi}})=&\{\emptyset\}\rhd\{eff\_p\_\xi\}.
\end{align*}
\end{small}
\end{itemize}

\item Actions for {\em applying} an action model $\xi$ built by the {\em insert} actions and bounded to objects $\omega\subseteq\Omega^{|pars(\xi)|}$ (where $\Omega$ is the set of {\em objects} used to induce the fluents $F$ by assigning objects in $\Omega$ to the $\Psi$ predicates, and $\Omega^k$ is the $k$-th Cartesian power of $\Omega$). The action parameters, $pars(\xi)$, are bound to the objects in $\omega$ that appear in the same position. These actions validate also that any state traversed by $P_{\Lambda}$ solutions is {\em consistent} with the {\em state-invariants} $\Phi$. The definition $\mathsf{apply_{\xi,\omega}}$ actions is also more compact in our compilation that the one previously proposed by~\citeauthor{aineto2018learning}~\citeyear{aineto2018learning} since are not using disjunctions to code the possible preconditions of an action schema.
\end{enumerate}
\end{itemize}

\begin{small}
\begin{align*}
\pre(\mathsf{apply_{\xi,\omega}})=&\{\neg mode_{inval}\},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{pre\_p\_\xi\wedge eff\_p\_\xi\}\rhd\{\neg p(\omega)\}_{\forall p\in{\mathcal I}_{\Psi,\xi}},\\
&\{\neg pre\_p\_\xi \wedge eff\_p\_\xi\}\rhd\{p(\omega)\}_{\forall p\in{\mathcal I}_{\Psi,\xi}}\},\\
&\{pre\_p\_\xi \wedge \neg p(\omega)\}\rhd\{mode_{inval}\}_{\forall p\in{\mathcal I}_{\Psi,\xi}},\\
&\{\neg\phi\}\rhd\{mode_{inval}\}_{\forall \phi\in\Phi},\\
&\{\emptyset\}\rhd\{\neg mode_{insert}\},
\end{align*}
\end{small}

\subsection{Pruning inconsistent action models with {\em domain mutex}}
We define a {\em domain mutex} as a $(p,q)$ predicates pair where both $p\in\Psi$ and $q\in\Psi$ are predicates that shape the set of fluents $F$ of a given planning problem and such that they satisfy the following formulae $p\leftrightarrow \neg q$ where are the predicate variables are universally quantified. For instance, predicates $holding(x)$ and $clear(x)$ from the {\em blocksworld} are {\em domain mutex} since they satisfy $\forall x\ holding(x)\leftrightarrow\neg clear(x)$ while predicates $clear(x)$ and $ontable(x)$ (also from the {\em blocksworld}) are not {\em domain mutex} because they do not always satisfy $\forall x\ clear(x)\leftrightarrow\neg ontable(x)$.

We pay attention to this particular class of {\em state-invariants} because they define the {\em state-properties} of a given type of objects~\cite{fox:TIM:JAIR1998} and because they enable an effectively pruning of inconsistent \strips\ action models. Our approach to implement this pruning is extending the conditional effects of the $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertPre_{p,\xi}}$ actions (i.e., the actions that determine a solution model $\mathcal{M}$) with extra conditional effects indicating that the programmed model is {\em invalid} (i.e., inconsistent with a {\em domain mutex} in $\Phi$). Note that this {\em consistency} checking is more effective than the one implemented at the $\mathsf{apply_{\xi,\omega}}$ actions since $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertPre_{p,\xi}}$ actions appear at an earlier stage of the planning process.

Formally, given a {\em domain mutex} $(p,q)$, s.t. both $p$ and $q$ belong to $\in{\mathcal I}_{\Psi,\xi}$, we extend the actions for setting a precondition $p$ in a given action schema $\xi$ as follows:

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertPre_{p,\xi}})=&\{\neg pre_{p}(\xi), \neg eff_{p}(\xi),\\
&mode_{insert},\neg mode_{inval}\},\\
\cond(\mathsf{insertPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre_{p}(\xi)\},\\
&\{pre_{q}(\xi)\}\rhd\{mode_{inval}\}.
\end{align*}
\end{small}
The same procedure is applied for action $insertPre_{q,\xi}$ to ban programming precondition $q$ iff $pre_{p}(\xi)$ precondition is already set. A similar procedure is also applied to $\mathsf{insertEff_{p,\xi}}$ and $\mathsf{insertEff_{q,\xi}}$ actions for banning in this case, two {\em negative effects} (or two {\em positive effects}) that are {\em domain mutex}. Now we show the actions that ban programming a positive (or negative) $p$ effect if its corresponding $q$ effect is already programmed:

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{insertEff_{p,\xi}})=&\{\neg eff_{p}(\xi), mode_{insert},\neg mode_{inval}\},\\
\cond(\mathsf{insertEff_{p,\xi}})=&\{\emptyset\}\rhd\{eff_{p}(\xi),\\
&\{pre_{q}(\xi),eff_{q}(\xi),pre_{p}(\xi)\}\rhd\{mode_{inval}\},\\
&\{\neg pre_{q}(\xi),eff_{q}(\xi),\neg pre_{p}(\xi)\}\rhd\{mode_{inval}\}.
\end{align*}
\end{small}

\subsection{Compilation properties}
\begin{mylemma}
Soundness. Any classical plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$ produces a \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi,M}$ learning task.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
According to the $P_{\Lambda}$ compilation, once a given precondition or effect is inserted into the action model $\mathcal{M}$ it cannot be removed back. In addition, once the action model $\mathcal{M}$ is applied it cannot be {\em reprogrammed}. In the compiled planning problem $P_{\Lambda}$, the value of the original fluents $F$ can exclusively be modified via $\mathsf{apply_{\xi,\omega}}$ actions.  Therefore, the goals of the original $P$ classical planning task can only be achieved executing an applicable sequence of $\mathsf{apply_{\xi,\omega}}$ actions that, starting in the corresponding initial state $I=s_0$ reach a state $G \subseteq s_n$ validating that every generated intermediate state $s_i$, s.t. $0\leq i\leq n$, is consistent with the input {\em state-invariants}. This is exactly the definition of the solution condition for an action model $\mathcal{M}$ to solve the $\Lambda=\tup{P,\Phi,M}$ learning task. 
\end{small}
\end{proof}

\begin{mylemma}
Completeness. Any \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi,M}$ learning task can be computed with a classical plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$.
\end{mylemma}

\begin{proof}[Proof]
\begin{small}
By definition ${\mathcal I}_{\Psi,\xi}$ fully captures the set of elements that can appear in a \strips\ action schema $\xi$ using predicates $\Psi$. In addition the $P_{\Lambda}$ compilation does not discard any possible action model $\mathcal{M}$ definable within ${\mathcal I}_{\Psi,\xi}$ while it can satisfy the domain mutex in $\Phi$. This means that for every \strips\ model $\mathcal{M}$ that solves the $\Lambda=\tup{P,\Phi,M}$, we can build a plan $\pi_{\Lambda}$ that solves $P_{\Lambda}$ by selecting the appropriate $\mathsf{insertPre_{p,\xi}}$ and $\mathsf{insertEff_{p,\xi}}$ actions for {\em programming} the precondition and effects of the corresponding action model $\mathcal{M}$ and then, selecting the corresponding $\mathsf{apply_{\xi,\omega}}$ actions that transform the initial state $I$ into a state that satisfies the goals $G$.
\end{small}
\end{proof}

The size of the classical planning task $P_{\Lambda}$ output by our compilation depends on the arity of the given {\em predicates} $\Psi$, that shape the propositional state variables $F$, and the number of parameters of the action models, $|pars(\xi)|$. The larger these arities, the larger $|{\mathcal I}_{\Psi,\xi}|$. The size of the ${\mathcal I}_{\Psi,\xi}$ set is the term that dominates the compilation size because it defines the $pre\_e\_\xi/eff\_e\_\xi$ fluents, the corresponding set of {\em insert} actions, and the number of conditional effects in the $\mathsf{apply_{\xi,\omega}}$ actions. Note that {\em typing} can be used straightforward to constrain the FOL interpretations of $\Psi$ over the parameters $pars(\xi)$ which significantly reduces $|{\mathcal I}_{\Psi,\xi}|$ and hence, the size of the classical planning task output by the compilation.


\section{Learning from observations of plan executions}
\label{sec:observations}
Inductive approaches for the learning of planning action models compute an action model starting from an input set of observations of plan executions. This section provides a formal model for such input observations and shows how to leverage {\em state-invariants} to automatically {\em complete} those input observations. The section ends with the extension of our compilation to exploit the {\em completed} observations for the learning of \strips\ action models.

\subsection{The observation model}
Given a planning problem $P=\tup{F,A,I,G}$, a plan $\pi$ and a trajectory $\tau(\pi,P)$, we define the \emph{observation of the trajectory} as an interleaved combination of actions and states that represents the observation from the execution of $\pi$ in $P$. Formally, $\mathcal{O}(\tau)=\tup{s_0^o,a_1^o,s_1^o \ldots , a_l^o, s_m^o}$, $s_0^o=I$, and:

\begin{itemize}
\item The {\bf observed actions} are consistent with $\pi$, which means that $\tup{a_1^o, \ldots, a_l^o}$ is a sub-sequence of $\pi$. The number of observed actions, $l$, ranges from $0$ (fully unobserved action sequence) to $|\pi|$ (fully observed action sequence).
\item The {\bf observed states} $\tup{s_0^o, s_1^o, \ldots, s_m^o}$ is a sequence of possibly {\em partially observable states}, except for the initial state $s_0^o$, which is fully observed. A partially observable state $s_i^o$ is one in which $|s_i^o| < |F|$; i.e., a state in which at least a fluent of $F$ is not observable. Note that this definition also comprises the case $|s_i^o| = 0$, when the state is fully unobservable. Whatever the sequence of observed states of $\mathcal{O}(\tau)$ is, it must be consistent with the sequence of states of $\tau(\pi,P)$, meaning that $\forall i, s_i^o \subseteq s_i$. The number of observed states, $m$, range from 1 (the initial state, at least), to $|\pi|+1$, and each {\em observed} states comprises $[1,|F|]$ fluents (the observation can still miss intermediate states that are {\em unobserved}).
\end{itemize}

We assume a bijective monotone mapping between actions/states of trajectories and observations~\cite{ramirez2009plan}, thus also granting the inverse consistency relationship (the trajectory is a superset of the observation). Therefore, transiting between two consecutive observed states in $\mathcal{O}(\tau)$ may require the execution of more than a single action ($\theta(s_i^o,\tup{a_1,\ldots,a_k})=s_{i+1}^o$, where ${\small k\geq 1}$ is unknown but finite. In other words, having an input observation $\mathcal{O}(\tau)$ does not imply knowing the actual length of $\pi$.

\subsection{Completing observations with {\em domain mutex}}
Our observation model follows the {\em open world} assumption, in other words, what is not observed is considered unknown. Here, we show that {\em state-invariants} are helpful to infer new knowledge that was unobserved.

Given a {\em domain mutex} $(p,q)$ and a state observation $s_j^o\in \mathcal{O}(\tau)$, {\small $(1\leq j\leq m)$}, such that the literal $p(\omega)\in s_j^o$ is an instantiation of predicate $p$ over some subset of objects $\omega\subseteq\Omega^{|pars(p)|}$ then, the state observation can be safely completed adding the new literal $\neg q(\omega)$ (despite $\neg q(\omega)$ was actually unobserved). For instance, if the literal {\tt\small holding(blockA)} is observed in a particular blocksword state and we have the {\em domain mutex} $\forall x\ holding(x)\leftrightarrow\neg clear(x)$ in the input set $\Phi$ of {\em state-invariants} we can safely add to the observation the literal {\tt\small $\neg$clear(blockA)} (despite this literal was actually unobserved). The process is repeated for all the observed states in $\mathcal{O}(\tau)$ and all the {\em domain mutex} in $\Phi$ to produce a new completed observation $\mathcal{O}(\tau)'$.

\subsection{Learning from completed observations with {\em classical planning}}
Let be $\mathcal{O}(\tau)'$ an observation completed as explained above, we extend here our compilation to constraint the possible \strips\ models with $\mathcal{O}(\tau)$':
\begin{itemize}
\item One fluent $\{observed_j\}_{0\leq j\leq m}$ to point at every $s_j^o\in\mathcal{O}(\tau)'$ state observation. Two fluents, $at_i$ and $next_{i,i+1}$, {\small $1\leq i \leq n$}, to iterate through the $n$ observed actions in $\mathcal{O}(\tau)'$. The former is used to ensure that actions are executed in the same order as they are observed. The latter is used to iterate to the next planning step when solving $P_{\Lambda}$.
\item Adding $at_1$ and $\{next_{i,i+1}\}$, {\small $1\leq i \leq n$} to the initial state and $observed_m$ to the goals $G$ of the classical planning problem and hence, constrain the solution plans to be consistent with all the state observations.
\item Adding the extra conditional effects {\small $\{at_{i},plan(name(a_i),\Omega^{pars(a_i)},i)\}\rhd\{\neg at_{i},at_{i+1}\}_{\forall i\in [1,n]}$} to the $\mathsf{apply_{\xi,\omega}}$ actions to ensure that actions are applied in the same order as they appear in $\mathcal{O}(\tau)'$.
\item Actions for {\em validating} the partially observed state $s_j^o\in\mathcal{O}(\tau)'$, {\tt\small $1\leq j< m$}. These actions are also part of the postfix of the solution plan $\pi_\Lambda$ and they are aimed at checking that the observable data of the input observation $\mathcal{O}(\tau)$' follows after the execution of the apply actions.
\item One $\mathsf{validate_{j}}$ action to constraint the solution plans to be consistent with the $s_j^o\in\mathcal{O}(\tau)'$ input state observation, {\small $(1\leq j\leq m)$}.  
\end{itemize}
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{j}})=&s_j^o\cup\{observed_{j-1}\},\\
\cond(\mathsf{validate_{j}})=&\{\emptyset\}\rhd\{\neg observed_{j-1}, observed_j\}.
\end{align*}
\end{small}

So far we explained the extension of the compilation for learning from a single observation $\mathcal{O}(\tau)'$. The extension to the more general case of a set of observation $\{\mathcal{O}(\tau_1),\ldots,\mathcal{O}(\tau_k)\}$ is implemented with a small modification. In particular, the actions in $P_{\Lambda}$ for {\em validating} the last state $s_m^o\in \mathcal{O}(\tau_t)$, {\tt\small $1\leq t\leq k$} reset also the current state and the current plan step. 


\section{Evaluation}
\label{sec:evaluation}

\section{Related work}
\label{sec:related}
{\em State-invariants} have been previously used to infer a HTN lanning model~\cite{lotinac2016constructing}.

In {\em Inductive Logic Programming} it is very common to make the hypothesis be consistent with some form deductive knowledge apart from the examples, what is usually called {\em background knowledge}~\cite{muggleton1994inductive}.



\section{Conclusions}
\label{sec:conclusions}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{planlearnbibliography}

\end{document}
