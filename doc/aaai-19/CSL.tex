\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{}
\setcounter{secnumdepth}{0}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{comment}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily,
  mathescape
}



\usepackage{multicol}
\usepackage{arydshln}
\usetikzlibrary{calc,backgrounds,positioning,fit}


\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}


\begin{document}

\title{A Classical Planning Model for Learning, Parsing and Production in Context-Sensitive Languages}
\author{\#39}


% Commented for blind submission
\author{Diego Aineto\and Sergio Jim\'enez\and Eva Onaindia\\
{\small Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\small Universitat Polit\`ecnica de Val\`encia.}\\
{\small Camino de Vera s/n. 46022 Valencia, Spain}\\
{\small \{dieaigar,serjice,onaindia\}@dsic.upv.es}}

\maketitle
\begin{abstract} 
This paper presents a novel approach for learning the acceptor automaton of a given {\em Context-Sensitive Language} (CSLs) from small sets of input strings. Our approach is to compile this learning task into a classical planning problem whose solutions are sequences of actions s.t. their prefix builds the acceptor automaton and their postfix validates that the built automaton accepts the input strings. The compilation is flexible to implement the three canonical tasks of Context-Sensitive Grammars (CSGs), {\it learning}, {\it parsing} and {\it production} within the same classical planning model. The paper reports the empirical data collected when learning the acceptor automata for the CSLs $\{a^nb^nc^n : n \geq 1 \}$ and $\{a^{2^n} : n \geq 1 \}$ and shows that our compilation outperforms a previous classical planning compilation for  learning {\em Context-Free Grammars}.
\end{abstract}


\section{Introduction}
\label{sec:section1}
A {\em formal grammar} is a set of symbols and production rules that describe how to form the possible strings of certain formal language~\cite{hopcroft:automatatheory:2001}. Usually three canonical tasks are defined over formal grammars:
\begin{itemize}
\item {\it Learning}: Given a set of strings, compute a grammar that is compliant with the input strings.
\item {\it Recognition} (also known as {\em parsing}): Given a formal grammar and a string, determine whether the string belongs to the language represented by the grammar.
\item {\it Production}: Given a formal grammar, generate a string that belongs to the language represented by the grammar.
\end{itemize}

Chomsky defined four types of formal grammars that differ in the form and generative capacity of their rules~\cite{chomsky2002syntactic}. Each grammar type generates a different class of formal language that is recognizable with a different kind of acceptor automaton: {\bf Type-0} corresponds to the {\em recursively enumerable} languages that can be recognized with a {\em Turing machine}. {\bf Type-1} corresponds to the {\em recursively enumerable} languages that can be recognized with a {\em linear-bounded Turing machine}. {\bf Type-2} corresponds to {\em context-free} languages that are recognizable with a {\em non-deterministic push-down automaton}. Finally, {\bf type-3} corresponds to {\em regular} languages that can be recognized with a {\em finite state machine}.

Figure~\ref{fig:csg}(a) shows an example of a CSG grammar ({\em Type-1}) that generates the $\{a^nb^nc^n : n \geq 1 \}$ language. The grammar defines ten production rules over three {\em terminal symbols} ($a$, $b$ and $c$) and five {\em non-terminal symbols} ($S$,$B$,$C$,$W$ and $Z$). This CSG produces the string $aaabbbccc$ applying the sequence of rules $\tup{1,1,0,2,3,4,5,2,3,4,5,2,3,4,5,6,7,7,8,9,9}$. The {\it parse tree} in Figure~\ref{fig:csg}(b) exemplifies this rule application and proves that the string $aaabbbccc$ belongs to the $\{a^nb^nc^n : n \geq 1 \}$ language.

\begin{figure}
    \begin{subfigure}[]{0.3\textwidth}
      \begin{lstlisting}
0. $S\rightarrow aBC$
1. $S\rightarrow aSBC$
2. $CB\rightarrow CZ$
3. $CZ\rightarrow WZ$
4. $WZ\rightarrow WC$
5. $WC\rightarrow BC$
6. $aB\rightarrow ab$
7. $bB\rightarrow bb$	
8. $bC\rightarrow bc$
9. $cC\rightarrow cc$     
      \end{lstlisting}

	\hspace*{1.8cm}(a)
  \end{subfigure}
  \begin{subfigure}[]{0.15\textwidth}
  \begin{tiny}  
\begin{lstlisting}
1. S
1. aSBC
0. aaSBCBC
2. aaaBCBCBC
3. aaaBCZCBC
4. aaaBWZCBC
5. aaaBWCCBC
2. aaaBBCCBC
3. aaaBBCCZC
4. aaaBBCWZC
5. aaaBBCWCC
2. aaaBBCBCC
3. aaaBBCZCC
4. aaaBBWZCC
5. aaaBBWCCC
6. aaaBBBCCC
7. aaabBBCCC
7. aaabbBCCC
8. aaabbbCCC
9. aaabbbcCC
9. aaabbbccC
$\bot$. aaabbbccc 
\end{lstlisting}
  \end{tiny}    
\hspace*{.3cm}(b)
  \end{subfigure}

  \caption{\small (a) Example of a {\em context-sensitive grammar}; (b) the corresponding {\it parse tree} for that grammar and the string $aaabbbccc$.}
  \label{fig:csg}
\end{figure}

Previous work showed that a classical planning compilation can implement {\it learning}, {\it parsing} and {\it production} for {\em Type-3} and {\em Type-2} languages~\cite{segovia2017generating}. In this work we show that a novel classical planning compilation implements these three canonical tasks for {\em Type-3}, {\em Type-2} and {\em Type-1} languages. In addition, this novel compilation has fewer input parameters since it does not require to bound the size of the grammar rules. The evaluation of our compilation approach reports the empirical data collected when learning the acceptor automata for the CSLs $\{a^nb^nc^n : n \geq 1 \}$ and $\{a^{2^n} : n \geq 1 \}$ and shows that it outperforms of a previous compilation for {\it learning}, {\it parsing} and {\it production} in {\em Context-Free Languages}.



\section{Background}
This section defines the formalization of CSGs and the classical planning model that we follow in this work.

\subsection{Classical planning}
Our approach for learning CSGs is compiling this inductive learning task into a classical planning task.

We use $F$ to denote the set of {\em fluents} (propositional variables) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$; i.e. either~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (without loss of generality, we will assume that $L$ does not contain conflicting values). We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents.

A {\em state} $s$ is a full assignment of values to fluents; $|s|=|F|$, so the size of the state space is $2^{|F|}$. Explicitly including negative literals $\neg f$ in states simplifies subsequent definitions but often we will abuse of notation by defining a state $s$ only in terms of the fluents that are true in $s$, as it is common in \strips\ planning.

A {\em classical planning frame} is a tuple $\Phi=\tup{F,A}$, where $F$ is a set of fluents and $A$ is a set of actions. An action $a\in A$ is defined with {\em preconditions}, $\pre(a)\subseteq\mathcal{L}(F)$, {\em positive effects}, $\eff^+(a)\subseteq\mathcal{L}(F)$, and {\em negative effects} $\eff^-(a)\subseteq\mathcal{L}(F)$. We say that an action $a\in A$ is {\em applicable} in a state $s$ iff $\pre(a)\subseteq s$. The result of applying $a$ in $s$ is the {\em successor state} denoted by $\theta(s,a)=\{s\setminus\eff^-(a))\cup\eff^+(a)\}$.

The result of applying action $a$ in state $s$ is the {\em successor} state $\theta(s,a)=\{s\setminus\eff_c^-(s,a))\cup\eff_c^+(s,a)\}$ where $\eff_c^-(s,a)\subseteq triggered(s,a)$ and $\eff_c^+(s,a)\subseteq triggered(s,a)$ are, respectively, the triggered {\em negative} and {\em positive} effects.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is an initial state and $G\subseteq\mathcal{L}(F)$ is a goal condition. A {\em plan} for $P$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$ that induces the {\em state trajectory} $\tup{s_0, s_1, \ldots, s_n}$ such that $s_0=I$ and $a_i$ ({\small $1\leq i\leq n$}) is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. The {\em plan length} is denoted with $|\pi|=n$ . A plan $\pi$ {\em solves} $P$ iff $G\subseteq s_n$; i.e.~if the goal condition is satisfied in the last state resulting from the application of the plan $\pi$ in the initial state $I$.

\subsection{Context-Sensitive Grammars}
We define a CSGs as a tuple $\mathcal{G}=\tup{V,\Sigma,R}$, where:
\begin{itemize}
\item $V$ is the finite set of {\em non-terminal symbols}, also called variables. With $v_0\in V$ is the start non-terminal symbol that represents the whole grammar.
\item $\Sigma$ is the finite set of {\em terminal symbols}, which are disjoint from the set of non-terminal symbols, i.e.~$V\cap \Sigma\neq\emptyset$. The set of terminal symbols is the {\em alphabet} of the language defined by $\mathcal{G}$
\item $R$ is a finite set of $\alpha v\beta\rightarrow \alpha\gamma\beta$ rules where $v\in V$, $\alpha,\beta\in(V\cup\Sigma)^*$ and $\gamma\in(V\cup\Sigma)^+$. That is, the left-hand and right-hand sides of any production rule $r\in R$ may be surrounded by a context of terminal and non-terminal symbols.
\end{itemize}

For any two strings $(e_1,e_2)$ we say that $e_1$ {\it directly yields} $e_2$, denoted by $e_1\Rightarrow e_2$, iff $e_1$ can be written as $e_3\alpha A\beta e_4$ and $e2$ can be written as $e_3\alpha \gamma\beta e_4$, for some production rule $(\alpha A\beta\rightarrow\alpha\gamma\beta)\in R$, and some context strings $e_3,e_4\in (V\cup \Sigma)^*$. Furthermore we say $e_1$ {\it yields} $e_2$, denoted by $e_1\Rightarrow^* e_2$, iff $\exists k\geq 0$ and $\exists u_1, \ldots, u_k$ such that $e_1=u_1\Rightarrow u_2\Rightarrow \ldots \Rightarrow u_k=e_2$ for some $k\geq 1$ and some strings $u_1,\ldots,u_{k-1}\in(V\cup\Sigma)^*$. In other words, the relation $\Rightarrow^*$ is the reflexive transitive closure of the $\Rightarrow$ relation.

Figure~\ref{fig:csg}(b) shows how the string $S$ yields the string $aaabbbccc$. The language of a CSG, $L(\mathcal{G})=\{e\in \Sigma^*: v_0\Rightarrow^* e\}$, is the set of strings that contain only terminal symbols and that can be yielded from the string that contains only the initial non-terminal symbol $v_0$ (denoted by $S$ in the example of Figure~\ref{fig:csg}(b)).

Given a CSG $\mathcal{G}$ and a string $e\in L(\mathcal{G})$ that belongs to its language, we define a {\it parse tree} $t_{\mathcal{G},e}$ as an ordered, rooted tree that determines a concrete syntactic structure of $e$ according to the rules in $\mathcal{G}$:
\begin{itemize}
\item Each node in a parse tree $t_{\mathcal{G},e}$ is either: An {\it internal node} that corresponds to the application of a rule $r\in R$ or a {\it leaf node} that corresponds to a terminal symbol $\sigma\in \Sigma$.
\item Edges in a parse tree $t_{\mathcal{G},e}$ connect non-terminal symbols to terminal or non-terminal symbols following a rule in $\mathcal{G}$.
\end{itemize}


\subsection{Linear-bounded Turing Machines}
A {\em Turing machine} is a tuple $M=\tup{Q,q_o,Q_{\bot},\mathcal{T},\square,\Sigma,\delta}$:
\begin{itemize}
\item $Q$, is a finite and non-empty set of machine states such that $q_0\in Q$ is the initial state of the machine and $Q_{\bot}\subseteq Q$ is the subset of acceptor states.  
\item $\mathcal{T}$ is the {\em tape alphabet}, that is a finite non-empty set of symbols that includes the {\em blank symbol} $\square\in\mathcal{T}$ (the only symbol allowed to occur on the tape infinitely often) and that contains $\Sigma\subseteq\mathcal{T}$, the set of symbols allowed to initially appear in the tape (also called the {\em input alphabet}).
\item $\delta: (Q\setminus Q_{\bot})\times \mathcal{T}\rightarrow Q\times\{left,right\}\times \mathcal{T}$ is the {\em transition function}. If $\delta$ is not defined for the current pair of machine state and tape symbol, then the machine halts.
\end{itemize}

A table is the most common convention to represent the transitions defined by $\delta$, where the table rows are indexed by the current tape symbol, while the table columns are indexed by the current machine state. For each possible pair of tape symbol and machine state, there is a table entry that defines: (1) the tape symbol to print at the current position of the header (2) whether the header is shifted {\em left} or {\em right} after the print operation and (3), the new state of the machine after the print operation. For instance, Figure~\ref{tab:tm-anbncn} shows the table that represents the $\delta$ function of a {\em Turing Machine} for recognizing the $\{a^nb^nc^n : n \geq 1 \}$ language. In this example the tape alphabet is $\Sigma=\{a,b,c,x,y,z,\square\}$ while the possible machine states are $Q=\{q_0,q_1,q_2,q_3,q_4,\underline{q_5}\}$ where \underline{$q_5$} is the only acceptor state.

\begin{figure}
\begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
    \hline
      & $q_0$ & $q_1$ & $q_2$ & $q_3$ & $q_4$ & \underline{$q_5$} \\ \hline
    a & x,r,$q_1$ & a,r,$q_1$ & - &  a,l,$q_3$ & - & - \\ \hline
    b & - & y,r,$q_2$ & b,r,$q_2$ & b,l,$q_3$ & - & - \\ \hline
    c & - & - & z,l,$q_3$ & - & - & - \\ \hline
    x & - & - & - & x,r,$q_0$ & - & - \\ \hline
    y & y,r,$q_4$ & y,r,$q_1$ & - & y,l,$q_3$ & y,r,$q_4$ & - \\ \hline
    z & - & - & z,r,$q_2$ & z,l,$q_3$ & z,r,$q_4$ & - \\\hline
    $\square$ & - & - & - & - & $\square$,r,$q_5$  & - \\                
    \hline
    \end{tabular}
\end{center}
  \caption{\small Example of a seven-symbol six-state {\em Turing Machine} for recognizing the $\{a^nb^nc^n : n \geq 1 \}$ language (\underline{$q_5$} is the only acceptor state).}
  \label{tab:tm-anbncn}
\end{figure}

A {\em Turing machine} is an abstract model of computation that operates on an infinite memory tape. An approach to implement in practice a Turing machine, is to bound the size of its tape. This can be done by: 
\begin{enumerate}
\item Extending the input alphabet with two special symbols that serve as the {\em left} and {\em right endmarkers}.
\item Limiting computation to the portion of the tape that contains the input plus the tape cells holding between the two endmarkers. This means that transitions do not print other symbols over the endmarkers and that never move neither to the left of the left endmarker nor to the right of the right endmarker.
\end{enumerate}


\section{Learning Context-Sensitive Grammars}

Our approach is to leverage classical planning to learn an \strips\ action model s.t. each action schema in the model encodes a transition of the aimed {\em Linear-bounded Turing Machine}. First we show how an entry in the table that defines the transitions of a {\em Linear-bounded Turing Machine} is modeled as a \strips\ action schema. Next we show how classical planning can be effectively used to learn action models of this kind.

\subsection{\strips\ Modeling of a Linear-bounded Turing Machine}
A classical planning frame $\Phi=\tup{F,A}$ encodes the {\em transition function} $\delta$ of a {\em Linear-bounded Turing Machine} $M$ as follows.

We assume that fluents $F$ are instantiated from a set of {\em predicates} $\Psi$, as in PDDL~\cite{fox2003pddl2}. Each predicate $p\in\Psi$ has an argument list of arity $ar(p)$. Given a set of {\em objects} $\Omega$ that represent the cells in the tape of the given Turing Machine $M$, the set of fluents $F$ is induced by assigning objects in $\Omega$ to the arguments of the predicates in $\Psi$; i.e.~$F=\{p(\omega):p\in\Psi,\omega\in\Omega^{ar(p)}\}$, where $\Omega^k$ is the $k$-th Cartesian power of $\Omega$.

The predicates in $\Psi$ are:
\begin{itemize}
\item {\tt (head ?x)} that encodes the current position of the header in the tape.
\item {\tt (next ?x1 ?x2)} encoding that the cell {\tt ?x2} follows cell {\tt ?x1} in the tape.
\item {\tt (symbol-$\sigma$ ?x)} encoding that the tape cell {\tt ?x} contains the symbol $\sigma\in\Sigma$.
\item {\tt (state-$q$)} encoding that $q\in Q$ is the current machine state.
\end{itemize}

Likewise we assume that actions $a\in A$ are instantiated from \strips\ operator schema. For each transition in $\delta$, a \strips\ action schema is defined such that:
\begin{itemize}
\item The {\bf header} of the schema is {\tt transition-id(?xl ?x ?xr)} where $id$ uniquely identifies the transition in $\delta$ and the parameters $?xl$, $?x$ and $?xr$ are tape cells.
\item The {\bf precoditions} of the schema includes {\tt(head ?x)} and {\tt (next ?xl ?x) (next ?x ?xr)} to force that $?x$ is the tape cell currently pointed by the header and that $?xl$ and $?xr$ respectively are its left and right neighbours. Additionally the schema includes preconditions {\tt(symbol-$\sigma$ ?x)} and {\tt (state-$q$)} to capture the symbol pointed by the header and the currrent machine state.
\item The {\bf delete effects} remove the symbol pointed by the header and the currrent machine state while the {\bf positive effects} set the new symbol pointed by the header and the new machine state.
\end{itemize}

The \strips\ action schema of Figure~\ref{fig:update-rule} models the rule $a,q_0\rightarrow x,r,q_1$ of the Turing Machine defined in Figure~\ref{tab:tm-anbncn}. The full encoding of the Turing Machine defined in Figure~\ref{tab:tm-anbncn} produces a total of sixteen \strips\ action schema with the same structure as the one of Figure~\ref{fig:update-rule}. 
\begin{figure}[hbt!]
\begin{scriptsize}
\begin{lstlisting}
(:action transition-1 ;; a,$q_0\rightarrow$ x,r,$q_1$
  :parameters (?xl ?x ?xr)
  :precondition (and (head ?x)                       
                     (symbol-a ?x) (state-$q_0$)
                     (next ?xl ?x) (next ?x ?xr))
  :effect (and (not (head ?x)) 
               (not (symbol-a ?x)) (not (state-$q_0$))
               (head ?xr) (symbol-x ?x) (state-$q_1$)))
\end{lstlisting}
\end{scriptsize}
 \caption{\small \strips\ action schema that models the transition $a,q_0\rightarrow x,r,q_1$ of the Turing Machine defined in Figure~\ref{tab:tm-anbncn}.}
\label{fig:update-rule}
\end{figure}

The execution of a {\em Linear-bounded Turing Machine} can then be defined as a {\em plan trace} $\mathcal{T}=\tup{s_0,a_1,s_1,\ldots,a_n,s_n}$ such that $s_0$ encodes the initial state of the tape plus the initial machine state and, for each {\small $1\leq i\leq n$}, $a_i$ is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. 

\subsection{Learning \strips\ Models with Classical Planning}
Learning \strips\ action schemes from plan traces is formalied as a $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ tuple, where:
\begin{itemize}
\item $\mathcal{M}$ is the set of {\em partially specified} operator schemas.
\item $\mathcal{T}=\tup{s_0,a_1,s_1,\ldots,a_n,s_{n}}$ is a {\em plan trace} obtained watching the execution of the classical plan $\pi=\tup{a_1, \ldots, a_n}$ such that, for each {\small $1\leq i\leq n$}, $a_i$ is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. 
\item $\Psi$ is the set of predicates that define the abstract state space of a given planning frame. 
\end{itemize}

A solution to a $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ learning task is a set of operator schema $\mathcal{M}'$ that is compliant with the input model $\mathcal{M}$, the plan trace $\mathcal{T}$ and the predicates $\Psi$. 

Given a learning task $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ the compilation defined by~\citeauthor{aineto:learningSTRIPS:ICAPS2018} outputs a classical planning task $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I_{\Lambda},G_{\Lambda}}$. A solution plan to the classical planning task $P_{\Lambda}$ resulting from this compilation is a sequence of actions that:
\begin{enumerate}
\item Programs the action model $\mathcal{M}'$. A solution plan starts with a {\em prefix} that, for each action schema $\xi\in\mathcal{M}$, determines its $pre(\xi)$, $del(\xi)$ and $add(\xi)$ sets.
\item Validates the action model $\mathcal{M}'$. The solution plan continues with a postfix that reproduces the given plan trace with the programmed action model $\mathcal{M}'$.
\end{enumerate}


\subsection{Learning CSGs with Classical Planning}
Learning the acceptor automaton of a CSL is a particular case of the $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ learning task such that:
\begin{itemize}
\item $\mathcal{M}$ is the set of {\em partially specified} operator schemas that encodes the preconditions and negative effects of the \strips\ schemas that encode the {\em transition function} $\delta$ of the {\em Linear-bounded Turing Machine} $M$.
\item $\mathcal{T}$ represents the execution of the {\em Linear-bounded Turing Machine} $M$ for the parsing of a given string.
\item $\Psi=\{${\small\tt(head ?x),(next ?x1 ?x2),(symbol-$\sigma$ ?x),(state-q)}$\}$ . 
\end{itemize}

Here we show that the existing classical planning compilation $P_{\Lambda}$ for learning \strips\ action schemes from plan traces~\cite{aineto:learningSTRIPS:ICAPS2018} can be adapted to learning the acceptor automaton of a given CSL. When learning a CSGs the input {\em plan trace} $\mathcal{T}$ is partially observed:
\begin{enumerate}
\item States $s_i$, with $1\leq i\leq n$, do not contain the fluents {\tt (state-q)} s.t. $q\in Q$.
\item At each state, $s_i$ with $1\leq i\leq n$, the exact executed action $a_i$ is unobserved while it is known that $a_i$ is built instantiaing the schema corresponding to a transition rule $\sigma,q\rightarrow \sigma',\{r,l\},q'$ provided that $\sigma$ is the symbol at the tape cell pointed by the header at state $s_i$.
\end{enumerate}
To handle the first source of partial observability the compilation does not require any modification because $s_0$ is fully observable since initially the machine state is always $q_0$ (likewise the machine state is always $q_n$ at the last state that corresponds to a string accepted by the learned automaton) and the compilation is robust to missing fluents in the intermediate states of the input plan trace $\mathcal{T}$.

Addressing the second source of partial observability requires a modification of the $P_{\Lambda}$ compilation. Adding extra preconditions to the apply actions so they are  only appliable actions iff they are built instantiaing the schema corresponding to the transition $\sigma,q\rightarrow \sigma',{r,l},q'$ provided that $\sigma$ is the symbol at the cell tape that is currently pointed by the header at state $s_i$.

\begin{lemma}
Soundness. Any classical plan $\pi$ that solves $P_{\Lambda}$ induces an action model $\mathcal{M}'$ that solves $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
Once operator schemas $\mathcal{M}'$ are programmed, they can only be applied and validated, because of the $mode_{prog}$ fluent. In addition, $P_{\Lambda}$ is only solvable if fluents $\{test_i\}$, {\small $1\leq i\leq n$} hold at the last reached state. These goals can only be achieved executing an applicable sequence of programmed operator schemas that reaches every state $s_i\in\mathcal{T}$, starting from the corresponding initial state and following the sequence of actions defined by the plans in $\Pi$. This means that the programmed action model $\mathcal{M}'$ complies with the provided input knowledge and hence, solves $\Lambda$.
\end{small}
\end{proof}


\begin{lemma}
Completeness. Any \strips\ action model $\mathcal{M}'$ that solves a $\Lambda=\tup{\mathcal{M},\Psi,\mathcal{T}}$ learning task, is computable solving the corresponding classical planning task $P_{\Lambda}$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
By definition, $F_v(\xi)\subseteq F_\Lambda$ fully captures the full set of elements that can appear in a \strips\ action schema $\xi\in\mathcal{M}$ given its header and the set of predicates $\Psi$. The compilation does not discard any possible \strips\ action schema definable within $F_v$ that satisfies the state trajectory constraint given by $\mathcal{T}$.
\end{small}
\end{proof}

The size of the classical planning task $P_{\Lambda}$ output by the compilation depends on:
\begin{itemize}
\item The arity of the actions headers in $\mathcal{M}$ and the predicates $\Psi$ that are given as input to the $\Lambda$ learning task. The larger these numbers, the larger the size of the $F_v(\xi)$ sets. This is the term that dominates the compilation size because it defines the $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ fluents set and the corresponding set of {\em programming} actions.
\item The number of given plan trace. The larger $|\mathcal{T}|$, the more $test_i$ fluents and $\mathsf{validate_{i}}$ actions in $P_{\Lambda}$.
\end{itemize}



\section{Parsing and Production of CSGs with classical planning}
The task of parsing can be implemented with the planning model $P_{\Lambda}$ introduced in the previous section by defining a classical planning instace whose fluents and actions encode the transition of the corresponding Turing machine, the initial state encodes the tape with the string to recognize and the goal conditions require that the turing machine ends at its acceptor states.

The task of production requires\ldots

\section{Evaluation}


\section{Related work}
%Our approach is related to recent work on structured prediction. Unlike existing approaches that leverage domain-specific knowledge to reduce the program hypotheses and enable fast inference, e.g., the unsupervised learning of probabilistic programs~\cite{Tenenbaum:programs:2015}, our approach is fully domain independent. On the other hand we cannot cope with noisy observations or probabilities over the space of outputs and inputs.

The learning of CFGs can also be understood in terms of activity recognition, such that the library of activities is formalized as a CFG, the library is initially unknown, and the input strings encode observations of the activities to recognize. {\it Activity recognition} is traditionally considered independent of the research done on automated planning, using handcrafted libraries of activities and specific algorithms~\cite{ravi2005activity}. An exception is the work by \citeauthor{ramirez2009plan}~[\citeyear{ramirez2009plan,ramirez2010probabilistic}] where goal recognition is formulated and solved with planning. As far as we know our work is the first that tightly integrates the tasks of (1) grammar learning, (2) recognition and (3) production using a common planning model and an off-the-shelf classical planner. 

Hierarchical Task Networks (HTNs) is a powerful formalism for representing libraries of plans~\cite{nau2003shop2}. HTNs are also defined at several levels such that the tasks at one level are decomposed into other tasks at lower levels with HTN decomposition methods sharing similarities with production rules in CFGs. There is previous work in generating HTNs~\cite{hogg2008htn,conf:ecai:Lotinac16} and an interesting research direction is to extend our approach for computing HTNs from flat sequences of actions. This aim is related to Inductive Logic Programming (ILP)~\cite{muggleton1999inductive} that learns logic programs from examples. Unlike logic programs (or HTNs) the CFGs that we generate are propositional and do not include variables. Techniques for learning high level state features that include variables are promising for learning lifted grammars~\cite{damir-derived-ijcai16}.


\section{Conclusions}
There is exhaustive previous work on learning CFGs given a corpus of correctly parsed input strings~\cite{sakakibara1992efficient,langley2000learning} or using positive and negative examples~\cite{de2010grammatical,muggleton2014meta}. This work addresses generating CFGs using only a small set of positive examples (in some cases even one single string that belongs to the language). Furthermore we follow a compilation approach that benefits straightforwardly from research advances in classical planning and that is also suitable for {\it production} and {\it recognition} tasks with arbitrary CFGs.

Our compilation bounds the number of rules $m$, the length of these rules $n$, the size of the stack $\ell$ and the length of the input strings $z$. If these bounds are too small, the classical planner used to solve the output planning task will not be able to find a solution. Larger values for these bounds do not formally affect to our approach, but in practice, the performance of classical planners is sensitive to the size of the input. Interestingly our approach can also follow an incremental strategy where we generate the CFG for a given sub-language and then encode this {\it sub-grammar} as an auxiliary procedure for generating more challenging CFGs~\cite{segovia2016hierarchical}. 

The size of the compilation output also depends on the number of examples. Empirical results show that our approach is able to generate non-trivial CFGs from very small data sets. Another interesting extension would be to add negative input strings, which would require a mechanism for validating that a given CFG does {\em not} generate a given string, or to accept incomplete input strings that would require combining the generation and production mechanisms.







\bibliographystyle{aaai}
\bibliography{tm-grammars}
\end{document}
