\def\year{2017}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article} %DO NOT CHANGE THIS
\usepackage{aaai18}  %Required
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
%PDF Info Is Required:
  \pdfinfo{}
\setcounter{secnumdepth}{0}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{comment}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}

\lstset{
  basicstyle=\ttfamily,
  mathescape
}



\usepackage{multicol}
\usepackage{arydshln}
\usetikzlibrary{calc,backgrounds,positioning,fit}


\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition



\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}


\begin{document}

\title{Learning of Context-Sensitive Grammars with Classical Planning}
\author{\#39}


% Commented for blind submission
\author{Diego Aineto\and Sergio Jim\'enez\and Eva Onaindia\\
{\small Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\small Universitat Polit\`ecnica de Val\`encia.}\\
{\small Camino de Vera s/n. 46022 Valencia, Spain}\\
{\small \{dieaigar,serjice,onaindia\}@dsic.upv.es}}

\maketitle
\begin{abstract}
This paper presents a novel approach for learning {\em Context-Sensitive Grammars} (CSGs) from small sets of input strings. The approach guarantees that the computed grammar generalizes provided that the input strings are long and diverse enough to cover all the grammar rules.  Our approach is to compile this learning task into a classical planning problem whose solutions are all sequences of actions with a prefix that build the CSG and and a postfix that validates the built grammar in the input strings. The compilation is flexible to implement the three canonical tasks of CSGs, {\it grammar generation}, {\it string production} and {\it string recognition} within the same classical planning model. The experimental validation of our compilation approach reports the empirical data collected when learning the CSGs for the languages $\{a^nb^nc^n : n \geq 1 \}$ and $\{a^{2^n} : n \geq 1 \}$ and shows that overcomes the performance  of a previous compilation for learning {\em Context-Free Grammars}.
\end{abstract}


\section{Introduction}
\label{sec:section1}
A {\em formal grammar} is a set of symbols and production rules that describe how to form the possible strings of certain formal language~\cite{hopcroft:automatatheory:2001}. Usually three canonical tasks are defined over formal grammars:
\begin{itemize}
\item {\it Learning}: Given a set of strings, compute a grammar that is compliant with the input strings.
\item {\it Production}: Given a formal grammar, generate strings that belong to the language represented by the grammar.
\item {\it Recognition} (also known as {\em parsing}): Given a formal grammar and a string, determine whether the string belongs to the language represented by the grammar.
\end{itemize}

Chomsky defined four types of formal grammars that differ in the form and generative capacity of their rules~\cite{chomsky2002syntactic}. Each grammar type generates a different class of formal language that is recognizable with a different kind of automaton: {\bf Type-0} corresponds to the {\em recursively enumerable} languages that can be recognized with a {\em Turing machine}. {\bf Type-1} corresponds to the {\em recursively enumerable} languages that can be recognized with a {\em Linear-bounded Turing machine}. {\bf Type-2} corresponds to {\em context-free} languages that are recognizable with a {\em non-deterministic push-down automaton}. Finally, {\bf type-3} corresponds to {\em regular} languages that can be recognized with a {\em Finite state machine}.

Figure~\ref{fig:csg}(a) shows an example of a CSG grammar ({\em Type-1}) that generates the $\{a^nb^nc^n : n \geq 1 \}$ language. The grammar defines ten production rules, contains three {\em terminal symbols} ($a$, $b$ and $c$) and five {\em non-terminal symbols} ($S$,$B$,$C$,$W$ and $Z$). This CSG can generate, for instance, the string $aaabbbccc$ by applying the sequence of rules $\tup{1,1,0,2,3,4,5,2,3,4,5,2,3,4,5,6,7,7,8,9,9}$. The {\it parse tree} in Figure~\ref{fig:csg}(b) exemplifies this rule application and proves that the string $aaabbbccc$ belongs to the $\{a^nb^nc^n : n \geq 1 \}$ language defined by the CSG grammar.

\begin{figure}
    \begin{subfigure}[]{0.3\textwidth}
      \begin{lstlisting}
0. $S\rightarrow aBC$
1. $S\rightarrow aSBC$
2. $CB\rightarrow CZ$
3. $CZ\rightarrow WZ$
4. $WZ\rightarrow WC$
5. $WC\rightarrow BC$
6. $aB\rightarrow ab$
7. $bB\rightarrow bb$	
8. $bC\rightarrow bc$
9. $cC\rightarrow cc$     
      \end{lstlisting}

	\hspace*{1.8cm}(a)
  \end{subfigure}
  \begin{subfigure}[]{0.15\textwidth}
  \begin{tiny}  
\begin{lstlisting}
1. S
1. aSBC
0. aaSBCBC
2. aaaBCBCBC
3. aaaBCZCBC
4. aaaBWZCBC
5. aaaBWCCBC
2. aaaBBCCBC
3. aaaBBCCZC
4. aaaBBCWZC
5. aaaBBCWCC
2. aaaBBCBCC
3. aaaBBCZCC
4. aaaBBWZCC
5. aaaBBWCCC
6. aaaBBBCCC
7. aaabBBCCC
7. aaabbBCCC
8. aaabbbCCC
9. aaabbbcCC
9. aaabbbccC
$\bot$. aaabbbccc 
\end{lstlisting}
  \end{tiny}    
\hspace*{.3cm}(b)
  \end{subfigure}

  \caption{\small (a) Example of a context-sensitive grammar; (b) the corresponding {\it parse tree} for the string $aaabbbccc$.}
  \label{fig:csg}
\end{figure}

Previous work showed that a classical planning compilation can implement the three canonical tasks (namely {\it grammar generation}, {\it string production} and {\it string recognition}) for {\em Type-3} and {\em Type-2} grammars~\cite{segovia2017generating}. In this work we show that a novel classical planning compilation implements these three canonical tasks for grammars of the three last three types ({\em Type-3}, {\em Type-2} and {\em Type-1}). In addition, this novel compilation has fewer input parameters since it does not require to specify any bound on the size of the grammar rules. The experimental validation of our compilation approach reports the empirical data collected when learning the CSGs for the languages $\{a^nb^nc^n : n \geq 1 \}$ and $\{a^{2^n} : n \geq 1 \}$ and shows that overcomes the performance  of a previous compilation for learning {\em Context-Free Grammars}.



\section{Background}
This section defines the formalization of CSGs and the classical planning model that we follow in this work.

\subsection{Classical planning}
Our approach for learning CSGs is compiling this inductive learning task into a classical planning task.

We use $F$ to denote the set of {\em fluents} (propositional variables) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$; i.e. either~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (without loss of generality, we will assume that $L$ does not contain conflicting values). We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$; i.e.~all partial assignments of values to fluents.

A {\em state} $s$ is a full assignment of values to fluents; $|s|=|F|$, so the size of the state space is $2^{|F|}$. Explicitly including negative literals $\neg f$ in states simplifies subsequent definitions but often we will abuse of notation by defining a state $s$ only in terms of the fluents that are true in $s$, as it is common in \strips\ planning.

A {\em classical planning frame} is a tuple $\Phi=\tup{F,A}$, where $F$ is a set of fluents and $A$ is a set of actions. An action $a\in A$ is defined with {\em preconditions}, $\pre(a)\subseteq\mathcal{L}(F)$, {\em positive effects}, $\eff^+(a)\subseteq\mathcal{L}(F)$, and {\em negative effects} $\eff^-(a)\subseteq\mathcal{L}(F)$. We say that an action $a\in A$ is {\em applicable} in a state $s$ iff $\pre(a)\subseteq s$. The result of applying $a$ in $s$ is the {\em successor state} denoted by $\theta(s,a)=\{s\setminus\eff^-(a))\cup\eff^+(a)\}$.

The result of applying action $a$ in state $s$ is the {\em successor} state $\theta(s,a)=\{s\setminus\eff_c^-(s,a))\cup\eff_c^+(s,a)\}$ where $\eff_c^-(s,a)\subseteq triggered(s,a)$ and $\eff_c^+(s,a)\subseteq triggered(s,a)$ are, respectively, the triggered {\em negative} and {\em positive} effects.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is an initial state and $G\subseteq\mathcal{L}(F)$ is a goal condition. A {\em plan} for $P$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$ that induces the {\em state trajectory} $\tup{s_0, s_1, \ldots, s_n}$ such that $s_0=I$ and $a_i$ ({\small $1\leq i\leq n$}) is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. The {\em plan length} is denoted with $|\pi|=n$ . A plan $\pi$ {\em solves} $P$ iff $G\subseteq s_n$; i.e.~if the goal condition is satisfied in the last state resulting from the application of the plan $\pi$ in the initial state $I$.

\subsection{Context-Sensitive Grammars}
We define a CSGs as a tuple $\mathcal{G}=\tup{V,\Sigma,R}$, where:
\begin{itemize}
\item $V$ is the finite set of non-terminal symbols, also called variables. With $v_0\in V$ is the start non-terminal symbol that represents the whole grammar.
\item $\Sigma$ is the finite set of terminal symbols, which are disjoint from the set of non-terminal symbols, i.e.~$V\cap \Sigma\neq\emptyset$. The set of terminal symbols is the alphabet of the language defined by $\mathcal{G}$
\item Rules in $R$ are defined as $\alpha v\beta\rightarrow \alpha\gamma\beta$ where $v\in V$, $\alpha,\beta\in(V\cup\Sigma)^*$ and $\gamma\in(V\cup\Sigma)^+$. That is, the left-hand and right-hand sides of any production rules $r\in R$ may be surrounded by a context of terminal and nonterminal symbols.
\end{itemize}

For any two strings $(e_1,e_2)$ we say that $e_1$ {\it directly yields} $e_2$, denoted by $e_1\Rightarrow e_2$, iff $e_1$ can be written as $e_3\alpha A\beta e_4$ and $e2$ can be written as $e_3\alpha \gamma\beta e_4$, for some production rule $(\alpha A\beta\rightarrow\alpha\gamma\beta)\in R$, and some context strings $e_3,e_4\in (V\cup \Sigma)^*$. Furthermore we say $e_1$ {\it yields} $e_2$, denoted by $e_1\Rightarrow^* e_2$, iff $\exists k\geq 0$ and $\exists u_1, \ldots, u_k$ such that $e_1=u_1\Rightarrow u_2\Rightarrow \ldots \Rightarrow u_k=e_2$ for some $k\geq 1$ and some strings $u_1,\ldots,u_{k-1}\in(V\cup\Sigma)^*$. In other words, the relation $\Rightarrow^*$ is the reflexive transitive closure of the $\Rightarrow$ relation.

For instance, Figure~\ref{fig:csg}(b) shows how the string $S$ yields the string $aaabbbccc$. The language of a CSG, $L(\mathcal{G})=\{e\in \Sigma^*: v_0\Rightarrow^* e\}$, is the set of strings that contain only terminal symbols and that can be yielded from the string that contains only the initial non-terminal symbol $v_0$ (which is denoted by $S$ in the example of Figure~\ref{fig:csg}(b)).

Given a CSG $\mathcal{G}$ and a string $e\in L(\mathcal{G})$ that belongs to its language, we define a {\it parse tree} $t_{\mathcal{G},e}$ as an ordered, rooted tree that determines a concrete syntactic structure of $e$ according to the rules in $\mathcal{G}$:
\begin{itemize}
\item Each node in a parse tree $t_{\mathcal{G},e}$ is either:
\begin{itemize}
\item An {\it internal node} that corresponds to the application of a rule $r\in R$.
\item A {\it leaf node} that corresponds to a terminal symbol $\sigma\in \Sigma$ and has no outgoing branches.
\end{itemize}
\item Edges in a parse tree $t_{\mathcal{G},e}$ connect non-terminal symbols to terminal or non-terminal symbols following the rules $R$ in $\mathcal{G}$.
\end{itemize}


\subsection{Linear-bounded Turing Machines}
We define a {\em Turing machine} as a tuple $M=\tup{Q,q_o,Q_{\bot},\mathcal{T},\square,\Sigma,\delta}$, where:
\begin{itemize}
\item $Q$, is a finite and non-empty set of machine states such that $q_0\in Q$ it the initial state of the machine and $Q_{\bot}\subseteq Q$ is the subset of terminal states.  
\item $\mathcal{T}$ is the {\em tape alphabet}, that is a finite an non-empty set of symbols that includes the {\em blank symbol} $\square\in\mathcal{T}$ (the only symbol allowed to occur on the tape infinitely often at any step during the computation).
\item $\Sigma$ is the {\em input alphabet}, the set of symbols allowed to initially appear in the tape.
\item $\delta: (Q\setminus Q_{\bot})\times \mathcal{T}\rightarrow Q\times \mathcal{T}\times\{left,right\}$ is the {\em transition function}. If $\delta$ is not defined for the current state of the machine and the current tape symbol, then the machine halts.
\end{itemize}

A table is the most common convention to represent the transitions defined by $\delta$, where the table rows are indexed with the current tape symbol, while the table columns are indexed by the current machine state. For each possible tape symbol and state of the machine there is a table entry that defines: (1) the tape symbol to print at the current position of the header (2) whether the header is shifted {\em left} or {\em right} after the print operation and (3), the new state of the machine after the print operation. For instance, Figure~\ref{tab:tm-anbncn} shows the table that represents the $\delta$ function of a {\em Turing Machine} for recognizing the $\{a^nb^nc^n : n \geq 1 \}$ language. In this example the tape alphabet is $\Sigma=\{a,b,c,x,y,z,\square\}$ while the possible machine states are $Q=\{q_0,q_1,q_2,q_3,q_4,\underline{q_5}\}$ where \underline{$q_5$} is the acceptor state.

\begin{figure}
\begin{center}
    \begin{tabular}{| c | c | c | c | c | c | c |}
    \hline
      & $q_0$ & $q_1$ & $q_2$ & $q_3$ & $q_4$ & \underline{$q_5$} \\ \hline
    a & x,r,$q_1$ & a,r,$q_1$ & - &  a,l,$q_3$ & - & - \\ \hline
    b & - & y,r,$q_2$ & b,r,$q_2$ & b,l,$q_3$ & - & - \\ \hline
    c & - & - & z,l,$q_3$ & - & - & - \\ \hline
    x & - & - & - & x,r,$q_0$ & - & - \\ \hline
    y & y,r,$q_4$ & y,r,$q_1$ & - & y,l,$q_3$ & y,r,$q_4$ & - \\ \hline
    z & - & - & z,r,$q_2$ & z,l,$q_3$ & z,r,$q_4$ & - \\\hline
    $\square$ & - & - & - & - & $\square$,r,$q_5$  & - \\                
    \hline
    \end{tabular}
\end{center}
  \caption{\small Example of a seven-symbol six-state {\em Turing Machine} for recognizing the $\{a^nb^nc^n : n \geq 1 \}$ language (\underline{$q_5$} is the acceptor state).}
  \label{tab:tm-anbncn}
\end{figure}

A {\em Turing machine} is an abstract model of computation that operates on an infinite memory tape. An approach to implement in practice a Turing machine, is to bound the size of its tape. This can be done by: 
\begin{enumerate}
\item Extending the input alphabet with two special symbols, serving as {\em left} and {\em right endmarkers}.
\item Limiting computation to the portion of the tape containing the input plus the two tape cells holding the endmarkers. This means that transitions may not print other symbols over the endmarkers and that may neither move to the left of the left endmarker nor to the right of the right endmarker.
\end{enumerate}


\section{Learning Context-Sensitive Grammars}

Our approach to learning CSGs is to leverage on classical planning to learn an \strips\ action model s.t. each action schema in the model encodes a transition of a {\em Linear-bounded Turing Machine} that recognices the strings given as input.

First we show how an entry in the table that defines the transitions of a {\em Linear-bounded Turing Machine} can be modeled as \strips\ action schema. Next we show how classical planning can be effectively used to learn action models of this kind.

\subsection{Modeling a Linear-bounded Turing Machine with Classical Planning}
Given $\delta$, the {\em transition function} of a {\em Linear-bounded Turing Machines} $M$, it can be encoded as a classical planning frame $P=\tup{F,A}$ as follows.

We assume that fluents $F$ are instantiated from a set of {\em predicates} $\Psi$, as in PDDL~\cite{fox2003pddl2}. Each predicate $p\in\Psi$ has an argument list of arity $ar(p)$. Given a set of {\em objects} $\Omega$ that represent the cells in the tape of the given Turing Machine $M$, the set of fluents $F$ is induced by assigning objects in $\Omega$ to the arguments of the predicates in $\Psi$; i.e.~$F=\{p(\omega):p\in\Psi,\omega\in\Omega^{ar(p)}\}$, where $\Omega^k$ is the $k$-th Cartesian power of $\Omega$. In more detail the predicates in $\Psi$ are:
\begin{itemize}
\item {\tt (head ?x)} that encodes the current position of the header in the tape.
\item {\tt (next ?x1 ?x2)} that encodes that the cell {\tt ?x2} follows cell {\tt ?x1} in the tape.
\item {\tt (symbol-$\sigma$ ?x)} that encodes that the tape cell {\tt ?x} contains the symbol $\sigma\in\Sigma$.
\item {\tt (state-$q$)} that encodes that $q\in Q$ is the current machine state.
\end{itemize}

Likewise we assume that actions $a\in A$ are instantiated from \strips\ operator schema. For each transition in $\delta$, a \strips\ action model is defined such that:
\begin{itemize}
\item The {\bf header} of the schema is {\tt rule-id(?xl ?x ?xr)} where $id$ uniquely identifies the transition in $\delta$ and the parameters $?xl$, $?x$ and $?xr$ are tape cells.
\item The {\bf precoditions} of the schema includes {\tt(head ?x)} and {\tt (next ?xl ?x) (next ?x ?xr)} to force that $?x$ is the current tape cell pointed by the header and that $?xl$ and $?xr$ respectively are its left and right neighbours. Additionally the schema includes preconditions {\tt(symbol-$\sigma$ ?x)} and {\tt (state-$q$)} to capture the current symbol pointed by the header and the currrent machine state.
\item The {\bf delete effects} remove the current symbol pointed by the header and the currrent machine state. Finally, the {\bf positive effects} set the new symbol pointed by the header and the new machine state 
\end{itemize}

To illustrate this, Figure~\ref{fig:update-rule} shows the rule $a,q_0\rightarrow x,r,q_1$ of the Turing Machine defined in Figure~\ref{tab:tm-anbncn}. The full encoding of the Turing Machine defined in Figure~\ref{tab:tm-anbncn} produces a total of sixteen \strips\ action schema with the same structure as the one of Figure~\ref{fig:update-rule}. 
\begin{figure}[hbt!]
\begin{scriptsize}
\begin{verbatim}
(:action rule1 ;; a,q_0 -> x,r,q_1
  :parameters (?xl ?x ?xr)
  :precondition (and (head ?x) 
                     (next ?xl ?x) (next ?x ?xr) 
                     (symbol-a ?x) (state-q_0))
  :effect (and (not (head ?x)) 
               (not (symbol-a ?x)) 
               (not (state-q_0))
               (symbol-x ?x) (head ?xr) (state-q_1)))
\end{verbatim}
\end{scriptsize}
 \caption{\small \strips\ action schema that models the first rule $a,q_0\rightarrow x,r,q_1$ of the Turing Machine defined in Figure~\ref{tab:tm-anbncn}.}
\label{fig:update-rule}
\end{figure}


\subsection{Learning CSGs with Classical Planning}
Here we show that the existing classical planning compilation for learning \strips\ action schemes from plan traces~\cite{aineto:learningSTRIPS:ICAPS2018} can be adapted to address the learning of CSGs. Note that in this case the set of preconditions and delete effects of the \strips\ action schemes is already known so the learning task reduces to learning positive effects.

We formalize the learning of \strips\ action schemes from plan traces as a tuple $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ where:
\begin{itemize}
\item $\mathcal{M}$, the set of {\em partially specified} operator schemas, wherein each $\xi\in\mathcal{M}$ is only composed of $head(\xi)$.
\item $\mathcal{T}=\tup{s_0,a_1,s_1,\ldots,a_n,s_{n}}$ is the {\em plan trace} obtained watching the execution of the classical plan $\pi=\tup{a_1, \ldots, a_n}$ such that, for each {\small $1\leq i\leq n$}, $a_i$ is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. 
\item $\Psi$ is the set of predicates that define the abstract state space of a given planning frame. 
\end{itemize}

A solution to a $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ learning task is a set of operator schema $\mathcal{M}'$ that is compliant with the input model $\mathcal{M}$, the given state observations $\mathcal{T}$ and the predicates $\Psi$. 

Given a learning task $\Lambda=\tup{\mathcal{M},\mathcal{T},\Psi}$ the compilation defined by~\citeauthor{aineto:learningSTRIPS:ICAPS2018} outputs a classical planning task $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I_{\Lambda},G_{\Lambda}}$:
\begin{itemize}
\item $F_{\Lambda}$ contains:
\begin{itemize}
\item The set of fluents $F$ built instantiating the predicates $\Psi$ with the objects $\Omega$ that appear in the input observations. Formally, $\Omega=\bigcup_{\small s\in\mathcal{O}} obj(s)$, where $obj$ is a function that returns the objects that appear in a given state.
\item Fluents $pre_f(\xi)$, $del_f(\xi)$ and $add_f(\xi)$, for every $f\in F_v(\xi)$, that represent the programmed action model. If a fluent $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ holds, it means that $f$ is a precondition/negative/positive effect in the schema $\xi\in \mathcal{M}'$. 
\item The fluents $mode_{prog}$ and $mode_{val}$ to indicate whether the operator schemas are programmed or validated, and the fluents $\{test_i\}_{1\leq i\leq |\mathcal{O}|}$, indicating the observation in $\mathcal{O}$ where the action model is validated.
\end{itemize}
\item $I_{\Lambda}$ encodes the first observation, $s_0\subseteq F$, and sets $mode_{prog}$ to true. Our compilation assumes that initially, operator schemas are programmed with every possible precondition (the most specific learning hypothesis), no negative effect and no positive effect. Therefore fluents $pre_f(\xi)$, for every $f\in F_v(\xi)$, hold also at the initial state.

\item $G_{\Lambda}=\bigcup_{1\leq i\leq |\mathcal{O}|}\{test_i\}$, requires that the programmed action model is validated in all the input observations.
\item $A_{\Lambda}$ comprises three kinds of actions:
\begin{enumerate}
\item Actions for {\em programming} operator schema $\xi\in\mathcal{M}$ that add a {\em positive} effect $f\in F_v(\xi)$ to the action schema $\xi\in\mathcal{M}$.
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programEff_{f,\xi}})=&\{\neg del_{f}(\xi),\neg add_{f}(\xi), mode_{prog}\},\\
\cond(\mathsf{programEff_{f,\xi}})=&\{pre_{f}(\xi)\}\rhd\{del_{f}(\xi)\},\\
& \{\neg pre_{f}(\xi)\}\rhd\{add_{f}(\xi)\}.
\end{align*}
\end{small}

\item Actions for {\em applying} a programmed operator schema $\xi\in\mathcal{M}$ bound with objects $\omega\subseteq\Omega^{ar(\xi)}$. Since operators headers are given as input, the variables $pars(\xi)$ are bound to the objects in $\omega$ that appear at the same position. 
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{apply_{\xi,\omega}})=&\{pre_{f}(\xi)\implies p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))}\\
&\cup \{\neg mode_{val}\},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{del_{f}(\xi)\}\rhd\{\neg p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{add_{f}(\xi)\}\rhd\{p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{mode_{prog}\}\rhd\{\neg mode_{prog}\},\\
&\{\emptyset\}\rhd\{mode_{val}\}.
\end{align*}
\end{small}

\item Actions for {\em validating} the partially observed state $s_i\in\mathcal{O}$, {\tt\small $1\leq i< |\mathcal{O}|$}.
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{i}})=&s_i\cup\{test_j\}_{j\in 1\leq j<i}\cup\{\neg test_j\}_{j\in i\leq j\leq |\mathcal{O}|}\\
& \cup \{mode_{val}\},\\
\cond(\mathsf{validate_{i}})=&\{\emptyset\}\rhd\{test_i,\neg mode_{val}\}.
\end{align*}
\end{small}
\end{enumerate}
\end{itemize}


\begin{lemma}
Soundness. Any classical plan $\pi$ that solves $P_{\Lambda}$ induces an action model $\mathcal{M}'$ that solves $\Lambda=\tup{\mathcal{M},\mathcal{O},\Psi,\Pi}$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
Once operator schemas $\mathcal{M}'$ are programmed, they can only be applied and validated, because of the $mode_{prog}$ fluent. In addition, $P_{\Lambda}$ is only solvable if fluents $\{test_i\}$, {\small $1\leq i\leq n$} hold at the last reached state. These goals can only be achieved executing an applicable sequence of programmed operator schemas that reaches every state $s_i\in\mathcal{T}$, starting from the corresponding initial state and following the sequence of actions defined by the plans in $\Pi$. This means that the programmed action model $\mathcal{M}'$ complies with the provided input knowledge and hence, solves $\Lambda$.
\end{small}
\end{proof}


\begin{lemma}
Completeness. Any \strips\ action model $\mathcal{M}'$ that solves a $\Lambda=\tup{\mathcal{M},\Psi,\mathcal{T},\Pi}$ learning task, is computable solving the corresponding classical planning task $P_{\Lambda}$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
By definition, $F_v(\xi)\subseteq F_\Lambda$ fully captures the full set of elements that can appear in a \strips\ action schema $\xi\in\mathcal{M}$ given its header and the set of predicates $\Psi$. The compilation does not discard any possible \strips\ action schema definable within $F_v$ that satisfies the state trajectory constraint given by $\mathcal{T},\Pi$.
\end{small}
\end{proof}

The size of the classical planning task $P_{\Lambda}$ output by the compilation depends on:
\begin{itemize}
\item The arity of the actions headers in $\mathcal{M}$ and the predicates $\Psi$ that are given as input to the $\Lambda$ learning task. The larger these numbers, the larger the size of the $F_v(\xi)$ sets. This is the term that dominates the compilation size because it defines the $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ fluents set and the corresponding set of {\em programming} actions.
\item The number of given state observations. The larger $|\mathcal{T}|$, the more $test_i$ fluents and $\mathsf{validate_{i}}$ actions in $P_{\Lambda}$.
\end{itemize}



\section{Parsing and Production of CSGs with classical planning}
The task of parsing can be implemented with the planning model introduced in the previous section by defining a classical planning instace whose fluents and actions encode the transition of the corresponding Turing machine, the initial state encodes the tape with the string to recognize and the goal conditions require that the turing machine ends in on its acceptor states.

The task of production requires to include actions that only write once a single symbol from the {\em tape alphabet} at the tape location indicated by the header.

\section{Evaluation}


\section{Related work}
%Our approach is related to recent work on structured prediction. Unlike existing approaches that leverage domain-specific knowledge to reduce the program hypotheses and enable fast inference, e.g., the unsupervised learning of probabilistic programs~\cite{Tenenbaum:programs:2015}, our approach is fully domain independent. On the other hand we cannot cope with noisy observations or probabilities over the space of outputs and inputs.

The learning of CFGs can also be understood in terms of activity recognition, such that the library of activities is formalized as a CFG, the library is initially unknown, and the input strings encode observations of the activities to recognize. {\it Activity recognition} is traditionally considered independent of the research done on automated planning, using handcrafted libraries of activities and specific algorithms~\cite{ravi2005activity}. An exception is the work by \citeauthor{ramirez2009plan}~[\citeyear{ramirez2009plan,ramirez2010probabilistic}] where goal recognition is formulated and solved with planning. As far as we know our work is the first that tightly integrates the tasks of (1) grammar learning, (2) recognition and (3) production using a common planning model and an off-the-shelf classical planner. 

Hierarchical Task Networks (HTNs) is a powerful formalism for representing libraries of plans~\cite{nau2003shop2}. HTNs are also defined at several levels such that the tasks at one level are decomposed into other tasks at lower levels with HTN decomposition methods sharing similarities with production rules in CFGs. There is previous work in generating HTNs~\cite{hogg2008htn,conf:ecai:Lotinac16} and an interesting research direction is to extend our approach for computing HTNs from flat sequences of actions. This aim is related to Inductive Logic Programming (ILP)~\cite{muggleton1999inductive} that learns logic programs from examples. Unlike logic programs (or HTNs) the CFGs that we generate are propositional and do not include variables. Techniques for learning high level state features that include variables are promising for learning lifted grammars~\cite{damir-derived-ijcai16}.


\section{Conclusions}
There is exhaustive previous work on learning CFGs given a corpus of correctly parsed input strings~\cite{sakakibara1992efficient,langley2000learning} or using positive and negative examples~\cite{de2010grammatical,muggleton2014meta}. This work addresses generating CFGs using only a small set of positive examples (in some cases even one single string that belongs to the language). Furthermore we follow a compilation approach that benefits straightforwardly from research advances in classical planning and that is also suitable for {\it production} and {\it recognition} tasks with arbitrary CFGs.

Our compilation bounds the number of rules $m$, the length of these rules $n$, the size of the stack $\ell$ and the length of the input strings $z$. If these bounds are too small, the classical planner used to solve the output planning task will not be able to find a solution. Larger values for these bounds do not formally affect to our approach, but in practice, the performance of classical planners is sensitive to the size of the input. Interestingly our approach can also follow an incremental strategy where we generate the CFG for a given sub-language and then encode this {\it sub-grammar} as an auxiliary procedure for generating more challenging CFGs~\cite{segovia2016hierarchical}. 

The size of the compilation output also depends on the number of examples. Empirical results show that our approach is able to generate non-trivial CFGs from very small data sets. Another interesting extension would be to add negative input strings, which would require a mechanism for validating that a given CFG does {\em not} generate a given string, or to accept incomplete input strings that would require combining the generation and production mechanisms.







\bibliographystyle{aaai}
\bibliography{tm-grammars}
\end{document}
