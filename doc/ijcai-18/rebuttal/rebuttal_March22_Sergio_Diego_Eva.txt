Rebuttal #12637:
-------------

We would like to thank all the reviewers for their thoughtful comments and insights as well as expressing our general agreement with the provided comments.


Lack of comparison
----------------------

The SAT-based system ARMS (AIJ07) is the closest system to compare with since ARMS also requires the operator heads and observations of correct plans. ARMS addresses, however, a different learning task:

(1) ARMS learns from plan traces (where the observed ACTIONS are assumed to be correct) while we learn from STATE observations (unobserved actions).
(2) ARMS addresses an OPTIMIZATION task (i.e. maximizing the # of covered examples) while ours is a SATISFYING task. We compute a conservative model that is guaranteed to be applicable at all the given observations.

As for LOCM, we think a better comparison would be with LOP since LOCM only learns the dynamic aspects of the domain (i.e. state changes that occur due to action application but not the static relations). However, LOP needs a set of optimal plans as input to the learning system.


Rebuttal #23413:
-------------
We definitely agree with the reviewer that a SAT-based compilation could also be used for learning and evaluating the tasks addressed in this paper. The only reason why we opt for planning is because when there is an unknown # of actions between state observations, the planning horizon is no longer known and this learning task is in our research agenda. Further, the planning community has shown terrific expertise in the development of informative and inexpensive heuristics (current heuristics are able to evaluate millions of symbolic states in seconds). Our belief is that this technology will play a key role in the assessment of generative models with inputs of large amounts of incomplete/noisy examples. 

Like happens with satisfying planners, our approach is extensible to the corresponding optimization task (in our case this amounts to minimizing the # of model edits required to cover a set of observations). As a first step, this paper aims at understanding the grounds of the compilation scheme for learning and evaluating models in fully-observable environments (e.g. classical planning tasks); but, extending this approach to partially-observable/noisy environments, is certainly in our agenda. 

Answers to the reviewer's questions:

1. We learn actions preconditions starting from the most specific hypothesis (all preconditions) just because we obtained slightly better experimental results. Nothing prevents us from starting with the most general hypothesis (zero preconditions), a random model, or even a previously learnt model (as it is the case of the evaluation task). In fact, the evaluation task starts with a given action model that may contain, for instance, no preconditions at all. 

2. We thank the reviewer for the suggestion that the "one test_i fluent" encoding is more natural. We will evaluate it and check if this brings some performance gain. 

Regarding the remarks on how the training set was generated, we refer the reviewer to the Rebuttal #26170.


Rebuttal #25749:
-------------
The contribution of our approach is leveraging classical planning not only for LEARNING, that allows us to report quantitative results over a wide range of different domains but also for EVALUATING how well a given STRIPS model matches a test set of observations.

Evaluating a given model according to a test set of observations represents a step towards solving the "model recognition" task. Given the high expressivenes of the STRIPS models (as expressive as a Turing Machine with finite tape), the task of model recognition is certainly relevant. Different generative models like policies, programs, grammars or different forms of domain-specific control knowledge are STRIPS compilable (as stated in the series of work by Jorge Baier, Sheila McIlraith et al). In this sense, our work poses a general framework to assess the validation of a generative model (provided that it is STRIPS compilable) in a given set of observations.

Regarding the remarks on the lack of comparison with other approaches, we refer the reviewer to the Rebuttal #12637.


Rebuttal #26170:
---------------

Generation of the training set
------------------------------

The 25 observations per domain were generated by solving one instance of the IPC; in some cases we modified the problem and limited the number of objects to keep the computation time low. Because our approach is a planning compilation, the limit on the # of observations is given by the planner performance. Current SATplanners have performance issues with horizons beyond 150-200 steps. Given that actions for programming preconds/effects are often applied in parallel, in a single step, this means we could handle no more than a hundred observations with current SATplanners (note that we require 2 actions to validate one observation, Fig 3). Ultimately, our goal was to evaluate how well the approach performs with few observations and so it is more important to have representative samples than a large number of observations, which would undoubtedly lead to better models but also longer learning times. 


Given the small size of the training set, there is no guarantee that the unobserved plan contains actions for all the domain schemas, which clearly has an impact in some results (e.g, the missing actions “disembark-truck” and “paint-down” of the driverlog and floortile domains, respectively). Another reason that explains that domains are not perfectly learnt (wrt the reference domain) is the appearance in the output models of preconditions that although correct they are usually skipped in the domain definition because of redundancy. For instance, we learned the preconditions (connected ?place1 ?place2) and (connected ?place2 ?place1) for the "move" action in visitall, but only (connected ?place1 ?place2) is specified in the IPC model.


An open question is the identification of the smallest # of objects per type required to learn a complete STRIPS model. This has recently been addressed for Schematic Invariants (Rintanen IJCAI17). We believe this is a promising research direction to obtain similar results in learning STRIPS models.

Regarding the remarks on the lack of comparison with other approaches, we refer the reviewer to the Rebuttal #12637.
