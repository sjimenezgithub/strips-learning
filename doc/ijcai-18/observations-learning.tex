%%%% ijcai18.tex

\typeout{IJCAI-18 Instructions for Authors}

% These are the instructions for authors for IJCAI-18.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).
\usepackage{ijcai18}

% Use the postscript times font!
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{tikz}
\usepackage{comment}

\newcommand{\tup}[1]{{\langle #1 \rangle}}

\newcommand{\pre}{\mathsf{pre}}     % precondition
\newcommand{\del}{\mathsf{del}}     % effect
\newcommand{\add}{\mathsf{add}}     % effect
\newcommand{\eff}{\mathsf{eff}}     % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\true}{\mathsf{true}}   % true
\newcommand{\false}{\mathsf{false}} % false
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % strips

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{multicol}
\usepackage{arydshln}
\usetikzlibrary{calc,backgrounds,positioning,fit}

% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Learning and Recognition of \strips\ Action Models from State Observations}

\author{Diego Aineto\and Sergio Jim\'enez\and Eva Onaindia\\
{\small Departamento de Sistemas Inform\'aticos y Computaci\'on}\\
{\small Universitat Polit\`ecnica de Val\`encia.}\\
{\small Camino de Vera s/n. 46022 Valencia, Spain}\\
{\small \{dieaigar,serjice,onaindia\}@dsic.upv.es}}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a classical planning compilation for learning \strips\ action models from state observations. The compilation approach does not require observing the precise actions that produced the state observations because such actions are determined by a classical planner. In addition, the compilation is extensible to (1) semantically evaluate the quality of learned \strips\ models and (2), estimate the probability distribution of the possible \strips\ models given a sequence of state observations.  
\end{abstract}

\section{Introduction}
Besides {\em plan synthesis}~\cite{ghallab2004automated}, planning action models are also useful for {\em plan/goal recognition}~\cite{ramirez2012plan}. At these planning tasks, automated planners are required to reason about an action model that correctly and completely captures the possible world transitions~\cite{geffner:book:2013}. Unfortunately, building planning action models is complex, even for planning experts, and this knowledge acquisition task is a bottleneck that limits the potential of {\em AI planning}~\cite{kambhampati:modellite:AAAI2007}.

The Machine Learning of planning action models is a promising alternative to hand-coding them and nowadays, there exist sophisticated algorithms like {\sc AMAN}~\cite{zhuo2013action}, {\sc ARMS}~\cite{yang2007learning}, {\sc LOCM}~\cite{cresswell2013acquiring} or {\sc SLAF}~\cite{amir:alearning:JAIR08}. Motivated by recent advances on the synthesis of different kinds of generative models with classical planning~\cite{bonet2009automatic,segovia2016hierarchical,segovia2017generating}, this paper presents a novel approach for learning \strips\ action models that introduces the following contributions:
\begin{enumerate}
\item Is defined as a classical planning compilation. This fact opens the door to the {\em bootstrapping} of planning action models and allows us to report results over a wide range of planning domains. 
\item Does not require observing the particular executed actions. An off-the-shelf classical planner determines these actions given the state observations.
\item Is extensible to semantically evaluate the quality of learned \strips\ models as well as to estimate the probability distribution of the possible \strips\ models given a sequence of state observations. 
\end{enumerate}
 

\section{Background}
This section defines the planning models used in this work as well as the output (an \strips\ action model) of the addressed learning task.

\subsection{Classical planning}
We use $F$ to denote the set of {\em fluents} (propositional variables) describing a state. A {\em literal} $l$ is a valuation of a fluent $f\in F$, i.e. either~$l=f$ or $l=\neg f$. A set of literals $L$ represents a partial assignment of values to fluents (WLOG we assume that $L$ does not assign conflicting values to any fluent). We use $\mathcal{L}(F)$ to denote the set of all literal sets on $F$, i.e.~all partial assignments of values to fluents.

A {\em state} $s$ is a full assignment of values to fluents, i.e. $|s|=|F|$, so the size of the state space is $2^{|F|}$. Explicitly including negative literals $\neg f$ in states simplifies subsequent definitions but often, we will abuse notation by defining a state $s$ only in terms of the fluents that are true in $s$, as is common in \strips\ planning.

A {\em classical planning frame} is a tuple $\Phi=\tup{F,A}$, where $F$ is a set of fluents and $A$ is a set of actions. Each action $a\in A$ comprises three sets of literals:
\begin{itemize}
\item $\pre(a)\subseteq\mathcal{L}(F)$, called {\em preconditions}, the literals that must hold for the action $a\in A$ to be applicable.
\item $\eff^+(a)\subseteq\mathcal{L}(F)$, called {\em positive effects}, that defines the fluents set to true by the application of the action $a\in A$.
\item $\eff^-(a)\subseteq\mathcal{L}(F)$, called {\em negative effects}, that defines the fluents set to false by the action application.
\end{itemize}
We say that an action $a\in A$ is {\em applicable} in a state $s$ iff $\pre(a)\subseteq s$. The result of applying $a$ in $s$ is the {\em successor state} denoted by $\theta(s,a)=\{s\setminus\eff^-(a))\cup\eff^+(a)\}$.

A {\em classical planning problem} is a tuple $P=\tup{F,A,I,G}$, where $I$ is an initial state and $G\subseteq\mathcal{L}(F)$ is a goal condition. A {\em plan} for $P$ is an action sequence $\pi=\tup{a_1, \ldots, a_n}$ that induces the {\em state trajectory} $\tup{s_0, s_1, \ldots, s_n}$ such that $s_0=I$ and, for each {\small $1\leq i\leq n$}, $a_i$ is applicable in $s_{i-1}$ and generates the successor state $s_i=\theta(s_{i-1},a_i)$. The {\em plan length} is denoted with $|\pi|=n$ . A plan $\pi$ {\em solves} $P$ iff $G\subseteq s_n$, i.e.~if the goal condition is satisfied at the last state reached after following the application of the plan $\pi$ in the initial state $I$.


\subsubsection{Classical planning with conditional effects}
Our approach for learning \strips\ action models is compiling the leaning task into a classical planning task with conditional effects.

An action $a\in A$ with conditional effects is defined as a set of {\em preconditions} $\pre(a)\in\mathcal{L}(F)$ and a set of {\em conditional effects} $\cond(a)$. Each conditional effect $C\rhd E\in\cond(a)$ is composed of two sets of literals $C\in\mathcal{L}(F)$, the {\em condition}, and $E\in\mathcal{L}(F)$, the {\em effect}. An action $a\in A$ is {\em applicable} in a state $s$ if and only if $\pre(a)\subseteq s$, and the {\em triggered effects} resulting from the action application are the effects whose conditions hold in $s$:
\[
triggered(s,a)=\bigcup_{C\rhd E\in\cond(a),C\subseteq s} E,
\]

The result of applying action $a$ in state $s$ is the {\em successor} state $\theta(s,a)=\{s\setminus\eff_c^-(s,a))\cup\eff_c^+(s,a)\}$ where $\eff_c^-(s,a)\subseteq triggered(s,a)$ and $\eff_c^+(s,a)\subseteq triggered(s,a)$ are, respectively, the triggered {\em negative} and {\em positive} effects.


\subsection{\strips\ action schemes}
This work addresses the learning of PDDL action schemes that follow the \strips\ requirement~\cite{mcdermott1998pddl,fox2003pddl2}. Figure~\ref{fig:stack} shows the {\em stack} action schema, coded in PDDL, from a four-operator {\em blocksworld}~\cite{slaney2001blocks}.

To formalize the output of the learning task, we assume that fluents $F$ are instantiated from a set of {\em predicates} $\Psi$, as in PDDL. Each predicate $p\in\Psi$ has an argument list of arity $ar(p)$. Given a set of {\em objects} $\Omega$, the set of fluents $F$ is induced by assigning objects in $\Omega$ to the arguments of predicates in $\Psi$, i.e.~$F=\{p(\omega):p\in\Psi,\omega\in\Omega^{ar(p)}\}$ s.t. $\Omega^k$ is the $k$-th Cartesian power of $\Omega$.

Let $\Omega_v=\{v_i\}_{i=1}^{\operatorname*{max}_{a\in A} ar(a)}$ be a new set of objects ($\Omega\cap\Omega_v=\emptyset$), denoted as {\em variable names}, and that is bound by the maximum arity of an action in a given planning frame. For instance, in a three-block blocksworld $\Omega=\{block_1, block_2, block_3\}$ while $\Omega_v=\{v_1, v_2\}$ because the operators with the maximum arity, {\small\tt stack} and {\small\tt unstack}, have arity two.

Let us also define $F_v$, a new set of fluents $F\cap F_v=\emptyset$, that results from instantiating $\Psi$ using only the objects in $\Omega_v$ and that defines the elements that can appear in an action schema. For the blocksworld, $F_v$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$), clear($v_2$), ontable($v_1$), ontable($v_2$), on($v_1,v_1$), on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}.

We assume also that actions $a\in A$ are instantiated from \strips\ operator schemes $\xi=\tup{head(\xi),pre(\xi),add(\xi),del(\xi)}$ where:
\begin{itemize}
\item $head(\xi)=\tup{name(\xi),pars(\xi)}$, is the operator {\em header} defined by its name and the corresponding {\em variable names}, $pars(\xi)=\{v_i\}_{i=1}^{ar(\xi)}$. The headers of a four-operator blocksworld are {\small\tt pickup($v_1$), putdown($v_1$), stack($v_1,v_2$)} and {\small\tt unstack($v_1,v_2$)}.
\item The preconditions $pre(\xi)\subseteq F_v$, the negative effects $del(\xi)\subseteq F_v$, and the positive effects $add(\xi)\subseteq F_v$ such that, $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$.
\end{itemize}

Finally we also define $F_v(\xi)\subseteq F_v$ as the subset of elements that can appear in a given action schema $\xi$. For instance, for the {\em stack} action schema $F_v({\tt stack})=F_v$ while $F_v({\tt pickup})$={\small\tt\{handempty, holding($v_1$), clear($v_1$), ontable($v_1$), on($v_1,v_1$)\}} only contains the fluents from $F_v$ that do not involve $v_2$ because the action header contained the single parameter $v_1$.


\begin{figure}
\begin{footnotesize}
\begin{verbatim}
(:action stack
 :parameters (?v1 ?v2 - object)
 :precondition (and (holding ?v1) (clear ?v2))
  :effect (and (not (holding ?v1)) 
               (not (clear ?v2))
               (handempty) (clear ?v1) 
               (on ?v1 ?v2)))
\end{verbatim}
\end{footnotesize}
 \caption{\small \strips\ operator schema coding, in PDDL, the {\em stack} action from the {\em blocksworld}.}
\label{fig:stack}
\end{figure}


\section{Learning \strips\ action models}
Learning \strips\ action models from fully available input knowledge, i.e. from plans where every action in the plan is available as well as its corresponding {\em pre-} and {\em post-states}, is straightforward. In this case, \strips\ operator schemes are derived lifting the literals that change between the pre and post-state of the corresponding action executions. Preconditions are derived lifting the minimal set of literals that appears in all the pre-states of the corresponding actions.

In this paper we address a more challenging learning task, where less input knowledge is available. The addressed learning task corresponds to observing an agent acting in the world but watching only the results of its plan executions, the actual executed actions are unobserved. This learning task is defined as $\Lambda=\tup{\Psi,\Xi,\mathcal{O}}$, where:
\begin{itemize}
\item $\Psi$ is the set of predicates that define the abstract state space of a given classical planning frame.
\item $\Xi$ is the set of empty operator schemes containing only the headers with the name and parameter of each operator schema $\xi\in\Xi$.
\item $\mathcal{O}=\tup{s_0,s_1,\ldots,s_{n}}$ is a sequence of {\em state observations} obtained observing the execution of an {\em unobserved} plan $\pi=\tup{a_1, \ldots, a_n}$.
\end{itemize}

A solution to $\Lambda$ is a set of operator schema $\Xi'$ compliant with the predicates in $\Psi$, the headers in $\Xi$ and the given sequence of state observations $\mathcal{O}$. A planning compilation is a suitable approach for addressing a $\Lambda$ learning task because a solution must not only determine the \strips\ action model $\Xi'$ but also, the {\em unobserved} plan $\pi=\tup{a_1, \ldots, a_n}$, that explains $\mathcal{O}$. Figure~\ref{fig:lexample} shows an example of a $\Lambda$ task for learning a \strips\ action model in the blocksworld from the sequence of five state observations that corresponds to inverting a 2-blocks tower.

\begin{figure}[hbt!]
{\footnotesize\tt ;;; Predicates in $\Psi$}
\begin{footnotesize}
\begin{verbatim}
(handempty) (holding ?o  - object)
(clear ?o - object) (ontable ?o - object)
(on ?o1 - object ?o2 - object)
\end{verbatim}
\end{footnotesize}
\vspace{0.2cm}
{\footnotesize\tt ;;; Headers in $\Xi$}
\begin{footnotesize}
\begin{verbatim}
(pickup v1) (putdown v1) 
(stack v1 v2} (unstack v1 v2)
\end{verbatim}
\end{footnotesize}
\vspace{0.2cm}
{\footnotesize\tt ;;; Observations in $\mathcal{O}$}
\begin{footnotesize}
\begin{verbatim}
;;; observation #0
(clear block2) (on block2 block1) 
(ontable block1) (handempty) 

;;; observation #1
(holding block2) (clear block1) 
(ontable block1)

;;; observation #2
(clear block1) (ontable block1) 
(clear block2) (ontable block2) 
(handempty) 

;;; observation #3
(holding block1) (clear block2) 
(ontable block2)

;;; observation #4
(clear block1) (on block1 block2) 
(ontable block2) (handempty) 
\end{verbatim}
\end{footnotesize}
 \caption{\small Example of a $\Lambda$ task for learning a \strips\ action model in the {\em blocksworld} from a sequence of five state observations.}
\label{fig:lexample}
\end{figure}


\subsection{Learning with classical planning}

Our approach for addressing the learning task, is compiling it into a classical planning task with conditional effects. The intuition behind the compilation is that a solution to the resulting classical planning task is a sequence of actions that:
\begin{enumerate}
\item Programs the \strips\ action model $\Xi'$. A solution plan starts with a {\em prefix} that, for each $\xi\in\Xi$, determines which fluents $f\in F_v(\xi)$ belong to its $pre(\xi)$, $del(\xi)$ and $add(\xi)$ sets.
\item Validates the \strips\ action model $\Xi'$ in $\mathcal{O}$. The solution plan continues with a postfix that produces the given sequence of states $\tup{s_0,s_1,\ldots,s_{n}}$ using the programmed action model $\Xi'$. 
\end{enumerate}

Given a learning task $\Lambda=\tup{\Psi,\Xi,\mathcal{O}}$ the compilation outputs a classical planning task $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I_{\Lambda},G_{\Lambda}}$:
\begin{itemize}
\item $F_{\Lambda}$ contains:
\begin{itemize}
\item The set of fluents $F$ that is built instantiating the predicates $\Psi$ with the objects appearing in the input observations $\mathcal{O}$. 
\item Fluents representing the programmed action model $pre_f(\xi)$, $del_f(\xi)$ and $add_f(\xi)$, for every $f\in F_v(\xi)$. If a fluent $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ holds, it means that $f$ is a precondition/negative effect/positive effect in the \strips\ operator schema $\xi\in \Xi$. For instance, the preconditions of the $stack$ schema (Figure~\ref{fig:stack}) are represented by the fluents {\small\tt pre\_holding\_stack\_$v_1$} and {\small\tt pre\_clear\_stack\_$v_2$} set to {\em True}.
\item Fluent $mode_{prog}$ indicating whether the operator schemes are programmed or validated (already programmed) and fluents $\{test_i\}_{1\leq i\leq n}$, indicating the observation where the action model is validated.
\end{itemize}
\item $I_{\Lambda}$ contains the fluents from $F$ that encode $s_0$ (the first observation) and $mode_{prog}$ set to true. In addition, our compilation assumes that initially operator schemes are programmed with every possible precondition, no negative effect and no positive effect. With this regard, the fluents $pre_f(\xi)$ hold at the initial state for every $f\in F_v(\xi)$.
\item $G_{\Lambda}=\bigcup_{1\leq i\leq n}\{test_i\}$, indicates that the programmed action model is validated in all the input observations.
\item $A_{\Lambda}$ comprises three kinds of actions:
\begin{enumerate}
\item Actions for {\em programming} operator schema $\xi\in\Xi$:
\begin{itemize}
\item Actions for {\bf removing} a {\em precondition} $f\in F_v(\xi)$ from the action schema $\xi\in\Xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programPre_{f,\xi}})=&\{\neg del_{f}(\xi),\neg add_{f}(\xi),\\
& mode_{prog}, pre_{f}(\xi)\},\\
\cond(\mathsf{programPre_{f,\xi}})=&\{\emptyset\}\rhd\{\neg pre_{f}(\xi)\}.
\end{align*}
\end{small}

\item Actions for {\bf adding} a {\em negative} or {\em positive} effect $f\in F_v(\xi)$ to the action schema $\xi\in\Xi$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programEff_{f,\xi}})=&\{\neg del_{f}(\xi),\neg add_{f}(\xi),\\
& mode_{prog}\},\\
\cond(\mathsf{programEff_{f,\xi}})=&\{pre_{f}(\xi)\}\rhd\{del_{f}(\xi)\},\\
&\{\neg pre_{f}(\xi)\}\rhd\{add_{f}(\xi)\}.
\end{align*}
\end{small}
\end{itemize}

\item Actions for {\em applying} an already programmed operator schema $\xi\in\Xi$ bound with the objects $\omega\subseteq\Omega^{ar(\xi)}$. We assume that the operators headers are known so the binding of the operator schema is done implicitly by order of appearance of the action parameters, i.e. variables $pars(\xi)$ are bound to the objects in $\omega$ appearing at the same position. Figure~\ref{fig:compilation} shows the PDDL encoding of the action for applying a programmed operator $stack$.
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{apply_{\xi,\omega}})=&\{pre_{f}(\xi)\implies p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{del_{f}(\xi)\}\rhd\{\neg p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{add_{f}(\xi)\}\rhd\{p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
&\{mode_{prog}\}\rhd\{\neg mode_{prog}\}.
\end{align*}
\end{small}

\item Actions for {\em validating} an observation {\tt\small $1\leq i\leq n$}. 
\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{i}})=&s_i\cup\{test_j\}_{j\in 1\leq j<i}\\
&\cup\{\neg test_j\}_{j\in i\leq j\leq n}\cup \{\neg mode_{prog}\},\\
\cond(\mathsf{validate_{i}})=&\{\emptyset\}\rhd\{test_i\}.
\end{align*}
\end{small}
\end{enumerate}
\end{itemize}


\begin{figure}[hbt!]
\begin{scriptsize}
\begin{verbatim}
(:action apply_stack
  :parameters (?o1 - object ?o2 - object)
  :precondition
   (and (or (not (pre_on_stack_v1_v1)) (on ?o1 ?o1))
        (or (not (pre_on_stack_v1_v2)) (on ?o1 ?o2))
        (or (not (pre_on_stack_v2_v1)) (on ?o2 ?o1))
        (or (not (pre_on_stack_v2_v2)) (on ?o2 ?o2))
        (or (not (pre_ontable_stack_v1)) (ontable ?o1))
        (or (not (pre_ontable_stack_v2)) (ontable ?o2))
        (or (not (pre_clear_stack_v1)) (clear ?o1))
        (or (not (pre_clear_stack_v2)) (clear ?o2))
        (or (not (pre_holding_stack_v1)) (holding ?o1))
        (or (not (pre_holding_stack_v2)) (holding ?o2))
        (or (not (pre_handempty_stack)) (handempty)))
  :effect
   (and (when (del_on_stack_v1_v1) (not (on ?o1 ?o1)))
        (when (del_on_stack_v1_v2) (not (on ?o1 ?o2)))
        (when (del_on_stack_v2_v1) (not (on ?o2 ?o1)))
        (when (del_on_stack_v2_v2) (not (on ?o2 ?o2)))
        (when (del_ontable_stack_v1) (not (ontable ?o1)))
        (when (del_ontable_stack_v2) (not (ontable ?o2)))
        (when (del_clear_stack_v1) (not (clear ?o1)))
        (when (del_clear_stack_v2) (not (clear ?o2)))
        (when (del_holding_stack_v1) (not (holding ?o1)))
        (when (del_holding_stack_v2) (not (holding ?o2)))
        (when (del_handempty_stack) (not (handempty)))
        (when (add_on_stack_v1_v1) (on ?o1 ?o1))
        (when (add_on_stack_v1_v2) (on ?o1 ?o2))
        (when (add_on_stack_v2_v1) (on ?o2 ?o1))
        (when (add_on_stack_v2_v2) (on ?o2 ?o2))
        (when (add_ontable_stack_v1) (ontable ?o1))
        (when (add_ontable_stack_v2) (ontable ?o2))
        (when (add_clear_stack_v1) (clear ?o1))
        (when (add_clear_stack_v2) (clear ?o2))
        (when (add_holding_stack_v1) (holding ?o1))
        (when (add_holding_stack_v2) (holding ?o2))
        (when (add_handempty_stack) (handempty))
        (when (modeProg) (not (modeProg)))))
\end{verbatim}
\end{scriptsize}
 \caption{\small Action for applying an already programmed schema $stack$ as encoded in PDDL (implications coded as disjunctions).}
\label{fig:compilation}
\end{figure}


\subsection{Compilation properties}

\begin{lemma}
Soundness. Any classical plan $\pi$ that solves $P_{\Lambda}$ induces an action model $\Xi'$ that solves the learning task $\Lambda$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
The compilation forces that once the preconditions of an operator schema $\xi \in \Xi'$ are programmed, they cannot be altered. The same happens with the positive and negative effects. Furthermore because of the preconditions of the $\mathsf{programPre_{f,\xi}}$ actions, effects are only programmable after preconditions are programmed. Once the operator schemes $\Xi'$ are programmed, they can only be applied because of the $mode_{prog}$ fluent. To solve $P_{\Lambda}$, the goals $\{test_i\}$, {\small $1\leq i\leq n$} can only be achieved: executing an applicable sequence of programmed operator schemes that reaches every state $s_i\in\mathcal{O}$, starting from $s_0$ and following  the sequence {\small $1\leq i\leq n$}. Therefore if $test_n$ is achieved, it means that the programmed action model $\Xi'$ is compliant with the provided input knowledge and hence, solves $\Lambda$.
\end{small}
\end{proof}


\begin{lemma}
Completeness. Any \strips\ action model $\Xi'$ that solves a $\Lambda=\tup{\Psi,\Xi,\mathcal{O}}$ learning task, is computable solving the corresponding classical planning task $P_{\Lambda}$.
\end{lemma}

\begin{proof}[Proof sketch]
\begin{small}
By definition, $F_v(\xi)\subseteq F_\Lambda$ fully captures the set of elements that can appear in a \strips\ action schema $\xi\in\Xi$. In addition, any possible \strips\ action schema $\Xi'$ that can be built with the fluents in $F_v$ can be computed  with the $P_{\Lambda}$ compilation. The only \strips\ action models that the compilation cannot compute are the ones that violate the state constraints defined by the $\mathcal{O}$ sequence, which constrain as well the solutions to the $\Lambda$ learning task.
\end{small}
\end{proof}


\section{Recognizing \strips\ action models}
Inspired by {\em plan recognition as planning}~\cite{ramirez2009plan}, we show that the previous compilation can be extended to (1) semantically evaluate the quality of learned \strips\ models, without comparing with a reference model and (2), address \strips\ action model recognition tasks.

\subsection{The \strips\ edit distance}
We assume that how well a given \strips\ action model $\Xi$ explains a given sequence of observations $\mathcal{O}$, depends on the amount of {\em edition} that one has to introduce to $\Xi$ to produce the sequence of obsevations $\mathcal{O}$. In the extreme, if the given model $\Xi$ perfectly explains the observations $\mathcal{O}$, no {\em edit} to the model must be introduced. 

Let us define the operations allowed to edit a given \strips\ action model. We call these operations the {\em \strips\ edit operations}. With the aim of keeping tractable the branching factor of the classical planning task that results from our compilation, we only define two {\em \strips\ edit operations}:
\begin{itemize}
\item {\em Deletion}. A fluent $pre_f(\xi)/del_f(\xi)/add_f(\xi)$, such that $f\in F_v(\xi)$, is removed from the operator schema $\xi\in\Xi$.
\item {\em Insertion}. A fluent $pre_f(\xi)/del_f(\xi)/add_f(\xi)$, such that $f\in F_v(\xi)$, is added to the operator schema $\xi\in\Xi$.
\end{itemize}

With this defined, we can now formalize an edit distance metric that assesses how dissimilar two given \strips\ action models are. This distance satisfies the {\em metric axioms} provided that the two {\em \strips\ edit operations} have the same positive cost.
\begin{definition}
Given two \strips\ action model $\Xi$ and $\Xi'$, such that both are built from the same set of possible elements $F_v$. The {\em \bf \strips\ edit distance}, denoted as $\delta(\Xi,\Xi')$, is the minimum number of {\em \strips\ edit operations} to transform $\Xi$ into $\Xi'$.
\end{definition}

Since the size of the $F_v$ set is bound, the maximum number of edits that can be introduced to a given \strips\ action model is bound as well. In more detail, for a given operator schema $\xi\in\Xi$ the maximum number of edits that can be introduced to their precondition set is $|F_v(\xi)|$. With regard to effects of $\xi$, the maximum number of edits that can be introduced are two times $|F_v(\xi)|$.

\begin{definition}
Given a \strips\ action model $\Xi$ built from the set of possible elements $F_v$. The {\em \bf maximum  \strips\ edit distance} is the upperbound $\delta(\Xi,*)=\sum_{\xi\in\Xi} 3|F_v(\xi)|$, that indicates the maximum value of the {\em \strips\ edit distance} from $\Xi$ to any \strips\ action model definable within $F_v$. 
\end{definition}


Likewise the {\em \strips\ edit distance} can be defined with regard to a sequence of observations $\mathcal{O}$.
\begin{definition}
 Given a \strips\ action model $\Xi$ built from the set of possible elements $F_v$ and a sequence of observations $\mathcal{O}$. The {\em \bf observations  \strips\ edit distance}, denoted by  $\delta(\Xi,\mathcal{O})$, is the minimum number of {\em \strips\ edit operations} to transform $\Xi$ into an action model $\Xi'$ defined within $F_v$ that can produce a plan $\pi=\tup{a_1, \ldots, a_n}$ such that $\pi$ induces the state observations $\mathcal{O}=\tup{s_0, s_1, \ldots, s_n}$.
\end{definition}

The {\em observations  \strips\ edit distance} allow us to semantically evaluate the quality of a learned strips action model with respect to a sequence of observations that acts as a test set. The lower this distance is, the better the quality of the learned model. The semantic nature of this evaluation is robust to learning episodes where actions are reformulated and still compliant with the inputs (e.g. the {\em blocksworld} operator {\small\tt stack} could be {\em learned} with the preconditions and effects of the {\small\tt unstack} operator and vice versa or the roles of actions parameters with the same type could be interchanged).


\subsection{Recognition of \strips\ models}
Given a set of possible \strips\ models and a sequence of state observations the {\em recognition of \strips\ models} is the task of computing the probability distribution of these models according to the given observations. Here we show that the {\em observations  \strips\ edit distance} is helpful to compute the probability distribution of the possible \strips\ models given a sequence of state observations. The main idea, taken from {\em plan recognition as planning}~\cite{ramirez2009plan}, is to map these distances into likelihoods using the Bayes rule.

According to the Bayes rule, the probability of an hypothesis $\mathcal{H}$ given the observations $\mathcal{O}$ can be computed with $P(\mathcal{H}|\mathcal{O})=\frac{P(\mathcal{O}|\mathcal{H})P(\mathcal{H})}{P(\mathcal{O})}$. In our scenario, the hypotheses are about the possible \strips\ action models that can be build given a set of operator headers, or in other words, given the $F_v(\xi)$ sets. Moreover, we assume that all the possible \strips\ action models are equiprobable so according to the Bayes rule $P(\mathcal{H}|\mathcal{O})$ is proportional to $P(\mathcal{O}|\mathcal{H})$. Finally, we assume that $P(\mathcal{O}|\mathcal{H})$ is given by the {\em observations  \strips\ edit distance}, mapping distances into probabilities (the larger the distance the lower the likelihood), according to the following expression $P(\mathcal{O}|\Xi)=1-\frac{\delta(\Xi,\mathcal{O})}{\delta(\Xi,*)}$.


With this regard, the probability distribution of the possible \strips\ models (within the set of operator headers, that is within the $F_v(\xi)$ sets) given a sequence of state observations,  $P(\Xi|\mathcal{O})$, can be computed by:
\begin{enumerate}
\item The {\em observations  \strips\ edit distance} is computed for every possible model.
\item The resulting distances are plugged into the previous$P(\Xi|\mathcal{O})$ equation to map distances into likelihoods.
\item Likelihoods are plugged into the Bayesâ€™ rule $P(\Xi|\mathcal{O})$ to obtain the normalized posterior probabilities. Since we are computing a probability distribution these probabilities must sum up $1.0$.
\end{enumerate}



\subsection{\strips\ model evaluation and recognition with classical planning}
The previous compilation can be extended to estimate the {\em observations  \strips\ edit distance}. In this case the tuple $\Lambda=\tup{\Psi,\Xi,\mathcal{O}}$ does not represent a learning task but the edition of a given \strips\ action model $\Xi$ to cover the sequence of observations $\mathcal{O}$. The extended compilation outputs a classical planning task $P_{\Lambda}'=\tup{F_{\Lambda},A_{\Lambda}',I_{\Lambda}',G_{\Lambda}}$ where:
\begin{itemize}
\item $F_{\Lambda}$ is defined as in the previous compilation.
\item $I_{\Lambda}'$ contains the fluents from $F$ that encode $s_0$ (the first observation) and $mode_{prog}$ set to true. In addition, the given \strips\ action model $\Xi$ is now encoded in the initial state. This means that fluents $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ with $f\in F_v(\xi)$ hold in the initial state if they appear in the given \strips\ action model $\Xi$.
\item $G_{\Lambda}$ is defined as in the previous compilation.
\item $A_{\Lambda}'$, comprises the three kinds of actions of $A_{\Lambda}$. The Actions for {\em applying} an already programmed operator schema and the actions for {\em validating} an observation {\tt\small $1\leq i\leq n$} are defined exactly as in the previous compilation. The only difference are the actions for {\em programming} operator schema $\xi\in\Xi$ that must include now actions for adding a precondition and for removing a {\em negative} or {\em positive} effect.
\end{itemize}

The probability distribution $P(\Xi|\mathcal{O})$ would be exact if the classical planning problems resulting from our compilation are solved optimally (according to the number of edit actions), and will be approximate if they are solved with a satisfying classical planner or furthermore if what is solved is not $P_{\Lambda}'$ but a relaxation of this task such as the {\em delete relaxation}.


\section{Evaluation}
This section evaluates our approach for learning \strips\ models starting from different amounts of input knowledge.

\subsubsection{Reproducibility}
We used IPC domains that satisfy the \strips\ requirement~\cite{fox2003pddl2}, taken from the {\sc planning.domains} repository~\cite{muise2016planning}. For the learning of the \strips\ action models we used observations sequences of 25 states per domain. All experiments are run on an Intel Core i5 3.10 GHz x 4 with 4 GB of RAM.

{\sc Madagascar} is the classical planner we use to solve the instances that result from our compilations because its ability to deal with dead-ends~\cite{rintanen2014madagascar}. In addition, {\sc Madagascar} can apply the actions for programming preconditions in a single planning step (in parallel) because these actions do not interact. Actions for programming action effects can also be applied in a single planning step reducing significantly the planning horizon.

The compilation source code, the evaluation scripts and the benchmarks are fully available at this anonymous repository {\em https://github.com/anonsub/strips-learning} so any experimental data reported in the paper can be reproduced. 

\subsubsection{Supervised evaluation of the learned models}
For each domain the learned model is compared with the actual model and its quality is quantified with the {\em precision} and {\em recall} metrics. Precision gives a notion of {\em soundness} while recall gives a notion of the {\em completeness} of the learned models. $Precision=\frac{tp}{tp+fp}$, where $tp$ is the number of true positives (predicates that correctly appear in the action model) and $fp$ is the number of false positives (predicates appear in the learned action model that should not appear). $Recall=\frac{tp}{tp+fn}$ where $fn$ is the number of false negatives (predicates that should appear in the learned action model but are missing).

When the learning hypothesis space is low constrained, the learned actions can be reformulated and still be compliant with the inputs. For instance in the {\em blocksworld}, operator {\small\tt stack} could be {\em learned} with the preconditions and effects of the {\small\tt unstack} operator (and vice versa). Furthermore, in a given action the role of the parameters that share the same type can be interchanged making non trivial to compute {\em precision} and {\em recall} with respect to a reference model.

To address these issues we defined an evaluation method robust to action reformulation. Precision and recall are often combined using the {\em harmonic mean}. This expression is called the {\em F-measure} (or the balanced {\em F-score}) and is formally defined as $F=2\times\frac{Precision\times Recall}{Precision+Recall}$. Given a reference \strips\ action model $\Xi^*$ and the learned \strips\ action model $\Xi$ we define the bijective function $f_{P\&R}:\Xi \mapsto \Xi^*$ such that $f_{P\&R}$ maximizes the accumulated {\em F-measure}. With this mapping defined we can compute the {\em precision} and {\em recall} of a learned \strips\ action $\xi\in\Xi$ with respect to the action $f_{P\&R}(\xi)\in \Xi^*$ even if actions are reformulated in the learning process.


\section{Conclusions}
As far as we know, this is the first work on learning \strips\ action models from state observations, exclusively using classical planning and evaluated over a wide range of different domains. Recently, ~\citeauthor{stern2017efficient}~\citeyear{stern2017efficient} proposed a classical planning compilation for learning action models but following the {\em finite domain} representation for the state variables and did not report experimental results since the compilation was not implemented.

%The empirical results show that since our approach is strongly based on inference can generate non-trivial models from very small data sets. In addition, the SAT-based planner {\sc Madagascar} is particularly suitable for the approach because its ability to deal with planning instances populated with dead-ends and because many actions for programming the \strips\ model can be done in parallel since they do not interact reducing significantly the planning horizon.

The size of the compiled classical planning instances depends on the number of input examples. Generating {\em informative} examples for learning planning action models is an open issue. Planning actions include preconditions that are only satisfied by specific sequences of actions, often, with a low probability of being chosen by chance~\cite{fern2004learning}. The success of recent algorithms for exploring planning tasks~\cite{geffner:novelty:IJCAI17} motivates the development of novel techniques able to autonomously collect informative learning examples. The combination of such exploration techniques with our learning approach is an intriguing research direction that opens the door to the bootstrapping of planning action models.

%Instead of enumerating the full sequence of states included in a trajectory, {\em state trajectory constraints} can be implicitly defined with {\em Linear Temporal Logic} (LTL)~\cite{haslum:LTL:ecai10}. For instance the LTL {\em eventually} operator, denoted by $\lozenge$, can define constraints that, unlike {\em state invariants}, must be true {\em at least at one} of the reached states. Despite this is beyond the scope of this paper, LTL constraints could be included in our compilation following the ideas for compiling temporally extended goals into classical planning~\cite{baier2006planning} that (1) transform the given LTL formula into an equivalent automata, (2) compute the cross product of this automata with the given classical planning task and (3) force the solution plans to always leave the LTL automata at an acceptor state by adding new goals to the classical planning task.

% Commented for blind submission
%\begin{small}
%\subsection*{Acknowledgment}
%Diego Aineto is partially supported by the {\it FPU} program funded by the Spanish government. Sergio Jim\'enez is partially supported by the {\it Ramon y Cajal} program funded by the Spanish government.
%\end{small}



\begin{table}
	\begin{center}
		\begin{scriptsize}                
			\begin{tabular}{l|l|l|l|l|l|l||l|l|}
				& \multicolumn{2}{|c|}{\bf Pre} & \multicolumn{2}{|c|}{\bf Add} & \multicolumn{2}{|c||}{\bf Del} & \multicolumn{2}{|c}{\bf}\\ \cline{2-9}			
				& \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} & \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} & \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c||}{\bf R} &  \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} \\
				\hline
				blocks & 0.57 & 0.44 & 0.63 & 0.56 & 0.57 & 0.44 & 0.59 & 0.48 \\
				driverlog & 0.2 & 0.07 & 0.18 & 0.29 & 0.2 & 0.14 & 0.19 & 0.17 \\
				ferry & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
				floor-tile & 0.5 & 0.14 & 0.75 & 0.55 & 0.6 & 0.27 & 0.62 & 0.32 \\
				Grid & - & - & - & - & - & - & - & - \\ % Segmentation fault
				gripper-strips & 0.25 & 0.17 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.22 \\
				hanoi & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
				hiking & - & - & - & - & - & - & - & - \\ % Time out 1000s
				n-puzzle & - & - & - & - & - & - & - & - \\ % Time out 1000s
				parking & 0.6 & 0.21 & 0.14 & 0.11 & 0.4 & 0.22 & 0.38 & 0.18 \\
				satellite & 0.25 & 0.07 & 0.33 & 0.4 & 0.67 & 0.5 & 0.42 & 0.32 \\
				Sokoban & - & - & - & - & - & - & - & - \\ % Time out 1000s
				transport & 0.33 & 0.1 & 0.33 & 0.4 & 0.0 & 0.0 & 0.22 & 0.17 \\
				zeno-travel & 0.25 & 0.07 & 0.17 & 0.14 & 0.0 & 0.0 & 0.14 & 0.07 \\
				\hline
				\bf  & - & - & - & - & - & - & - & - \\
			\end{tabular}
		\end{scriptsize}
	\end{center}
	\caption{\small Precision and recall values obtained when learning from labels without computing the $f_{P\&R}$ mapping.}
	\label{fig:labelsnomap}                
	
	\begin{center}
		\begin{scriptsize}                
			\begin{tabular}{l|l|l|l|l|l|l||l|l|}
				& \multicolumn{2}{|c|}{\bf Pre} & \multicolumn{2}{|c|}{\bf Add} & \multicolumn{2}{|c||}{\bf Del} & \multicolumn{2}{|c}{\bf}\\ \cline{2-9}			
				& \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} & \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} & \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c||}{\bf R} &  \multicolumn{1}{|c|}{\bf P} & \multicolumn{1}{|c|}{\bf R} \\
				\hline
				blocks & 0.71 & 0.56 & 0.75 & 0.67 & 0.71 & 0.56 & 0.73 & 0.59 \\
				driverlog & 0.6 & 0.21 & 0.27 & 0.43 & 0.6 & 0.43 & 0.49 & 0.36 \\
				ferry & 0.5 & 0.29 & 0.75 & 0.75 & 0.5 & 0.5 & 0.58 & 0.51 \\
				floor-tile & 0.5 & 0.14 & 0.75 & 0.55 & 0.6 & 0.27 & 0.62 & 0.32 \\
				Grid & - & - & - & - & - & - & - & - \\ % Segmentation fault
				gripper-strips & 1.0 & 0.67 & 1.0 & 1.0 & 1.0 & 1.0 & 1.0 & 0.89 \\
				hanoi & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\
				hiking & - & - & - & - & - & - & - & - \\ % Time out 1000s
				n-puzzle & - & - & - & - & - & - & - & - \\ % Time out 1000s
				parking & 0.6 & 0.21 & 0.14 & 0.11 & 0.4 & 0.22 & 0.38 & 0.18 \\
				satellite & 0.25 & 0.07 & 0.33 & 0.4 & 0.67 & 0.5 & 0.42 & 0.32 \\
				Sokoban & - & - & - & - & - & - & - & - \\ % Time out 1000s
				transport & 1.0 & 0.3 & 0.67 & 0.8 & 0.67 & 0.4 & 0.78 & 0.5 \\
				zeno-travel & 0.5 & 0.14 & 0.5 & 0.43 & 0.33 & 0.14 & 0.44 & 0.24 \\
				\hline
				\bf  & - & - & - & - & - & - & - & - \\
			\end{tabular}
		\end{scriptsize}
	\end{center}
	\caption{\small Precision and recall values obtained when learning from labels but computing the $f_{P\&R}$ mapping.}
	\label{fig:labels}                
\end{table}

\begin{table}
	\begin{footnotesize}
		\begin{center}
			\begin{tabular}{l|c|c|c|}			
				& Total time & Preprocess & Plan length  \\
				\hline
				Blocks & 1.40 & 0.00 & 70  \\
				Driverlog & 1.50 & 0.00 & 89 \\
				Ferry & 1.49 & 0.00 & 64 \\
				Floortile & 351.38 & 0.11 & 156 \\
				Grid & - & - & - \\ % Segmentation fault
				Gripper & 0.04 & 0.00 & 59 \\
				Hanoi & 2.33 & 0.01 & 49  \\
				Hiking & - & - & - \\ % Time out 1000s
				Parking & - & - & - \\ % Time out 1000s
				Satellite & 78.36 & 0.03 & 98 \\
				Sokoban & - & - & - \\ % Time out 1000s
				Transport & 285.61 & 0.10 & 106 \\
				Zenotravel & 6.20 & 0.46 & 71 \\
			\end{tabular}
		\end{center}
	\end{footnotesize}
 \caption{\small Planning results obtained when learning from labels.}
\label{fig:planlabels}                        
\end{table}





%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{observations-learning}

\end{document}

