
\section{Learning action models from plan executions}
\label{sec:learning}

This section details the \FAMA approach for learning action models. The notion of the learning task is defined in section \ref{learning_task} and the components of the compilation scheme are described in sections \ref{propositional_encoding} and \ref{conditional_effects}. Subsequently, the compilation approach is fully detailed in section \ref{compilation} and the last section presents some theoretical properties of the compilation scheme.

\FAMA addresses the learning and evaluation of PDDL action models that follow the \strips\ requirement~\cite{mcdermott1998pddl,fox2003pddl2}. An \strips\ action model is a tuple $\xi=\tup{name(\xi),pars(\xi),pre(\xi),add(\xi),del(\xi)}$ where:

\begin{itemize}
	\item The name, $name(\xi)$, and parameters, $pars(\xi)$, of the action model define the {\em header} of the model.
	\item $pre(\xi)$, $del(\xi)$ and $add(\xi)$ represent the preconditions, negative effects and positive effects of the action model, respectively, such that, $del(\xi)\subseteq pre(\xi)$, $del(\xi)\cap add(\xi)=\emptyset$ and $pre(\xi)\cap add(\xi)=\emptyset$.
\end{itemize}


As an example, Figure~\ref{fig:stack} shows the action model of the {\em stack} operator from the four-operator {\em blocksworld} domain~\cite{slaney2001blocks} encoded in PDDL.

\begin{figure}[hbt!]
	\begin{footnotesize}
		\begin{verbatim}
		(:action stack
		:parameters (?v1 ?v2 - object)
		:precondition (and (holding ?v1) (clear ?v2))
		:effect (and (not (holding ?v1)) (not (clear ?v2)) (handempty) (clear ?v1) (on ?v1 ?v2)))
		\end{verbatim}
	\end{footnotesize}
	\caption{PDDL encoding of the action model of the {\em stack} operator from the four-operator {\em blocksworld} domain.}
	\label{fig:stack}
\end{figure}



\subsection{Learning Task}
\label{learning_task}

Our {\em learning task} consists in learning classical planning action models by observing one or more agents acting in a world definable by a {\em classical planning frame} $\Phi=\tup{F,A}$. The learning task is formalized by the pair $\Lambda=\tup{\mathcal{M},\tau}$:


\begin{itemize}
\item $\mathcal{M}$ is the set of {\bf initial action models}. This set is {\em empty}, when learning from scratch, or {\em partially specified}, when some fragments of the action models are known a priori.
\item $\tau$ is the observed {\bf plan trace} such that:
\begin{enumerate}
 \item Observations in $\tau$ are {\em noiseless}, meaning that if the value of a fluent or an action is observed in $\tau$, then the observation is correct.
\item The initial state $s_0\in\tau$ is a {\em fully observed} state including positive and negative fluents, i.e.~$|s_0|=|F|$. Consequently, the corresponding set of predicates $\Psi$ and objects $\Omega$ that shape the fluents in $F$ are inferrable from $s_0$.
\item The header of an action model is either given by $\mathcal{M}$ or inferrable from $\tau$. In the latter case, $\tau$ must contain at least one instantiation of the respective action model header.
%\item The set of actions $A$ is inferrable either from $\mathcal{M}$ or $\tau$. In other words, the action name and parameters that shape the actions in $A$ are given by $\mathcal{M}$ otherwise, $\tau$ contains at least one instantiation of every action model in $\mathcal{M}$.
\item We allow plan traces with \NO state trajectories and action sequences. In the extreme, all actions and intermediate states may be missing, provided that the final state is at least partially observed. The least informative plan trace is thus $\tau = \tup {s_0,s_n}$.
\end{enumerate}
\end{itemize}

Ultimately, we can always assume that $\mathcal{M}$ will contain predicates in $\Psi$ as well as the headers of the actions models, either explicitly provided in $\mathcal{M}$ or inferred from $\tau$. A {\em solution} to a learning task $\Lambda=\tup{\mathcal{M},\tau}$ is a set of action models $\mathcal{M}'$ that is compliant with the input models $\mathcal{M}$ and the observed plan trace $\tau$.

Figure~\ref{fig:example-plans} shows an example of a learning task $\Lambda=\tup{\mathcal{M},\tau}$ corresponding to the observation of the execution of the four-action plan $\pi=\tup{\small\tt (unstack\ B\ A), (putdown\ B), (pickup\ A), (stack\ A\ B)}$ for inverting a two-block tower. In this example $\tau=\langle s_0,${\small\tt (putdown\ B),(stack\ A\ B)}, $s_4\rangle$, which means that only the first and the last state are observed (the three intermediate states $s_1$, $s_2$ and $s_3$ are fully unknown) and that actions $a_2$ and $a_3$ are observed while $a_1$ and $a_4$ are unknown. The set of initial action models $\mathcal{M}$ only contains two of the four headers needed, but can be completed with the headers {\small\tt(putdown ?v1)} and {\small\tt(stack ?v1 ?v2)} inferred from $\tau$.

\begin{figure}[hbt!]
{\footnotesize\tt ;;;;;; Action headers in $\mathcal{M}$}
\begin{footnotesize}
\begin{verbatim}
(pickup ?v1) (unstack ?v1 ?v2)
\end{verbatim}
\end{footnotesize}
\vspace{0.2cm}
{\footnotesize\tt ;;;;;; Plan trace $\tau$}
\begin{footnotesize}
\begin{verbatim}
;;; Initial state observation
(clear B) (ontable A) (handempty) (on B A)
(not (clear A)) (not (ontable B)) (not (holding A)) (not (holding B))
(not (on A A)) (not (on A B)) (not (on B B))
\end{verbatim}
\end{footnotesize}

\begin{footnotesize}
\begin{verbatim}
;;; Action observation
(putdown B)

;;; Action observation
(stack A B)
\end{verbatim}
\end{footnotesize}

\begin{footnotesize}
	\begin{verbatim}
	;;; State observation
	(clear A) (on A B) (ontable B) 
	\end{verbatim}
\end{footnotesize}

 \caption{\small Task $\Lambda=\tup{\mathcal{M},\tau}$ associated to the observation $\tau=\langle s_0,${\small\tt (putdown\ B),(stack\ A\ B)}, $s_4\rangle$}
\label{fig:example-plans}
\end{figure}

The definition of the learning task is extensible to the more general case where the execution of several plans from the same action models are observed. In this case, $\Lambda=\tup{\mathcal{M},\mathcal{T}}$, where $\mathcal{T}=\{\tau_1,\ldots,\tau_{k}\}$ such that each $\tau\in \mathcal{T}$ is a plan trace that satisfies the previous 1--4 assumptions. In this case, the {\em learned} action models $\mathcal{M}'$ have to be compliant with the input models $\mathcal{M}$ and also with every observed plan trace $\tau\in \mathcal{T}$.


\subsection{A propositional encoding for \strips\ action models}
\label{propositional_encoding}

%We say that two \strips\ operator schemes $\xi$ and $\xi'$ are {\em comparable} if both schemas have the same parameters so they share the same space of possible \strips\ models (formally, iff $pars(\xi)=pars(\xi'$). Therefore  we can claim that blocksworld operators {\tt stack} and {\tt unstack} are {\em comparable} while  {\tt stack} and {\tt pickup} are not. We say that two \strips\ action models $\mathcal{M}$ and $\mathcal{M}'$ are {\em comparable} iff there exists a bijective function $\mathcal{M} \mapsto \mathcal{M}^*$ that maps every $\xi\in\mathcal{M}$ to a comparable action schema $\xi'\in\mathcal{M'}$ and vice versa.

In this section we formalize a propositional encoding of an \strips\ action model. This encoding is at the core of the \FAMA compilation approach for addressing the learning task defined in section \ref{learning_task}.

Let $\Omega_v=\{v_i\}_{i=1}^{\operatorname*{max}_{a\in A} ar(a)}$ be a new set of objects ($\Omega\cap\Omega_v=\emptyset$), denoted as {\em variable names}, which is bound to the maximum arity of an action in a given planning frame. For instance, in a three-block {\em blocksworld} $\Omega=\{block_1, block_2, block_3\}$ while $\Omega_v=\{v_1,v_2\}$ because the actions with the maximum arity have arity two; i.e., any instantiation of the {\small\tt stack} or the {\small\tt unstack} models.

We define $\Psi_v$ as the set of predicates $\Psi$ parameterized with the {\em variable names} of $\Omega_v$ as arguments. The set $\Psi_v$  defines the elements that can appear in the preconditions and effects of the action models. In the {\em blocksworld} domain, this set contains eleven elements, $\Psi_v$={\small\tt\{handempty, holding($v_1$), holding($v_2$), clear($v_1$), clear($v_2$), ontable($v_1$), ontable($v_2$), on($v_1,v_1$), on($v_1,v_2$), on($v_2,v_1$), on($v_2,v_2$)\}}. In more detail, for a given action model $\xi$, we define $\Psi_{\xi}\subseteq \Psi_v$ as the subset of elements of $\Psi_v$ that can appear in $\xi$. For instance, $\Psi_{\tt stack}=\Psi_v$ whereas $\Psi_{\tt pickup}$={\small\tt\{handempty, holding($v_1$),clear($v_1$),ontable($v_1$),on($v_1,v_1$)\}} excludes the elements from $\Psi_v$ that involve $v_2$ because {\small\tt pickup} actions have arity one. The size of the space of possible \strips\ models for a given $\xi$ is $2^{2|\Psi_{\xi}|}$ (recall that negative effects appear as preconditions and that they cannot be positive effects, and also that a positive effect cannot appear as a precondition). For the {\em blocksworld}, $2^{2|\Psi_{\tt stack}|}=4,194,304$ while for the {\tt pickup} operator this number is only 1024.

We are now ready to define the propositional encoding of $pre(\xi)$, $del(\xi)$ and $add(\xi)$. For every $\xi$ and $p\in \Psi_{\xi}$, we create:

\begin{itemize}
\item $pre_p(\xi)$: fluent formed by the combination of the prefixes {\small \texttt{pre}} and $name(\xi)$ plus a fluent of arity 0 that results from appending the elements of $p$ (e.g. {\small \texttt{pre\_stack\_on\_v1\_v2}}, for $name(\xi)={\small\texttt{stack}}$ and $p={\small\texttt{on($v_1,v_2$)}}$)
\item $del_p(\xi)$: fluent formed by the combination of the prefixes {\small \texttt{del}} and $name(\xi)$ plus a fluent of arity 0 that results from appending the elements of $p$ (e.g. {\small  \texttt{del\_stack\_on\_v1\_v2}})
\item $add_p(\xi)$: fluent formed by the combination of the prefixes {\small \texttt{add}} and $name(\xi)$ plus a fluent of arity 0 that results from appending the elements of $p$ (e.g. {\small \texttt{add\_stack\_on\_v1\_v2}})
\end{itemize}


%The fluents $pre_p(\xi)$, $del_p(\xi)$ and $add_p(\xi)$, for every $p\in \Psi_{\xi}$, that represent the propositional encoding for the preconditions, negative and positive effects of an action model $\xi$. If a fluent $pre_p(\xi)/del_p(\xi)/add_p(\xi)$ holds, it means that $p$ is a precondition/negative/positive effect in that action model.

For a given action model $\xi$, if a fluent $pre_p(\xi)/del_p(\xi)/add_p(\xi)$ holds in a state, it means that $p$ is a precondition/negative/positive effect of $\xi$. For instance, Figure~\ref{fig:encodedstack} shows the conjunction of fluents that represents the propositional encoding for the preconditions, negative effects and positive effects of the action model of the {\em stack} operator shown in Figure~\ref{fig:stack}.

\begin{figure}[hbt!]
\begin{footnotesize}
\begin{verbatim}
(pre_stack_holding_v1) (pre_stack_clear_v2)
(del_stack_holding_v1) (del_stack_clear_v2)
(add_stack_handempty) (add_stack_clear_v1) (add_stack_on_v1_v2)
\end{verbatim}
\end{footnotesize}
 \caption{\small Propositional encoding for the {\em stack} action model from a four-operator {\em blocksworld}.}
\label{fig:encodedstack}
\end{figure}


\subsection{Classical planning with conditional effects}
\label{conditional_effects}

Our approach to learning action models is to compile this learning task into a classical planning task with conditional effects.  Conditional effects allow us to compactly define actions whose effects depend on the current state. Supporting conditional effects is now a requirement of the IPC~\cite{vallati:IPC:AIM2015} and many classical planners cope with conditional effects without compiling them away.

An action $a\in A$ with conditional effects is defined as a set of {\em preconditions} $\pre(a)\in\mathcal{L}(F)$ and a set of {\em conditional effects} $\cond(a)$. Each conditional effect $C\rhd E\in\cond(a)$ is composed of two sets of literals $C\in\mathcal{L}(F)$, the {\em condition}, and $E\in\mathcal{L}(F)$, the {\em effect}. An action $a\in A$ is {\em applicable} in a state $s$ if and only if $\pre(a)\subseteq s$, and the {\em triggered effects} resulting from the action application are the effects whose conditions hold in $s$:
\[
triggered(s,a)=\bigcup_{C\rhd E\in\cond(a),C\subseteq s} E,
\]

The result of applying action $a$ in state $s$ is the {\em successor} state $\theta(s,a)=\{s\setminus\eff_c^-(s,a))\cup\eff_c^+(s,a)\}$ where $\eff_c^-(s,a)\subseteq triggered(s,a)$ and $\eff_c^+(s,a)\subseteq triggered(s,a)$ are, respectively, the triggered {\em negative} and {\em positive} effects.


\subsection{Compilation}
\label{compilation}

In section \ref{sec:motivation}, we exposed our reasons to solve the learning task $\Lambda=\tup{\mathcal{M},\tau}$ via compiling it into a classical planning task $P_{\Lambda}$. Our compilation scheme  builds upon the approach presented in \cite{aineto2018learning} but \FAMA comes up with a more general and flexible scheme able to capture any type of input plan trace.

The intuition behind the \FAMA compilation is that a solution plan $\pi_\Lambda$ to $P_{\Lambda}$ induces the output set of action models $\mathcal{M}'$ that solves $\Lambda$. Specifically, a solution plan $\pi_\Lambda$ serves two purposes:

\begin{enumerate}
\item {\bf To program the set of action models $\mathcal{M}'$}. $\pi_\Lambda$ comprises a plan {\em prefix} whose actions determine the predicates $p\in \Psi_{\xi}$ that belong to $pre(\xi)$, $del(\xi)$ and $add(\xi)$ for each $\xi\in\mathcal{M}$.
\item {\bf To validate the set of action models $\mathcal{M}'$}. $\pi_\Lambda$ also comprises a plan {\em postfix} whose actions target the validation of the available plan trace $\tau$ with the programmed action models $\mathcal{M}'$.
\end{enumerate}

%\subsection{Learning from observations of plan executions}
Here we formalize the compilation for learning \strips\ action models with classical planning. Given a learning task $\Lambda=\tup{\mathcal{M},\tau}$, with $\tau$ formed by an $n$-action sequence $\tup{a_1, \ldots, a_n}$ and a $m$-state trajectory $\tup{s_0, s_1, \ldots, s_m}$ (i.e., $\tau = \langle s_0, a_1, \ldots, a_n, s_m \rangle$), the compilation outputs a classical planning task $P_{\Lambda}=\tup{F_{\Lambda},A_{\Lambda},I_{\Lambda},G_{\Lambda}}$ such that:

\begin{itemize}

\item $F_{\Lambda}$ contains:
\begin{itemize}
\item The set of fluents of $s_0$; i.e., $F$.
\item The fluents $pre_p(\xi)$, $del_p(\xi)$ and $add_p(\xi)$, for every $\xi \in \mathcal{M}$ and $p\in \Psi_{\xi}$, as explained in section \ref{propositional_encoding}
\item A set of fluents $F_{\pi}=\{plan(name(a_i),\Omega^{ar(a_i)},i)\}_{\small 1\leq i\leq n}$ to represent the $i^{th}$ observable action of $\tau$. In the example of Figure~\ref{fig:example-plans}, the two observed actions {\small \texttt{(putdown B)}} and {\small \texttt{(stack  A  B)}} would be encoded as fluents  {\small \texttt{(plan-putdown B i1)}} and {\small \texttt{(plan-stack A B i2)}} to indicate that {\small \texttt{(putdown B)}} is observed in the first place and {\small \texttt{(stack  A  B)}} is the second observed action.
\item Two fluents, $at_i$ and $next_{i,i+1}$, {\small $1\leq i \leq n$}, to iterate through the $n$ observed actions of $\tau$. The former is used to ensure that actions are executed in the same order as they are observed in $\tau$. The latter is used to iterate to the next planning step when solving $P_{\Lambda}$.
\item A set of fluents $\{test_j\}_{0\leq j\leq m}$, to point at the state observation $s_j\in\tau$ where the action model is
validated. In the example of Figure~\ref{fig:example-plans} two tests are required to validate the programmed action model, one test at $s_0$ and another one at $s_4$.
\item A fluent, $mode_{prog}$, to indicate whether the action models are being programmed or validated.
\end{itemize}

\item $I_{\Lambda}$ encodes $s_0$ and the following fluents set to true: $mode_{prog}$, $test_0$, $F_{\pi}$, $at_1$ and $\{next_{i,i+1}\}$, {\small $1\leq i \leq n$}. Our compilation assumes that action models are initially programmed with no precondition, no negative effect and no positive effect.

\item $G_{\Lambda}$ includes the positive fluents $at_n$ and $test_m$. When these two goals are achieved by the solution plan $\pi_\Lambda$, we will be certain that the programmed action models are validated in all the actions and states observed in the input plan trace $\tau$.

\item $A_{\Lambda}$ comprises three kinds of actions:
\begin{enumerate}
\item Actions for {\em programming} an action model $\xi\in\mathcal{M}$. These actions will form the prefix of the solution plan $\pi_\Lambda$ and they are aimed at generating the appropriate state configuration to shape $\xi$. Among the programming actions, we find:
\begin{itemize}
\item Actions which support the addition of a {\em precondition} $p\in \Psi_{\xi}$ to the action model $\xi\in\mathcal{M}$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programPre_{p,\xi}})=&\{\neg pre_{p}(\xi), \neg del_{p}(\xi),\neg add_{p}(\xi), mode_{prog}\},\\
\cond(\mathsf{programPre_{p,\xi}})=&\{\emptyset\}\rhd\{pre_{p}(\xi)\}.
\end{align*}
\end{small}

\item Actions which support the addition of a {\em negative} or {\em positive} effect $p\in \Psi_{\xi}$ to the action model $\xi\in\mathcal{M}$.

\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{programEff_{p,\xi}})=&\{\neg del_{p}(\xi),\neg add_{p}(\xi), mode_{prog}\},\\
\cond(\mathsf{programEff_{p,\xi}})=&\{pre_{p}(\xi)\}\rhd\{del_{p}(\xi)\},\{\neg pre_{p}(\xi)\}\rhd\{add_{p}(\xi)\}.
\end{align*}
\end{small}
\end{itemize}

\item Actions for {\em applying} a programmed action model $\xi\in\mathcal{M}$ bound to objects $\omega\subseteq\Omega^{ar(\xi)}$. These actions will be part of the postfix of the solution plan $\pi_\Lambda$ and they execute $\xi$ according to the current state configuration, i.e., the values of $pre_p(\xi)$, $del_p(\xi)$ and $add_p(\xi)$. Since action headers are known, the variables $pars(\xi)$ are bound to the objects in $\omega$ that appear in the same position. Figure~\ref{fig:compilation} shows the PDDL encoding for applying a programmed action model of the {\em stack} operator from the {\em blocksworld} domain.


\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{apply_{\xi,\omega}})=&\{pre_{p}(\xi)\implies p(\omega)\}_{\forall p\in\Psi_\xi},\\
\cond(\mathsf{apply_{\xi,\omega}})=&\{del_{p}(\xi)\}\rhd\{\neg p(\omega)\}_{\forall p\in\Psi_\xi},\\
&\{add_{p}(\xi)\}\rhd\{p(\omega)\}_{\forall p\in\Psi\xi},\\
&\{mode_{prog}\}\rhd\{\neg mode_{prog}\}.
\end{align*}
\end{small}

%\hspace*{7pt}\pre(\mathsf{apply_{\xi,\omega}})=&\{pre_{f}(\xi)\implies p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))}\cup \{\neg mode_{val}\},\\
%\cond(\mathsf{apply_{\xi,\omega}})=&\{del_{f}(\xi)\}\rhd\{\neg p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
%&\{add_{f}(\xi)\}\rhd\{p(\omega)\}_{\forall p\in\Psi,f=p(pars(\xi))},\\
%&\{mode_{prog}\}\rhd\{\neg mode_{prog}\},\\
%&\{\emptyset\}\rhd\{mode_{val}\}.


\begin{figure}[hbt!]
\begin{scriptsize}
\begin{verbatim}
(:action apply_stack
  :parameters (?o1 - object ?o2 - object)
  :precondition
   (and (or (not (pre_stack_on_v1_v1)) (on ?o1 ?o1))
        (or (not (pre_stack_on_v1_v2)) (on ?o1 ?o2))
        (or (not (pre_stack_on_v2_v1)) (on ?o2 ?o1))
        (or (not (pre_stack_on_v2_v2)) (on ?o2 ?o2))
        (or (not (pre_stack_ontable_v1)) (ontable ?o1))
        (or (not (pre_stack_ontable_v2)) (ontable ?o2))
        (or (not (pre_stack_clear_v1)) (clear ?o1))
        (or (not (pre_stack_clear_v2)) (clear ?o2))
        (or (not (pre_stack_holding_v1)) (holding ?o1))
        (or (not (pre_stack_holding_v2)) (holding ?o2))
        (or (not (pre_stack_handempty)) (handempty)))
  :effect
   (and (when (del_stack_on_v1_v1) (not (on ?o1 ?o1)))
        (when (del_stack_on_v1_v2) (not (on ?o1 ?o2)))
        (when (del_stack_on_v2_v1) (not (on ?o2 ?o1)))
        (when (del_stack_on_v2_v2) (not (on ?o2 ?o2)))
        (when (del_stack_ontable_v1) (not (ontable ?o1)))
        (when (del_stack_ontable_v2) (not (ontable ?o2)))
        (when (del_stack_clear_v1) (not (clear ?o1)))
        (when (del_stack_clear_v2) (not (clear ?o2)))
        (when (del_stack_holding_v1) (not (holding ?o1)))
        (when (del_stack_holding_v2) (not (holding ?o2)))
        (when (del_stack_handempty) (not (handempty)))
        (when (add_stack_on_v1_v1) (on ?o1 ?o1))
        (when (add_stack_on_v1_v2) (on ?o1 ?o2))
        (when (add_stack_on_v2_v1) (on ?o2 ?o1))
        (when (add_stack_on_v2_v2) (on ?o2 ?o2))
        (when (add_stack_ontable_v1) (ontable ?o1))
        (when (add_stack_ontable_v2) (ontable ?o2))
        (when (add_stack_clear_v1) (clear ?o1))
        (when (add_stack_clear_v2) (clear ?o2))
        (when (add_stack_holding_v1) (holding ?o1))
        (when (add_stack_holding_v2) (holding ?o2))
        (when (add_stack_handempty) (handempty))
        (when (modeProg) (not (modeProg)))))
\end{verbatim}
\end{scriptsize}
 \caption{\small PDDL action for applying an already programmed model for $stack$ (implications are coded as disjunctions).}
\label{fig:compilation}
\end{figure}


When the input plan trace contains observed actions, the extra conditional effects

$\{at_{i},plan(name(a_i),\Omega^{ar(a_i)},i)\}\rhd\{\neg at_{i},at_{i+1}\}_{\forall i\in [1,n]}$ are included in the $\mathsf{apply_{\xi,\omega}}$ actions to ensure that actions are applied in the same order as in $\tau$.\\

\item Actions for {\em validating} the partially observed state $s_j\in\tau$, {\tt\small $1\leq j< m$}. These actions are also part of the postfix of the solution plan $\pi_\Lambda$ and they are aimed at checking that the input plan trace $\tau$ follows after the apply actions.


\begin{small}
\begin{align*}
\hspace*{7pt}\pre(\mathsf{validate_{j}})=&s_j\cup\{test_{j-1}\},\\
\cond(\mathsf{validate_{j}})=&\{\emptyset\}\rhd\{\neg test_{j-1}, test_j\}.
\end{align*}
\end{small}
\end{enumerate}
\end{itemize}


In some contexts, it is reasonable to assume that some parts of the action model are known and so there is no need to learn the entire model from scratch \cite{ZhuoNK13}. In \FAMA, when an action model $\xi$ is partially specified, the known preconditions and effects are encoded as fluents $pre_p(\xi)$, $del_p(\xi)$ and $add_p(\xi)$ set to true in the initial state $I_{\Lambda}$. In this case, the corresponding programming actions, $\mathsf{programPre_{p,\xi}}$ and $\mathsf{programEff_{p,\xi}}$, become unnecessary and are removed from $A_{\Lambda}$, thereby making the classical planning task $P_{\Lambda}$ easier to be solved.

%The compilation is flexible to the particular scenario where the number of {\em missing states} or {\em missing actions} in the input plan trace $\tau$ is bound. In both cases the output planning problem $P_{\Lambda}$ becomes simpler since the plan horizon is bound so the classical planner does not require to determine how many {\em apply} actions are necessary between any two state observations, i.e. between the application of two {\em validate} actions.

So far we have explained the compilation for learning from a single input trace. However, the compilation is extensible to the more general case $\Lambda=\tup{\mathcal{M},\mathcal{T}}$, where $\mathcal{T}=\{\tau_1,\ldots,\tau_k\}$ is a set of plan traces. Taking this into account, a small modification is required in our compilation approach. In particular, the actions in $P_{\Lambda}$ for {\em validating} the last state $s_m^t\in \tau_t$, {\tt\small $1\leq t\leq k$} of a plan trace $\tau_t$ have now to reset the current state and the current plan. These actions are now redefined as:


\begin{small}
	\begin{align*}
	\hspace*{7pt}\pre(\mathsf{validate_{j}})=&s_m^t\cup\{test_{j-1}\}\cup \{\neg mode_{prog}\},\\
	\cond(\mathsf{validate_{j}})=&\{\emptyset\}\rhd\{\neg test_{j-1},test_j\} \cup \\
	&\{\neg f\}_{\forall f\in s_m^t, f \notin s_0^{t+1}}\cup \{f\}_{\forall f\in s_0^{t+1}, f \notin s_m^t},\\
	&\{\neg f\}_{\forall f\in F_{\pi_t}}\cup \{f\}_{\forall f\in F_{\pi_{t+1}}}.\\
	\end{align*}
\end{small}

Finally, we will detail the composition of a solution plan $\pi_\Lambda$ to a planning task $P_\Lambda$ and the mechanism to extract the output set of action models $\mathcal{M}'$ from $\pi_\Lambda$. The plan of Figure~\ref{fig:plan-lplan} shows a solution to the task $P_{\Lambda}$ that encodes a learning task $\Lambda=\tup{\mathcal{M},\tau}$ for obtaining the action models of the {\em blocksworld} domain, where the models for {\tt\small pickup}, {\tt\small putdown} and {\tt\small unstack} are already specified in $\mathcal{M}$. Therefore, this plan programs and validates the action model for {\tt\small stack}, using the plan trace of Figure~\ref{fig:example-plans}. Plan steps $00-01$ program the preconditions of the {\tt\small stack} model, steps $02-06$ program the action model effects and steps $07-11$ validate the programmed model following the four-action plan shown in Figure~\ref{fig:example-plans}.

\begin{figure}[hbt!]
	{\footnotesize\tt
		{\bf 00} : (program\_pre\_stack\_holding\_v1) \\
		01 : (program\_pre\_stack\_clear\_v2)\\
		{\bf 02} : (program\_eff\_stack\_clear\_v1)\\
		03 : (program\_eff\_stack\_clear\_v2)\\
		04 : (program\_eff\_stack\_handempty)\\
		05 : (program\_eff\_stack\_holding\_v1)\\
		06 : (program\_eff\_stack\_on\_v1\_v2)\\
		{\bf 07} : (apply\_unstack blockB blockA i1 i2)\\
		08 : (apply\_putdown blockB i2 i3)\\
		09 : (apply\_pickup blockA i3 i4)\\
		10 : (apply\_stack blockA blockB i4 i5)\\
		{\bf 11} : (validate\_1)
	}
	\caption{\small Plan for programming and validating the $stack$ action model (using the plan trace $\tau$ of Figure~\ref{fig:example-plans}) as well as previously specified action models for $pickup$, $putdown$ and $unstack$.}
	\label{fig:plan-lplan}
\end{figure}

Given a solution plan $\pi_\Lambda$ that solves $P_{\Lambda}$, the set of action models $\mathcal{M}'$ that solves $\Lambda=\tup{\mathcal{M},\tau}$ are computed in linear time and space. In order to do so, $\pi_\Lambda$ is executed in the initial state $I_{\Lambda}$ and the action model $\mathcal{M}'$ will be given by the fluents $pre_f(\xi)$, $del_f(\xi)$ and $add_f(\xi)$ that are set to true in the last state reached by $\pi_\Lambda$, $s_g=\theta(I_\Lambda,\pi_\Lambda)$. For each $\xi \in \mathcal{M'}$, we build the sets of preconditions, positive effects and negative effects as follows:

%As shown in Figure \ref{fig:plan-lplan}, the learned action models $\mathcal{M'}$ can be easily inferred from the solution plan $\pi_\Lambda$, just by looking at the programming actions. However, for the shake of formalizing the extraction of the learned models, we will infer them from the final state of the solution plan $s_g=\theta(I_\Lambda,\pi_\Lambda)$ by looking at the values for the fluents $pre_p(\xi)$, $del_p(\xi)$ and $add_p(\xi)$ which encode the preconditions and effects of the validated action models. For each $\xi \in \mathcal{M'}$, we build the sets of preconditions, positive effects and negative effects as follows:

\begin{small}
	\begin{align*}
	\hspace*{7pt}pre(\xi)=& \{p ~|~ pre_p(\xi) \in s_g\}_{\forall p \in \Psi_\xi},\\
	\hspace*{7pt}add(\xi)=& \{p ~|~ add_p(\xi) \in s_g\}_{\forall p \in \Psi_\xi},\\
	\hspace*{7pt}del(\xi)=& \{p ~|~ del_p(\xi) \in s_g\}_{\forall p \in \Psi_\xi}.
	\end{align*}
\end{small}


\subsection{Properties of the compilation}

Given a plan $\pi_\Lambda$ that solves $P_\Lambda$, and the learned action models $\mathcal{M'}$ that solve $\Lambda=\tup{\mathcal{M}, \tau}$, \FAMA contributes with the following compilation properties:

\begin{mylemma}
Soundness. Any classical plan $\pi_\Lambda$ that solves $P_{\Lambda}$ induces a set of action models $\mathcal{M}'$ that solves $\Lambda=\tup{\mathcal{M},\tau}$.
\end{mylemma}

\begin{proof}[Proof sketch]
\begin{small}
  Once action models $\mathcal{M}'$ are programmed, they can only be applied and validated because of the $mode_{prog}$ fluent. In addition, $P_{\Lambda}$ is only solvable if fluents {\tt\small $at_n$} and {\tt\small $test_m$} hold at the last state reached by $\pi_\Lambda$. By the definition of the $\mathsf{apply_{\xi,\omega}}$ and the $\mathsf{validate_{j}}$ actions, these goals can only be achieved executing an applicable sequence of programmed action models that reaches every state $s_j\in\tau$, starting in the corresponding initial state and following the sequence of $n$ observed actions of $\tau$. This means that the programmed action model $\mathcal{M}'$ complies with the provided input knowledge and hence, that $\mathcal{M}'$ is a solution to $\Lambda$.
\end{small}
\end{proof}


\begin{mylemma}
Completeness. Any set of action models $\mathcal{M}'$ that solves $\Lambda=\tup{\mathcal{M},\tau}$ is computable solving the corresponding classical planning task $P_{\Lambda}$.
\end{mylemma}

\begin{proof}[Proof sketch]
\begin{small}
  By definition, $\Psi_{\xi}\subseteq \Psi_v$ fully captures the set of elements that can appear in an action model $\xi\in\mathcal{M}$. The compilation does not discard any possible set of action models $\mathcal{M}'$ definable within $\Psi_v$ that satisfies the observed state trajectory and action sequence of $\tau$. This means that for every $\mathcal{M}'$ that solves $\Lambda$, there exists a plan $\pi_\Lambda$ that can be built selecting the appropriate programming, apply and validate actions from the $P_{\Lambda}$ compilation.
  %a solution plan $\pi_\Lambda$ can be built selecting the corresponding  $\mathsf{programPre_{p,\xi}}$ and $\mathsf{programEff_{p,\xi}}$ actions according to $\mathcal{M}'$ and later, selecting the corresponding $\mathsf{apply_{\xi,\omega}}$ and $\mathsf{validate_{i}}$ actions according to $\tau$.
\end{small}
\end{proof}

The size of the planning task $P_{\Lambda}$ output by the compilation approach depends on:

\begin{itemize}
\item The arity of the actions and the fluents in $\tau$ given as input in $\Lambda$. The larger the arity, the larger the size of the $\Psi_{\xi}$ sets. This is the term that dominates the compilation size because it defines the $pre_p(\xi)/del_p(\xi)/add_p(\xi)$ fluents and the corresponding set of {\em programming} actions.
\item The length of the observed action sequence and the state trajectory of $\tau$. The larger the number of observed actions, $a_i\in\tau$ s.t. $1\leq i\leq n$, the more $\{at_i\}$ fluents. The larger the number of observed states, $s_j\in\tau$ s.t. $1\leq j\leq m$, the more $\{test_j\}$ fluents and $\{\mathsf{validate_{j}}\}$ actions in $P_{\Lambda}$.
\end{itemize}

An interesting aspect of our approach is that when a {\em fully} or {\em partially specified} \strips\ action model $\mathcal{M}$ is given in $\Lambda$, the $P_{\Lambda}$ compilation also serves to validate whether the observed $\tau$ follows the given model $\mathcal{M}$:

\begin{itemize}
	\item $\mathcal{M}$ is proved to be a {\em valid} action model for the given input data in $\tau$ iff a solution plan for $P_{\Lambda}$ can be found.
	\item $\mathcal{M}$ is proved to be a {\em invalid} action model for the given input data $\tau$ iff $P_{\Lambda}$ is unsolvable. This means that $\mathcal{M}$ cannot be compliant with the given observation of the plan execution.
\end{itemize}


The validation capacity of our compilation is beyond the functionality of VAL (the plan validation tool~\cite{howey2004val}) because our $P_{\Lambda}$ compilation is able to address {\em model validation} of a partial (or even an empty) action model with a partially observed plan trace. On the other hand, VAL requires (1) a full plan and (2), a full action model for plan validation.

%\subsection{Optimizing the compilation with background knowledge}
%A distinctive feature of Inductive Logic Programming (ILP) is that ILP can leverage {\em background knowledge} to learn logic programs from data~\cite{muggleton1994inductive}. Inspired by ILP, we show that our approach for the learning of \strips\ action models can also leverage {\em background knowledge} in this case to optimize the performance of the $P_{\Lambda}$ compilation.

%\subsubsection{Static predicates}
%A {\em static predicate} $p \in \Psi_\Lambda$ is a predicate that does not appear in the effects of any action~\cite{fox:TIM:JAIR1998}. Therefore, one can get rid of the mechanism for programming these predicates in the effects of any action schema while keeping the compilation complete. Given a static predicate $p$:
%\begin{itemize}
%\item Fluents $del_p(\xi)$ and $add_p(\xi)$, such that $p\in \Psi_v$ is the static predicate, can be discarded for every $\xi\in\mathcal{M}$.
%\item Actions $\mathsf{programEff_{p,\xi}}$ (s.t. $p\in \Psi_v$ is a parameterized static predicate) can also be discarded for every $\xi\in\mathcal(M)$.
%\end{itemize}

%Static predicates can also constrain the space of possible preconditions by looking at the given set of state observations in $\tau$. One can assume that if a precondition $p\in \Psi_v$ (s.t. $p\in \Psi_v$ is a static predicate parameterized with $\Omega_v$) is not compliant with the observations in $\tau$ then, fluents $pre_p(\xi)$ and actions $\mathsf{programPre_{p,\xi}}$ can be discarded for every $\xi\in\mathcal{M}$. For instance, in the {\em zenotravel}~\cite{long20033rd} domain $pre\_next\_board\_v1\_v1$, $pre\_next\_debark\_v1\_v1$, $pre\_next\_fly\_v1\_v1$, $pre\_next\_zoom\_v1\_v1$, $pre\_next\_refuel\_v1\_v1$ can be discarded (and their corresponding programming actions) because a precondition {\tt\small(next ?v1 ?v1 - flevel)} will never hold at any state in $\tau$.

%Furthermore looking as well at the given plan traces, fluents $pre_p(\xi)$ and actions $\mathsf{programPre_{p,\xi}}$ are also discardable for every $\xi\in\mathcal(M)$ if a precondition $p\in \Psi_v$ (s.t. $p\in \Psi_v$ is a static predicate parameterized with $\Omega_v$) is not possible according to $\tau$. Back to the {\em zenotravel} domain, if a plan trace $\tau$ contains the action {\tt\small (fly plane1 city2 city0 fl3 fl2)} and the corresponding state observations contain the static literal {\tt\small (next fl2 fl3)} but does not contain {\tt\small (next fl2 fl2)}, {\tt\small (next fl3 fl3)} or {\tt\small (next fl3 fl2)} the only possible precondition including the static predicate is $pre\_next\_fly\_v5\_v4$.

%\subsubsection{State constraints}
%The notion of {\em state-constraint} is very general and has been used in different areas of AI and for different purposes.  If we restrict ourselves to planning, {\em state-constraints} are abstractions for compactly specifying sets of states. For instance, {\em state-constraints} in planning allow to specify the set of states where a given action is applicable, the set of states where a given {\em derived predicate} holds or the set of states that are considered goal states.

%{\em State invariants} is a kind of state-constraints useful for computing more compact state representations~\cite{helmert2009concise} or making {\em satisfiability planning} and {\em backward search} more efficient~\cite{rintanen2014madagascar,alcazar2015reminder}. Given a classical planning problem $P=\tup{F,A,I,G}$, a {\em state invariant} is a formula $\phi$ that holds at the initial state of a given classical planning problem, $I\models \phi$, and at every state $s$, built from $F$, that is reachable from $I$ by applying actions in $A$.

%The formula $\phi_{I,A}^*$ represents the {\em strongest invariant} and exactly characterizes the set of all states reachable from $I$ with the actions in $A$. For instance Figure~\ref{fig:strongest-invariant} shows five clauses that define the {\em strongest invariant} for {\em blocksworld}. There are infinitely many strongest invariants, but they are all logically equivalent, and computing the strongest invariant is PSPACE-hard as hard as testing plan existence.

%\begin{figure}[hbt!]
%  \begin{footnotesize}
%    \begin{center}
%$\forall x_1,x_2\ ontable(x_1)\leftrightarrow\neg on(x_1,x_2)$.\\
%$\forall x_1,x_2\ clear(x_1)\leftrightarrow\neg on(x_2,x_1)$.\\
%$\forall x_1,x_2,x_3\ \neg on(x_1,x_2)\vee\neg on(x_1,x_3)\ such\ that\ x_2\neq x_3$.\\
%$\forall x_1,x_2,x_3\ \neg on(x_2,x_1)\vee\neg on(x_3,x_1)\ such\ that\ x_2\neq x_3$.\\
%$\forall x_1,\ldots,x_n\ \neg(on(x_1,x_2)\wedge on(x_2,x_3)\wedge\ldots\wedge on(x_{n-1},x_n)\wedge on(x_n,x_1)).$
%    \end{center}
%\end{footnotesize}
% \caption{\small An example of the strongest invariant for the {\em blocksworld} domain.}
%\label{fig:strongest-invariant}
%\end{figure}


%A {\em mutex} (mutually exclusive) is a state invariant that takes the form of a binary clause and indicates a pair of different properties that cannot be simultaneously true~\cite{kautz:mutex:IJCAI1999}. For instance in a three-block {\em blocksworld}, $\phi_1=\neg on(block_A,block_B)\vee \neg on(block_A,block_C)$ is a mutex because $block_A$ can only be on top of a single block.

%A {\em domain invariant} is an instance-independent invariant, i.e. holds for any possible initial state and set of objects. Therefore, if a given state $s$ holds $s\nvDash \phi$ such that $\phi$ is a {\em domain invariant}, it means that $s$ is not a valid state. Domain invariants are often compactly defined as {\em lifted invariants} (also called schematic invariants)~\cite{rintanen:schematicInvariants:AAAI2017}. For instance, $\phi_2=\forall x:\ (\neg handempty\vee \neg holding(x))$, is a {\em domain mutex} for the {\em blocksworld} because the robot hand is never empty and holding a block at the same time.

%An interesting contribution of our compilation is that the validation of an action model can also be done with {\em state constraints}. Given $\Phi$, a set of either state or trajectory constraints, our validate actions can be adapted to check also that the learned model satisfy a constraint $\phi\in\Phi$:
%\begin{small}
%\begin{align*}
%\hspace*{7pt}\pre(\mathsf{validate_{i}})=&\phi\cup\{test_j\}_{j\in 1\leq j<i}\cup\{\neg test_j\}_{j\in i\leq j\leq |\Phi|}\cup \{mode_{val}\},\\
%\cond(\mathsf{validate_{i}})=&\{\emptyset\}\rhd\{test_i,\neg mode_{val}\}.
%\end{align*}
%\end{small}

%This redefinition of validate actions applies also to {\em trajectory constraints} because LTL formulae can be represented using classical action preconditions and goals by encoding the Non-deterministic B\"{u}chi Automaton (NBA), that is equivalent to the corresponding LTL formula, as part of the classical planning tasks~\cite{baier2006planning}.

%Because of the combinatorial nature of the search for a solution plan, the sooner unpromising nodes are pruned from the search the more efficient the computation of a solution plan. Constraints can be used to confine earlier the set of possible \strips\ action models and reduce then the learning hypothesis space. With regard to our compilation, {\em domain mutex} are useful to reduce the amount of applicable actions for programming a precondition or an effect for a given action schema. For example given the {\em domain mutex} $\phi=(\neg f_1\vee \neg f_2)$ such that $f_1\in F_v(\xi)$ and $f_2\in F_v(\xi)$, we can redefine the corresponding programming actions for {\bf removing} the {\em precondition} $f_1\in F_v(\xi)$ from the action schema $\xi\in\mathcal{M}$ as:

%\begin{small}
%\begin{align*}
%\hspace*{7pt}\pre(\mathsf{programPre_{f_1,\xi}})=&\{\neg del_{f_1}(\xi),\neg add_{f_1}(\xi), mode_{prog}, pre_{f_1}(\xi), pre_{f_2}(\xi)\},\\
%\cond(\mathsf{programPre_{f_1,\xi}})=&\{\emptyset\}\rhd\{\neg pre_{f_1}(\xi)\}.
%\end{align*}
%\end{small}








