
\section{Evaluation of action models}
\label{sec:evaluation}

In this section we introduce the metrics used by \FAMA to evaluate the action models that result from solving a learning task $\Lambda$. First, we will describe two standard syntactic metrics and then a semantic evaluation measure that leverages these two syntactic metrics is defined in section \ref{semantic_precision_recall}. Finally, section \ref{edit_distance} explains how \FAMA computes the semantic-based metric.

\vspace{0.1cm}

When the planning reference model of the input observations (i.e., the GTM) is available, the quality of the learned action models is measurable using two well-studied syntax-based metrics, {\em precision} and {\em recall}, commonly used in tasks such as information retrieval and recommender systems~\cite{davis2006relationship}. These two syntactic metrics are generally more informative than counting the number of errors between the learned action models and the GTM. Intuitively, precision gives a notion of {\em soundness} while recall gives a notion of the {\em completeness} of the learned models:

\begin{itemize}
\item $Precision=\frac{tp}{tp+fp}$, where $tp$ is the number of {\em true positives} (in our case, predicates that correctly appear in the action model) and $fp$ is the number of {\em false positives} (predicates of the learned model that should not appear).
\item $Recall=\frac{tp}{tp+fn}$, where $fn$ is the number of {\em false negatives} (predicates that should appear in the learned model but are missing).
\end{itemize}


Introducing semantic-based evaluation metrics can be justified on several grounds:

\begin{itemize}
\item The GTM is unknown
\item Though a planning reference model exists, a test-based model evaluation on a dataset is preferable or needed as complementary to a syntactic evaluation
\item The semantics of an action model learned from observations of plan executions can be easily altered. That is, it is very likely to learn a semantically correct but syntactically incorrect model. We will refer to this problem as \emph{reformulation}.
\end{itemize}

\emph{Reformulation} has a large impact in the action-model learning task. For instance, the roles of two {\em comparable} action models can be swapped. Two action models $\xi$ and $\xi'$ are comparable if both have the same parameters (iff $pars(\xi)=pars(\xi'$)) and so they share the same space of possible models. Hence, the {\em blocksworld} operator {\small\tt stack} could be {\em learned} with the preconditions and effects of the {\small\tt unstack} operator, and viceversa, because they are comparable. On the contrary, this reformulation will not happen between the {\tt stack} and {\tt pickup} because they are not comparable. In the same way, the roles of two action parameters that share the same type can also be swapped (e.g., interchanging the role of the two parameters of the operator {\small\tt stack} or the opreator {\small\tt unstack}) and yet the learned models would be semantically correct with respect to the given input observations. A more complex kind of reformulation occurs when two or more action models are learned in a single \emph{macro-action}.


Semantic alterations typically happen when the observed input data given in $\tau$ is scarce. Defining a proper semantic evaluation metric is key because the application of syntax-based metrics may report low scores for learned models that are actually {\em sound} and {\em complete} but correspond to {\em reformulations} of the GTM. In the following sections, we introduce a novel evaluation metric that is robust to different types of reformulation.




\subsection{Semantic-based precision and recall}
\label{semantic_precision_recall}

We define semantic-based metrics which are conceptually grounded on the precision and recall metrics introduced above. The rationale behind these novel metrics lies in counting the number of \emph{editing operations} that are necessary to match two sets of action models. Given $\mathcal{M}$, the two allowed editing operations are:

\begin{itemize}
\item {\em Deletion}. A fluent $pre_p(\xi)/del_p(\xi)/add_p(\xi)$ is removable from $\xi\in\mathcal{M}$.
\item {\em Insertion}. A fluent $pre_p(\xi)/del_p(\xi)/add_p(\xi)$ can be added to $\xi\in\mathcal{M}$.
\end{itemize}

%The \ARMS system showed that the error of the learned action models can also be estimated executing a set of plan traces with the learned model ~\cite{yang2007learning}. ARMS tackles the semantic evaluation by counting the number of preconditions not met during the execution of the plan trace with the learned models.

The \ARMS system showed that a semantic evaluation can be done via validation of a set of plan traces with the learned model~\cite{yang2007learning}. The underlying idea is that an error indication of the learned action models is obtained by counting the number of preconditions that are not satisfied during the execution of the plan trace with the learned models. This approach can be understood as modifying the plan trace (by adding the necessary preconditions to the intermediate states) so as to allow the execution of the observed actions using the learned models. In other words, modifying the plan trace to fit the model. Inspired by this approach, we present an alternative method that, instead, modifies the learned models so that they can explain the given plan traces.

%Inspired by this approach, we present an alternative method, in which instead of modifying the trace, we modify the learned models so they can explain the given plan traces.

\vspace{0.1cm}

We interpret the semantic evaluation of action models as a learning task $\Lambda = \tup{\mathcal{M}, \mathcal{T}}$, where:

\begin{itemize}
	\item $\mathcal{M}$ is a previously \textbf{learned set of action models} obtained using any learning approach such as \FAMA.
	
	\item$\mathcal{T}$ is a set of plan traces  used for \textbf{testing}.
\end{itemize}

A solution to this task is an \textbf{edited set of action models} $\mathcal{M'}$ able to explain $\mathcal{T}$, and obtained by applying \emph{deletion} and \emph{insertion} operations to $\mathcal{M}$. While it is always recommended for the test set to be different from the one used during learning, this is specially important for satisfying approaches such as \FAMA; otherwise $\mathcal{M'}$ = $\mathcal{M}$, since $\mathcal{M}$ can already explain $\mathcal{T}$ without any modification.

We now provide formal definitions of $INS(\mathcal{M},\mathcal{M'})$ and $DEL(\mathcal{M},\mathcal{M'})$, the insertions and deletions needed to transform the set of action models $\mathcal{M}$ into $\mathcal{M'}$.

%We say that two action models $\mathcal{M}$ and $\mathcal{M}'$ are {\em comparable} iff there exists a bijective function $\mathcal{M} \mapsto \mathcal{M'}$ that maps every $\xi\in\mathcal{M}$ to a comparable action model $\xi'\in\mathcal{M'}$ and vice versa.


%\begin{mydefinition}
%  Let $\mathcal{M}$ and $\mathcal{M}'$ be two {\em comparable} sets of action models. We compute $ins(\mathcal{M}, \mathcal{M'})$ and $del(\mathcal{M}, \mathcal{M'})$ as:
%  \begin{small}
%  	\begin{align*}
%  	ins(\mathcal{M}, \mathcal{M'})=&\{pre_p(\xi) ~|~ p \notin pre(\xi), p \in pre(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')} \cup\\
%  	&\{add_p(\xi) ~|~ p \notin add(\xi), p \in add(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')} \cup\\
%  	&\{del_p(\xi) ~|~ p \notin del(\xi), p \in del(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')}\\
%  	\\
%  	del(\mathcal{M}, \mathcal{M'})=&\{pre_p(\xi) ~|~ p \in pre(\xi), p \notin pre(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')} \cup\\
%  	&\{add_p(\xi) ~|~ p \in add(\xi), p \notin add(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')} \cup\\
%  	&\{del_p(\xi) ~|~ p \in del(\xi), p \notin del(\xi')\}_{\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t. name(\xi) = name(\xi')}
%  	\end{align*}
%  \end{small}
%\end{mydefinition}

\begin{mydefinition}
	Let $PRE(\xi) = \underset{\forall p \in pre(\xi)}{\bigcup} pre_p(\xi)$, $ADD(\xi) = \underset{\forall p \in add(\xi)}{\bigcup} add_p(\xi)$, and $DEL(\xi) = \underset{\forall p \in del(\xi)}{\bigcup} del_p(\xi)$ be the set of propositional fluents that represent preconditions, positive and negative effects of a given action model $\xi$. We define:
	\begin{small}
		\begin{align*}
		INS(\mathcal{M}, \mathcal{M'})=&PRE(\xi') \backslash PRE(\xi) \cup\\
		&ADD(\xi') \backslash ADD(\xi) \cup \\
		&DEL(\xi') \backslash DEL(\xi),\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t.\ name(\xi) = name(\xi')\\
		\\
		DEL(\mathcal{M}, \mathcal{M'})=&PRE(\xi) \backslash PRE(\xi') \cup\\
		&ADD(\xi) \backslash ADD(\xi') \cup \\
		&DEL(\xi) \backslash DEL(\xi'),\forall \xi\in\mathcal{M}, \xi'\in\mathcal{M'} s.t.\ name(\xi) = name(\xi')\\
		\end{align*}
	\end{small}
\end{mydefinition}

Note that the number of {\em deletions} ($\left|DEL(\mathcal{M},\mathcal{M'})\right|$) that is required to transform $\mathcal{M}$ into $\mathcal{M}'$ matches our previous definition for the number of {\em false positives} when (1) $\mathcal{M}$ is the set of learned action models and (2) $\mathcal{M}'$ is the given GTM that serves as reference. Likewise the number of insertions ($\left|INS(\mathcal{M},\mathcal{M'})\right|$) required to transform $\mathcal{M}$ into $\mathcal{M}'$ corresponds to the number of {\em false negatives} in the learned models $\mathcal{M}$ with respect to the GTM $\mathcal{M}'$.
%As explained, these numbers can be normalized with the amount of {\em true positives} to compute the {\em precision} and {\em recall} of a given learned model with respect to a reference model. Further, we can claim that all these numbers are bounded. Since $F_v$ is a bound set, the maximum number of edits that can be introduced to a given action model defined within $F_v$ is bound as well. In more detail, for an operator schema $\xi\in\mathcal{M}$ the maximum number of edits that can be introduced to their precondition set is $|F_{\xi}|$. This is the same max number of edits that can be introduced to the positive and to the negative effects of $\xi\in\mathcal{M}$.

%Inspired by this approach for the semantic evaluation of learned action models, we define now the semantic version of precision and recall to asses the quality of a learned action model with respect to a partially observed plan trace.

Given that the evaluation task is defined in terms of a learning task $\Lambda$, there might exist potentially many edited models $\mathcal{M'}$ which are solution to this task. Although the actual GTM is included among the solution set, it is impossible to identify it, so we define the best solution based on its proximity to the input model.

\begin{mydefinition}
  Given a set of action models $\mathcal{M}$, and all the sets of action models $\mathcal{M'}$ able to explain the plan traces $\mathcal{T}$. The {\bf closest compliant set of action models}, $\mathcal{M^*}$, is the comparable set of action models closest to $\mathcal{M}$ (in terms of editions) and able to explain $\mathcal{T}$;
  \[\mathcal{M^*}=\underset{\forall \mathcal{M}' \rightarrow \mathcal{T}}{\arg\min} \ \left| INS(\mathcal{M},\mathcal{M'}) \cup DEL(\mathcal{M},\mathcal{M'}) \right|\]
\end{mydefinition}

The {\bf closest compliant set of action models} allows us to define a semantic version of {\em precision} and {\em recall}, following the previous correspondences ({\em false positives} $\equiv$ {\em deletions} and {\em false negatives} $\equiv$ {\em insertions}). Also, by definition, the number of preconditions and effects of the learned action models is equal to the sum of {\em true positives} and {\em false positives}; that is, given $size(\mathcal{M}) = \left|pre(\xi)\right| + \left|add(\xi)\right| + \left|del(\xi)\right| \forall \xi \in \mathcal{M}$, we know that $size(\mathcal{M}) = tp + fp$.

The semantic-based definition of precision and recall are given by:

\begin{small}
	\begin{align*}
	sem\text{-}Precision=&\frac{tp}{tp + fp}=\frac{size(\mathcal{M})- \left|del(\mathcal{M},\mathcal{M^*})\right|}{size(\mathcal{M})}\\
	sem\text{-}Recall=&\frac{tp}{tp + fn}=\frac{size(\mathcal{M})- \left|del(\mathcal{M},\mathcal{M^*})\right|}{size(\mathcal{M}) - \left|del(\mathcal{M},\mathcal{M^*})\right| + \left|ins(\mathcal{M},\mathcal{M^*})\right|}
	\end{align*}
\end{small}


As we can observe in the two above formulas, precision and recall are not computed with respect to a GTM but with respect to the closest compliant set of action models that explain the test set of plan traces $\mathcal{T}$. Thereby, in the case that the {\bf closest compliant set of action models} $\mathcal{M^*}$ is the GTM, we conclude that the values of the syntactic measures {\em Precision} and {\em Recall} are exactly the same as {\em sem\text{-}Precision} and {\em sem\text{-}Recall}.

The intuition behind this evaluation is to {\em semantically} assess how well the learned action models $\mathcal{M}$ explain a set of given observations of plan executions according to the amount of {\em edition} required by $\mathcal{M}$ to induce the observations. Unlike the semantic metric defined by ARMS, our novel semantic definitions of precision and recall are not sensitive to flaws that appear more than once in the plan traces since the flaws are corrected only once in the learned models instead of at every intermediate state of the plan traces.

%This semantic evaluation approach is again flexible to various amount and kind of available input knowledge.


\subsection{Semantic evaluation with classical planning}
\label{edit_distance}

%Our compilation is extensible to compute the {\em closest compliant set of action models} and hence, our semantic versions of the {\em precision} and {\em recall} metrics. This extension considers that the input models $\mathcal{M}$, is {\em non-empty} so instead of learning an action model from scratch we simply edit $\mathcal{M}$ until it satisfies the given input observations. In other words, now $\mathcal{M}$ is a set of given operator schemas, wherein each $\xi\in\mathcal{M}$ initially contains the $pre(\xi)$, $del(\xi)$ and $add(\xi)$ sets. A solution to the classical planning task resulting from the extended compilation is a sequence of actions that:

The compilation scheme presented in section \ref{sec:compilation} is extensible to address the evaluation task $\Lambda=\tup{\mathcal{M}, \mathcal{T}}$ defined in section \ref{semantic_precision_recall}. In this extended task, $\mathcal{M}$ represents a set of previously learned action models; therefore, rather than learning the action models from scratch, we simply edit $\mathcal{M}$ until it satisfies the given test set of plan traces $\mathcal{T}$. A solution to the classical planning task resulting from the extended compilation is a plan that:

\begin{enumerate}
\item {\bf Edits the action models $\mathcal{M}$ to build $\mathcal{M}'$}. A solution plan starts with a prefix that modifies the preconditions and effects of the action schemes in $\mathcal{M}$ using the two {\em editing operations} defined above, {\em deletion} and {\em insertion}.
\item {\bf Validates the edited model $\mathcal{M}'$ in the observed plan traces}. The solution plan continues with a postfix that validates the edited model $\mathcal{M}'$ on the given observations $\mathcal{T}$, as explained in Section~\ref{sec:compilation} for the models that are programmed from scratch.
\end{enumerate}

Given $\Lambda=\tup{\mathcal{M},\mathcal{T}}$, the output of the extended compilation is a planning task $P_{\Lambda}'=\tup{F_{\Lambda},A_{\Lambda}',I_{\Lambda},G_{\Lambda}}$ such that:

\begin{itemize}
\item $F_{\Lambda}$, $I_{\Lambda}$ and $G_{\Lambda}$ are defined as in the previous compilation. Note that, the input action model $\mathcal{M}$ is encoded in the initial state. This means that the fluents $pre_p(\xi)/del_p(\xi)/add_p(\xi)$, $p\in \Psi_\xi$, hold in $I_{\Lambda}$ iff they appear in $\mathcal{M}$.
\item $A_{\Lambda}'$, comprises the same three kinds of actions of $A_{\Lambda}$. The actions for {\em applying} an already programmed action model and the actions for {\em validating} an observation are defined exactly as in the previous compilation. The only difference here is that the {\em programming actions} now implement the two editing operations (i.e., they also include the actions for {\em deleting} a precondition or negative/positive effect from an action model).
\end{itemize}

Figure~\ref{fig:plan-pdistance} shows the plan for editing the action model of the operator {\tt\small stack} of the {\em blocksworld} domain where only the two positive effects {\tt\small (handempty)} and {\tt\small (clear ?v1)} are missing. In this case the edited action model is again validated in the plan trace shown in Figure~\ref{fig:example-plans}.

\begin{figure}[hbt!]
{\footnotesize\tt
  {\bf 00} : (insert\_add\_stack\_handempty)\\
  01 : (insert\_add\_stack\_clear\_var1)\\
  {\bf 02} : (apply\_unstack blockB blockA i1 i2)\\
  03 : (apply\_putdown blockB i2 i3)\\
  04 : (apply\_pickup blockA i3 i4)\\
  05 : (apply\_stack blockA blockB i4 i5)\\
  {\bf 06} : (validate\_1)
}
\caption{\small Plan for editing and validating the action model {\tt\small{stack}} in which the positive effects {\tt\small{(handempty)}} and {\tt\small{(clear ?v1)}} are missing.}
\label{fig:plan-pdistance}
\end{figure}

Assuming we are using an optimal planner to solve $P_{\Lambda}'$, the solution plan of this problem will induce the \emph{closest compliant set of action models} $\mathcal{M^*}$. Therefore, our compilation enables the computation of the semantic versions of \emph{precision} and \emph{recall}. An argument can be made, however, that solving optimally $P_{\Lambda}'$ may turn the evaluation process very time consuming. Considering this, $sem\text{-}Precision$ and $sem\text{-}Recall$ can be approximated if $P_{\Lambda}'$ is solved with a satisfying planner. In this case, no guarantees can be made that the edited models will be the closest compliant ones, but a planner will always try to minimize the length of the plan and hence the number of editing operations applied to the input models.


%Our interest when solving the classical planning task that is output by our extended compilation is not in the resulting action model $\mathcal{M}'$ but in the number of {\em edit operations} (insertions and deletions) that is required to compute $\mathcal{M}'$. For instance, in the example of Figure~\ref{fig:plan-pdistance}, $\mathcal{M}'$ is validated in the given observations after inserting two new positive effects to the initial action model, e.g. $\delta(\mathcal{M},\mathcal{T})=2$ . In this case $\delta(\mathcal{M},*)=3\times 2\times (11+5)$ since there are 4 action schemes ({\small\tt pickup}, {\small\tt putdown}, {\small\tt stack} and {\small\tt unstack}) and $|F_v|=|F_{\tt stack}|=|F_{\tt unstack}|=11$ while $|F_{\tt pickup}|=|F_{\tt putdown}|=5$  (as shown in Section~\ref{sec:learning}). The {\em observation edit distance} is exactly computed if the classical planning task resulting from our compilation is optimally solved (according to the number of edit actions); is approximated if it is solved with a satisfying planner; and is a less accurate estimate (but faster to be computed) if the solved task is a relaxation of the classical planning task that results from our compilation~\cite{bonet2001planning}.

%Last but not least, the compilation approach is also flexible to compute the {\em edit distance} between two {\em comparable} \strips\ action models, $\mathcal{M}$ and $\mathcal{M}'$. A solution to the planning task resulting from this compilation is a sequence of actions that edits the action model $\mathcal{M}$ to produce $\mathcal{M}'$ using to the two {\em edit operations}, deletion and insertion. In this case the edited model is not validated on a sequence of observations or plans but on the given action model $\mathcal{M}'$ that acts as a reference. The sets of fluents $F_{\Lambda}$ and $I_{\Lambda}$ are defined like in the previous compilation. With respect to the actions, $A_{\Lambda}'$ again implement the two {\em edit operations} ({\em insertion} and {\em deletion} for both preconditions and effects) but does not contain $\mathsf{apply_{\xi,\omega}}$ or $\mathsf{validate_{i}}$ actions because the edited \strips\ action model are not validated in any observation of plan executions. Finally, the goals are also different and are now defined by the set of fluents, $pre_f(\xi)/del_f(\xi)/add_f(\xi)$ that represent all the operator schema $\xi\in\mathcal{M'}$, such that $f\in F_{\xi}$. To illustrate this, the plan of Figure~\ref{fig:plan-mdistance} solves the classical planning task that corresponds to computing the distance between a \emph{blocksworld} action model, where the positive effects {\tt\small (handempty)} and {\tt\small (clear ?v1)} of the {\tt\small stack} schema are missing, and the actual four-operator {\em blocksworld} model. The plan edits first the {\tt\small stack} schema, {\em inserting} these two positive effects. Again our interest is in the number of required {\em edit operations}, e.g. $\delta(\mathcal{M},\mathcal{M'})=2$.

%\begin{figure}[hbt!]
%{\tt\small
%00 : (insert\_add\_handempty\_stack)\\
%01 : (insert\_add\_clear\_stack\_var1)
%}
% \caption{\small Plan for computing the distance between a \emph{blocksworld} action model, where the positive effects {\tt\small (handempty)} and {\tt\small (clear ?v1)} of the {\tt\small stack} schema are missing, and the actual four-operator {\em blocksworld} model.}
%\label{fig:plan-mdistance}
%\end{figure}









