
\section{Introduction}
\label{sec:introduction}

There is common agreement in the planning community that the unavailability of a complete domain model is a bottleneck in the applicability of planning technology to many real-world domains~\cite{kambhampati:modellite:AAAI2007}. Motivated by the difficulty and cost of crafting action models, research in action-model learning has seen huge advances. Since the emergence of pioneer learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or noisy states~\cite{zhuo2013action,MouraoZPS12}, from null state information~\cite{cresswell2013acquiring}, from incomplete domain models~\cite{ZhuoNK13,ZhuoK17} and many more.

A system for learning planning action models, receives as an input observations of the agent's plan execution and outputs an abstract version of the capability model that reflects the physics of the real-world domain. The primary underlying motivation for acquiring planning action models is to solve model-based planning tasks afterwards. Recently, model-based representations are also becoming popular in explainable AI planning as they form a common basis for communicating with users and facilitate the generation of transparent and explainable decisions~\cite{FoxLM17} as well as explanations in terms of the differences with a human mental model~\cite{ChakrabortiSK18}.

Two different types of data sources are generally identified in the generation of explanations~\cite{Sheh17,Roberts18}: (1) data that embody enough information to fully trace the steps of the decision-making process of the agent and so accurate and detailed explanations can be generated; (2) data that do not capture the decision-making process because the observed external behaviour of the agent responds to a black-box. A similar distinction can be established with the type of observations that algorithms use to learn planning action models. Up to date, all existing learning approaches assume that a given input plan trace contains a fully observed sequence of the executed actions. This seriously restricts the applicability of the learning approach to contexts in which the behaviour of the agent is fully observable and a human annotator correctly labels the executed actions.

Motivated by recent advances on the synthesis of different kinds of generative models with classical planning~\cite{bonet2009automatic,segovia2016generalized,segovia2016hierarchical,segovia2017generating}, in this paper we claim that a planning model is learnable even though an accurate representation of the agent's behaviour is not available. Particularly, we present a novel learning algorithm, called \FAMA, capable of inferring the preconditions and effects of \strips\ action models, the vanilla action model for automated planning~\cite{fikes1971strips}, under minimum input knowledge. 

While current learning systems accept varying degrees of observability in the states traversed by the input observed actions, none of them allow partial observability in the sequence of executed actions. \FAMA, on the other hand, allows incomplete or empty action sequences. This way, the minimal observability case acceptable by \FAMA is when the algorithm is only fed with the input and output of a plan trace, namely the initial and final state of a planning task. Like many Machine Learning (ML) techniques, \FAMA is able to operate with only input/output pairs and an unknown or a partially known model of the agent. Unlike ML algorithms, \FAMA requires a symbolic structured representation of the input knowledge. In this sense, recent investigations tackle the problem of learning symbolic representations from low-level sensing information and unstructured data~\cite{KonidarisKL18,AsaiF18}.

\FAMA is a solving schema, based on AI planning technology, that automatically compiles the task of learning \strips\ actions into a planning task which is then solved with a planner~\cite{aineto2018learning}. \FAMA is thereby a model-based approach that automatically builds its own planning model by logical inference from the input observations. A solution to this planning task is a sequence of actions that determines the preconditions and effects of the \strips\ action models. The construct of the planning model starts out with the input observations of the agent's behaviour that, as mentioned above, may comprise none of the actions executed by the agent. \FAMA can thus be regarded as a solving process that generates an instance of a planning model, and whose solution must be \emph{compliant} with the input plan traces. This behaviour largely differs from ML techniques, which aim to minimize an error function on the training data. Moreover, \FAMA requires far less sample data (input observations) than typical ML algorithms, thus alleviating the dependency on the assumption that there are enough data for learning the action models~\cite{Zhuo15}.

A key aspect in action-model learning is the evaluation method to assess the quality and performance of the learning approach. The most common method is to use a syntax-based evaluation that compares the learned model with a reference model. \FAMA instead proposes two novel semantic evaluation metrics that build upon two well-known ML metrics, {\em precision} and {\em recall}~\cite{davis2006relationship}, to evaluate action models with respect to observations of plan executions. Our semantic evaluation is generally more informative that counting the number of error between two models and alleviates the limitations of a purely syntax-based assessment: (a) that the learned model is syntactically different from the reference model but semantically correct and (b) that the learned model comprises correct though unnecessary preconditions in regards to the reference model. This latter issue is concerned with the qualification problem, the actual impossibility of listing all the preconditions required for a real world action to have its intended effects~\cite{GinsbergS88}.

Our semantic evaluation method is built on the same compilation scheme for solving a learning task. In particular, \FAMA also accepts as an input an initial action model of the agent's behaviour, either complete or partially specified~\cite{ZhuoNK13,ZhuoK17}, alongside the observations. In this case, \FAMA returns a model that follows the input model and is compliant with the observations. We designed an {\em edition} mechanism that serves to correct the input model to the output model, which in turn defines an assessment of the accuracy with which the input model explains the observations. Interpreting {\em edition} as a distance-based concept between two models could be exploited in model reconciliation~\cite{KulkarniCZVZK16}.


%Like other learning approaches, \FAMA also accepts as an input an initial action model of the agent's behaviour, either complete or partially specified~\cite{ZhuoNK13,ZhuoK17}. In this case, the output model of \FAMA is compliant with both the input model and with the observations. Following this behaviour, we can define a distance measure between the two models which yields an assessment of the accuracy with which the input model explains the observations. In other words, the distance measure provides a mechanism to explicitly correct or adjust a model to the observations. By using this distance-based concept, \FAMA contributes with two novel semantic evaluation metrics that are generally more informative than counting the number of errors between two models. Moreover, this new evaluation method opens up a way towards model reconciliation~\cite{KulkarniCZVZK16}.

All in all, \FAMA is a planning-based solving scheme that outputs a \strips\ action model using an automatically built planning model from minimal input knowledge. Unlike extensive data-ML approaches, \FAMA only requires a small amount of input plan traces. Unlike most relevant action-model learning algorithms, \FAMA does not require the traces to contain any observed action executed by the agent. Ultimately, \FAMA adopts certain ML-like characteristics, like the ability to learn from data that do not explicitly exhibit the agent¡s behaviour or the capacity of correcting an initially given action model. A first description of the \FAMA compilation scheme previously appeared in our previous conference paper~\cite{aineto2018learning}. This paper brings the following contributions over the first version of the compilation:
\begin{itemize}
\item A unified formulation for learning and evaluating action models from observations of plan executions. In the case of minimal observability, these executions only comprise the initial and final state of the plan traces.
\item A thorough elaboration of two semantic evaluation metrics that build upon the notions of {\em precision} and {\em recall} to evaluate the output action models with respect to observations of plan executions.
\item An exhaustive empirical evaluation over 14 domains from the International Planning Competitions (IPCs). We include an analysis of the impact that the size of the input knowledge has in the performance of \FAMA, a comparison with \ARMS and a detailed experimentation when \FAMA is executed with minimal input knowledge.
\end{itemize}






The paper is organized as follows. Section~\ref{sec:background} introduces classical planning concepts and reviews related work on learning planning action models. Section~\ref{sec:motivation} motivates our compilation-to-planning approach for learning action models. Section~\ref{sec:learning} formalizes the learning task and presents the compilation scheme, the core of \FAMA. Sections~\ref{sec:evaluation} explains the evaluation of a learned model with respect to a reference model (syntactic evaluation) and with respect to a set of plan traces (semantic evaluation). Section~\ref{sec:experiments} reports the results of the experimental evaluation and, finally, Section~\ref{sec:conclusions} discusses the strengths and weaknesses of the compilation approach and proposes several opportunities for future research.








