
\section{Introduction}
\label{sec:introduction}

There is common agreement in the planning community that the unavailability of a complete domain model is a bottleneck in the applicability of planning technology to many real-world domains~\cite{kambhampati:modellite:AAAI2007}. Motivated by the difficulty and cost of crafting action models, research in action-model learning has seen huge advances since the emergence of the pioneer learning system ARMS~\cite{yang2007learning}. Ever since we have seen models able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or noisy states~\cite{zhuo2013action,MouraoZPS12}, from null state information~\cite{cresswell2013acquiring}, from incomplete domain models~\cite{ZhuoNK13,ZhuoK17} and many more.

The primary underlying motivation for acquiring planning action models is to be able to solve model-based planning tasks afterwards. A learning system receives as an input observations of the agent's plan execution and outputs an abstract version of the capability model that reflects the physics of the real-world domain. Recently, model-based representations are also becoming popular in explainable AI planning as they form a common basis for communicating with users and facilitate the generation of transparent and explainable decisions~\cite{FoxLM17} as well as explanations in terms of the differences with a human mental model~\cite{ChakrabortiSK18}.

Two different types of data sources are generally identified in the generation of explanations~\cite{Sheh17,Roberts18}: (1) data that embody enough information to fully trace the steps of the decision-making process of the agent and so accurate and detailed explanations can be generated; (2) data that do not accurately represent the decision-making process because the observed external behaviour of the agent responds to a black-box model. A similar distinction can be established with the type of observations that algorithms use to learn planning action models. Up to date, all existing learning approaches assume that a given input plan trace contains a fully observed sequence of the executed actions. This seriously restricts the applicability of the learning approach to contexts in which the behaviour of the agent is fully observable and a human annotator correctly labels the observed actions.

In this paper we claim that a planning model is learnable even though an accurate representation of the agent's behaviour is not available. Particularly, we present a novel learning algorithm, called \FAMA, capable of inferring the preconditions and effects of \strips\ action models, the vanilla action model for automated planning~\cite{fikes1971strips}, under a minimum input knowledge. While current learning systems accept input plan traces to a greater or lesser degree of observability in the states traversed by the sequence of observed actions, \FAMA extends the degree of observability to the action sequence. This way, we can define a percentage of observability of the actions up to a null observability. The minimal observability case acceptable by \FAMA is when the algorithm is only fed with the input and output of a plan trace, namely the initial and final state of a planning task. Like many Machine Learning (ML) techniques, \FAMA is able to operate with only correct input/ouput pairs and an unknown or a partially known model of the agent. Unlike ML algorithms, \FAMA requires a symbolic structured representation of the input knowledge. In this sense, recent investigations tackle the problem of learning symbolic representations from low-level sensing information and unstructured data~\cite{KonidarisKL18,AsaiF18}.


\FAMA is a solving schema, based on AI planning technology, that automatically compiles the task of learning \strips\ actions into a planning task which is then solved with a planner~\cite{aineto2018learning}. \FAMA is thereby a model-based approach that automatically builds its own planning model by logical inference from the input observations. A solution to this planning task is a sequence of actions that determines the preconditions and effects of the \strips\ action models. The construct of the planning model starts out with the input observations of the agent's behaviour that, as commented above, may comprise none of the actions executed by the agent. \FAMA can thus be regarded as a solving process that generates an instance of a planning model, and whose solution must be \emph{compliant} with the input plan traces. This behaviour largely differs from ML techniques, which aim to minimize an error function on the training data. Moreover, \FAMA requires far less sample data (input observations) than ML algorithms, thus alleviating the dependency on the assumption that there are enough data for learning the action models~\cite{Zhuo15}.


Like other learning approaches, \FAMA also accepts as an input an initial action model of the agent's behaviour, either complete or partially specified~\cite{ZhuoNK13,ZhuoK17}. In this case, the output model of \FAMA is compliant with both the input model and with the observations. Following this behaviour, we can define a distance measure between the two models which yields an assessment of the accuracy with which the input model explains the observations. In other words, the distance measure provides a mechanism to explicitly correct or adjust a model to the observations. By using this distance-based concept, \FAMA contributes with two novel semantic evaluation metrics that are generally more informative than counting the number of errors between two models. Moreover, this new evaluation method opens up a way towards model reconciliation~\cite{KulkarniCZVZK16}.

All in all, \FAMA is a planning-based solving scheme that outputs a \strips\ action model using an automatically built planning model from minimal input knowledge. Unlike extensive data-ML approaches, \FAMA only requires a small amount of input plan traces. Unlike most relevant action-model learning algorithms, \FAMA does not require the traces to contain any observed action executed by the agent. Ultimately, \FAMA is a model-based planning approach that adopts certain ML-like characteristics, like the ability to learn only from data that do not explicitly exhibit the agent¡s behaviour or the capacity of correcting an initially given action model.



Motivated by recent advances on the synthesis of different kinds of generative models with classical planning~\cite{bonet2009automatic,segovia2016generalized,segovia2016hierarchical,segovia2017generating}, this paper presents \FAMA, an innovative approach for learning \strips\ action models defined as a classical planning compilation. A first description of the compilation scheme previously appeared in our previous conference paper~\cite{aineto2018learning}. \FAMA brings the following contributions over the first version of the compilation:

\begin{itemize}
\item A unified formulation for learning and evaluating action models from observations of plan executions. In the case of minimal observability, these execution only comprise the initial and final state of the plan traces.
\item A thorough elaboration of two semantic evaluation metrics that build upon two well-known syntax-based ML metrics, {\em precision} and {\em recall}~\cite{davis2006relationship}, to evaluate the output action models with respect to observations of plan executions.
\item An exhaustive empirical evaluation over 14 domains from the International Planning Competitions (IPCs). We include an analysis of the impact of the size of the input knowledge in the accuracy and performance of \FAMA, a comparison with \ARMS and a detailed experimentation when \FAMA is executed with minimal input knowledge.
\end{itemize}


The paper is organized as follows. Section~\ref{sec:background} introduces classical planning concepts and reviews related work on learning planning action models. Section~\ref{sec:motivation} motivates our compilation-to-planning approach for learning action models. Section~\ref{sec:learning} formalizes the learning task and presents the compilation scheme, the core of \FAMA. Sections~\ref{sec:evaluation} explains the evaluation of a learned model with respect to a reference model (syntactic evaluation) and with respect to a set of plan traces (semantic evaluation). Section~\ref{sec:experiments} reports the results of the experimental evaluation and, finally, Section~\ref{sec:conclusions} discusses the strengths and weaknesses of the compilation approach and proposes several opportunities for future research.








