
\section{Introduction}
\label{sec:introduction}

There is common agreement in the planning community that the unavailability of an \textcolor[rgb]{1.00,0.00,0.00}{adequate} domain model is a bottleneck in the applicability of planning technology to many real-world domains~\cite{kambhampati:modellite:AAAI2007}. Motivated by the difficulty and cost of crafting action models, research in action-model learning has seen huge advances. Since the emergence of pioneer learning systems like ARMS~\cite{yang2007learning}, we have seen systems able to learn action models with quantifiers~\cite{AmirC08,ZhuoYHL10}, from noisy actions or noisy states~\cite{zhuo2013action,MouraoZPS12}, from null state information~\cite{cresswell2013acquiring}, from incomplete domain models~\cite{ZhuoNK13,ZhuoK17} and many more.

A system for learning planning action models receives as an input observations of the agent's plan execution and \textcolor[rgb]{1.00,0.00,0.00}{outputs an approximation of the actions that embody the physics of the real-world domain being modeled.} The primary underlying motivation for acquiring planning action models is to solve model-based planning tasks afterwards, \textcolor[rgb]{1.00,0.00,0.00}{but there exists as well a large variety of planning-related tasks that rely upon the existence of a planning model. Among these tasks, we might cite: \emph{goal and plan recognition} approaches based on a domain theory~\cite{ramirez2009plan,ramirez2012plan,SohrabiRU16}; \emph{transparent planning}, in which an agent implicitly communicates its true goal by making its intentions and its action selection transparent (recognizable) to observers~\cite{MacNallyLRP18}; or \emph{deceptive path-planning}, which draws on the definition of path-planning domains and aims at finding a path such that the probability of an observer identifying the final destination is minimised~\cite{MastersS17}. Planning models are also used in \emph{explainable AI planning} to form a common basis for communicating with users and facilitate the generation of transparent and explainable decisions~\cite{FoxLM17} as well as explanations in terms of the differences with a human mental model~\cite{ChakrabortiSK18}. \emph{Counterplanning} requires a model of the opponent agent in order to recognize its goals \cite{PozancoEFB18} and \emph{model reconciliation} aims to conform the models of two agents with respect to an observation of a plan computed with one of the two models~\cite{ChakrabortiSZK17}.}

\textcolor[rgb]{1.00,0.00,0.00}{Motivated by the requirement for a planning model in many different tasks and the recent advances on the use of classical planning for the generation of different types of planning models (regular automata, context-free grammars, finite-state machines, \strips)} \cite{bonet2009automatic,segovia2016generalized,segovia2016hierarchical,segovia2017generating}, in this paper we claim that a planning model is learnable even though an accurate representation of the agent's behavior is not available. Particularly, we present a novel learning algorithm, called \FAMA, capable of inferring the preconditions and effects of \strips\ action models, the vanilla action model for automated planning~\cite{fikes1971strips}, under minimal observability.


%Up to date, all existing learning approaches assume that a given input plan trace contains a fully observed sequence of the executed actions. This heavily restricts the applicability of the learning approach to contexts in which the behaviour of the agent is fully observable and a human annotator correctly labels the executed actions.

\textcolor[rgb]{1.00,0.00,0.00}{Current learning approaches assume that the observation of the agent's plan execution (plan trace) encompasses the fully observed sequence of the executed actions; i.e, they assume all the actions performed by the agent are observable. This heavily restricts the applicability of the learning approaches to contexts where the behaviour of the agent is fully observable, which also commonly entails a human annotator that correctly labels the executed actions. On the other hand, learning approaches accept a varying degree of observability in the states traversed in the plan trace, ranging from fully observable to fully unobservable states (see section \ref{related_work} for details).} In contrast, \FAMA allows for an incomplete or empty sequence of observable actions, and the minimum observability case acceptable by \FAMA is when the algorithm is only fed with the initial and final state of a plan trace. Like many Machine Learning (ML) techniques, \FAMA is able to operate with only input/output pairs of states and an unknown or a partially known model of the agent. Unlike ML algorithms, \FAMA requires a symbolic structured representation of the input knowledge. In this sense, recent investigations tackle the problem of learning symbolic representations from low-level sensing information and unstructured data~\cite{KonidarisKL18,AsaiF18}.

\textcolor{red}{\FAMA is a new learning approach, based on AI planning technology, that automatically compiles the task of learning \strips\ actions into a planning task which is then solved with a planner~\cite{aineto2018learning}. The construction of a \strips\ planning model starts out from a set of plan traces containing the observation of several plan executions. As mentioned above, a plan trace may comprise none of the actions executed by the agent but must include, at least, the initial state $s_0$ and final state $s_f$ of the execution. The compilation scheme lies in defining a planning task from the set of plan traces using a set of \emph{building actions} that insert the preconditions and effects of a learned action, and a set of \emph{validating actions} that validate the learned actions in the plan traces. Hence, a solution to this planning task is a plan that determines the preconditions and effects of the actions of a \strips\ planning model $M$ while ensuring consistency with the input traces. We say that a model $M$ is \emph{consistent} with a plan trace when $M$ can produce a solution $\pi$ to the planning problem $\tup{s_0,s_f}$ so that: (1) $\pi$ contains the observed actions of the plan trace, if any, and (2) the states generated by the application of $\pi$ to $s_0$ will encompass all the (possibly) partially observed states of the trace.}

\FAMA is thus a model-based approach that automatically builds its own planning model by logical inference from the input plan traces that contain the observations of the agent execution. This behaviour largely differs from ML techniques, which aim to minimize an error function on the training data. Moreover, \FAMA requires far less sample data (example plan traces) than typical ML algorithms, thus alleviating the dependency on the assumption that there are enough data for learning the action models~\cite{Zhuo15}.

A key aspect in action-model learning is the evaluation method to assess the quality and performance of the learning approach. The most common method is to use a syntax-based evaluation that compares the learned model with a reference model. \FAMA proposes instead two novel semantic evaluation metrics that build upon two well-known ML metrics, {\em precision} and {\em recall}~\cite{davis2006relationship}, to evaluate the learned action models with respect to observations of plan executions. Our semantic evaluation is generally more informative than counting the number of errors between two models and alleviates two important limitations of a purely syntax-based assessment: (a) that the learned model is syntactically different from the reference model but semantically correct and (b) that the learned model comprises correct though unnecessary preconditions in regards to the reference model. This latter issue is concerned with the qualification problem, which is defined as the actual impossibility of listing all the preconditions required for a real world action to have its intended effects~\cite{GinsbergS88}.

Our semantic evaluation method is built on the same compilation scheme for solving a learning task. In particular, \FAMA also accepts an input initial action model $M$ of the agent's behaviour, either complete or partially specified~\cite{ZhuoNK13,ZhuoK17}, alongside the observation of the agent execution. In this case, \FAMA returns a model $M'$ that follows the input model $M$ and is consistent with the observations. We designed an {\em edition} mechanism that serves to correct the input model to the output model, which in turn defines an assessment of the accuracy with which $M$ explains the observations. Interpreting the edition measure as a distance-based concept between two models can also be exploitable in model reconciliation~\cite{KulkarniCZVZK16}.


%Like other learning approaches, \FAMA also accepts as an input an initial action model of the agent's behaviour, either complete or partially specified~\cite{ZhuoNK13,ZhuoK17}. In this case, the output model of \FAMA is compliant with both the input model and with the observations. Following this behaviour, we can define a distance measure between the two models which yields an assessment of the accuracy with which the input model explains the observations. In other words, the distance measure provides a mechanism to explicitly correct or adjust a model to the observations. By using this distance-based concept, \FAMA contributes with two novel semantic evaluation metrics that are generally more informative than counting the number of errors between two models. Moreover, this new evaluation method opens up a way towards model reconciliation~\cite{KulkarniCZVZK16}.

%All in all, \FAMA is a planning-based solving scheme that outputs a \strips\ action model using a planning model that is automatically built from minimal input knowledge. Unlike extensive-data ML approaches, \FAMA only requires a small amount of input plan traces. Unlike most relevant action-model learning algorithms, \FAMA does not require the traces to contain any observed action executed by the agent.

\textcolor[rgb]{1.00,0.00,0.00}{In summary, \FAMA is a novel learning approach characterized by:
\begin{itemize}
\item compiling the task of learning a \strips\ planning model into a planning task that is automatically built from a set of input plan traces.
\item the plan traces are correct (no noise is considered in the observations) but may be incomplete in the number of observed actions as well as in the number and contents of the observed states.
\item the planning task resulting from the compilation comprises actions for programming the preconditions and effects of the \strips\ actions and actions for validating the learned \strips\ actions.
\item a semantic evaluation proposal that enables to assess a learned model beyond a merely syntactic comparison to a reference model.
\end{itemize} }

A first description of the \FAMA compilation scheme already appeared in our previous conference paper~\cite{aineto2018learning}. This paper brings the following contributions over the first version of the compilation:

\begin{itemize}
\item A unified formulation for learning and evaluating action models from observations of plan executions. In the case of minimum observability, these executions only comprise the initial and final state of the plan traces.
\item A thorough elaboration of two semantic evaluation metrics that build upon the notions of {\em precision} and {\em recall} to evaluate the output action models with respect to observations of plan executions.
\item An exhaustive empirical evaluation over 15 domains from the International Planning Competitions (IPCs). We include an analysis of the impact that the size of the input knowledge has in the performance of \FAMA, a comparison with \ARMS, and a detailed experimentation when \FAMA is executed with minimal input knowledge.
\end{itemize}


The paper is organized as follows. Section~\ref{sec:background} introduces classical planning concepts and reviews related work on learning planning action models. \textcolor{red}{Section~\ref{task_definition} formally defines the learning task and motivates our compilation-to-planning approach for learning action models.} Section~\ref{sec:learning} presents the compilation scheme, the core of \FAMA. Sections~\ref{sec:evaluation} explains the evaluation of a learned model with respect to a reference model (syntactic evaluation) and with respect to a set of plan traces (semantic evaluation). Section~\ref{sec:experiments} reports the results of the experimental evaluation and, finally, Section~\ref{sec:conclusions} discusses the strengths and weaknesses of the compilation approach and proposes several opportunities for future research.








