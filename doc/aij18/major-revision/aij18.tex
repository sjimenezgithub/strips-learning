% Template for Elsevier CRC journal article
% version 1.1 dated 16 March 2010

% This file (c) 2010 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science
% but may easily be adapted to other journals

% Changes since version 1.0
% - elsarticle class option changed from 1p to 3p (to better reflect CRC layout)

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at http://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Important note on usage                  %%
%% -----------------------                  %%
%% This file must be compiled with PDFLaTeX %%
%% Using standard LaTeX will not work!      %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '3p' and 'times' class options of elsarticle are used for Elsevier CRC
\documentclass[3p,times]{elsarticle}

%% The `ecrc' package must be called to make the CRC functionality available
\usepackage{ecrc}
%% The ecrc package defines commands needed for running heads and logos.
%% For running heads, you can set the journal name, the volume, the starting page and the authors

%% set the volume if you know. Otherwise `00'
\volume{00}

%% set the starting page if not 1
\firstpage{1}

%% Give the name of the journal
\journalname{Artificial Intelligence}

%% Give the author list to appear in the running head
%% Example \runauth{C.V. Radhakrishnan et al.}
\runauth{}

%% The choice of journal logo is determined by the \jid and \jnltitlelogo commands.
%% A user-supplied logo with the name <\jid>logo.pdf will be inserted if present.
%% e.g. if \jid{yspmi} the system will look for a file yspmilogo.pdf
%% Otherwise the content of \jnltitlelogo will be set between horizontal lines as a default logo

%% Give the abbreviation of the Journal.
\jid{procs}

%% Give a short journal name for the dummy logo (if needed)
\jnltitlelogo{Artificial Intelligence}

%% Hereafter the template follows `elsarticle'.
%% For more details see the existing template files elsarticle-template-harv.tex and elsarticle-template-num.tex.

%% Elsevier CRC generally uses a numbered reference style
%% For this, the conventions of elsarticle-template-num.tex should be followed (included below)
%% If using BibTeX, use the style file elsarticle-num.bst

%% End of ecrc-specific commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%%\usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
\usepackage[figuresright]{rotating}

\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{multirow}
\usepackage{listings}
\usepackage{xspace}


\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{calc,backgrounds,positioning,fit}
\usepackage{subcaption}
\usetikzlibrary{arrows,automata}
\usepackage{arydshln}



% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

\newtheorem{mytheorem}{Theorem}
\newtheorem{mylemma}[mytheorem]{Lemma}
\newtheorem{mydefinition}[mytheorem]{Definition}
\newtheorem{myproposition}[mytheorem]{Proposition}
\newtheorem{myconstruction}{Construction}


\mathchardef\mh="2D
\newcommand{\pre}{\mathsf{pre}}  % precondition
\newcommand{\eff}{\mathsf{eff}}  % effect
\newcommand{\cond}{\mathsf{cond}}   % conditional effect
\newcommand{\add}{\mathsf{add}}  % add effect
\newcommand{\del}{\mathsf{del}}  % delete effect
\newcommand{\PE}{\mathrm{PE}}     % precondition
\newcommand{\PSPACE}{\mathrm{PSPACE}}     % precondition
\newcommand{\NPSPACE}{\mathrm{NPSPACE}}     % precondition
\newcommand{\strips}{\textsc{Strips}}     % precondition


\newcommand{\ARMS}{{\small {\sffamily ARMS}}\xspace}
\newcommand{\CAMA}{{\small {\sffamily CAMA}}\xspace}
\newcommand{\SLAF}{{\small {\sffamily SLAF}}\xspace}
\newcommand{\LAMP}{{\small {\sffamily LAMP}}\xspace}
\newcommand{\NOISTA}{{\small {\sffamily NOISTA}}\xspace}
\newcommand{\LOCM}{{\small {\sffamily LOCM}}\xspace}
\newcommand{\LOCMtwo}{{\small {\sffamily LOCM2}}\xspace}
\newcommand{\LOP}{{\small {\sffamily LOP}}\xspace}
\newcommand{\AMAN}{{\small {\sffamily AMAN}}\xspace}
\newcommand{\FAMA}{{\small {\sffamily FAMA}}\xspace}

\newcommand{\FO}{{\small {\sffamily FO}}\xspace}
\newcommand{\PO}{{\small {\sffamily PO}}\xspace}
\newcommand{\POstar}{{\small {\sffamily PO*}}\xspace}
\newcommand{\NO}{{\small {\sffamily NO}}\xspace}

\newcommand{\pbox}[1]{\makebox[2em][l]{#1}}

\newcommand{\tup}[1]{{\langle #1 \rangle}}

\lstset{
  basicstyle=\ttfamily,
  mathescape
}

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\dochead{}
%% Use \dochead if there is an article header, e.g. \dochead{Short communication}

\title{Learning action models with minimal observability}
\author[label1]{Diego Aineto}
\author[label1]{Sergio Jim\'{e}nez Celorrio}
\author[label1]{Eva Onaindia}
\address[label1]{Department of Computer Systems and Computation, Universitat Polit\`ecnica de Val\`encia. Spain}


%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}


\begin{abstract}
  This paper presents \FAMA, a novel approach for learning \strips\ action models from observations of plan executions that compiles the learning task into a classical planning task. Unlike all existing learning systems, \FAMA is able to learn when the actions of the plan executions are partially or totally unobservable and information on intermediate states is partially provided. This flexibility makes \FAMA an ideal learning approach in domains where only sensoring data are accessible. Additionally, we leverage the compilation scheme and extend it to come up with an evaluation method that allows us to assess the quality of a learned model syntactically, that is, with respect to the actual model; and, semantically, that is, with respect to a set of observations of plan executions. We also show that the extended compilation scheme can be used to lay the foundations of a framework for action model comparison. \FAMA is exhaustively evaluated over a wide range of IPC domains and its performance is compared to \ARMS, a state-of-the-art benchmark in action model learning.
\end{abstract}

\begin{keyword}
Action model learning\sep AI planning\sep Machine Learning
%% keywords here, in the form: keyword \sep keyword

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text

%% The Appendices part is started with the command \appendix;
%% appendix sections are then done as normal sections
%% \appendix

%% \section{}
%% \label{}

% HLP: Expressiveness is pushed when pure compilations are used. Otherwise we just use them.


\input{introduction}
\input{background}
\input{motivation}
\input{learning_task}
\input{evaluation}
\input{experiments}


\section{Conclusions}
\label{sec:conclusions}


In this paper we have presented \FAMA, an approach for learning \strips\ action models based on the compilation of the learning task into a planning task. The distinctive characteristic of \FAMA over other state-of-the-art approaches is its ability to learn from minimal input knowledge and, more particularly, from minimal observability plan traces with no observed actions and no observed intermediate states. By lightening the input constraints, \FAMA opens up the way for action model learning to operate on real-world problems, as opposite to current approaches where the heavy input restrictions have limited their applicability to synthetic benchmarks. Our approach is thus very well suited for learning action models in data-based applications where the only observable information is a possibly incomplete sequence of partially observed states.

Besides its capacity of working with minimal observability, \FAMA is also able to learn from very small amounts of input knowledge, a clear advantage in domains where obtaining enough training samples is difficult or costly. While the experimental evaluation shows in general an exponential increase of the computation time as the number of training traces augments, \FAMA is able to learn action models more accurate than those of \ARMS with very limited input knowledge. Unlike extensive-data ML approaches, our work explores an alternative research direction that exploits logic reasoning to learn sound models from minimal input observability.

This great flexibility of \FAMA to different amounts of minimal input knowledge impacts the complexity of the planning tasks. When the length of the input plan traces is unknown, that is, we ignore the number of steps of the decision-making process of the agent, the planning task to be solved becomes a PSPACE-complete scenario. This is one of the reasons that justifies our compilation-to-planning scheme in contrast to the most extended SAT-solving scheme used in many existing learning systems. Additionally, our planning-based solving scheme allows us to leverage {\em off-the-shelf} classical planners and benefit from the multiple advances in classical planning research.

A key contribution of this work is the proposal of two novel metrics to semantically evaluate the learned action models. These two metrics build upon the well-known syntax-based metrics \emph{precision} and \emph{recall}. Our semantic evaluation mitigates the common issue known as \emph{reformulation} that appears when training sets of minimal observability are used. Due to the lack of observable information, \FAMA can learn semantically valid models that are syntactically different from the reference model. The application of the semantic-based precision and recall allows us to assess the validity of the learned models even in domains where a reference model is not available.
%Remarkably, the action models of the {\em gripper-strips}, {\em n-puzzle} and {\em grid-visit-all} domains were perfectly learned from only observations of the initial and final states.

We highlight that the semantic evaluation of \FAMA is done via the same compilation-to-planning scheme that we use to learn the action models. Given that the input of our learning task definition accepts an initial action model of the agent's behaviour, either complete or partially specified, this solving scheme is exploitable to computing a model that follows the initial model and is compliant with a test set of plan traces. In other words, \FAMA is applicable in model reconciliation tasks by defining a distance metric that measures how close the two models are~\cite{KulkarniCZVZK16}.

More importantly, \FAMA is applicable not only in environments where the domain model is unknown but also in environments where the executable actions are unknown as well. This ability  broadens the range of application to goal reasoning tasks such as plan recognition under imperfect observability \cite{SohrabiRU16}, planning for transparency~\cite{MacNallyLRP18} or counterplanning~\cite{PozancoEFB18}. The application of \FAMA to these tasks offers a plan-based solution where the assumption of a known domain model can be removed. In other words, \FAMA opens up the way towards domain-free goal reasoning.

Finally, we would like to add a note on the open issue of generating {\em informative} plan traces for learning planning action models. Planning actions include preconditions that are only satisfied by specific sequences of actions which have low probability of being chosen by chance~\cite{fern2004learning}. The success of recent algorithms for exploring planning tasks~\cite{FrancesRLG17} motivates the development of novel techniques that enable to autonomously collect informative learning examples. The combination of such exploration techniques with our learning approach is an appealing research direction towards the bootstrapping of planning action models.






%When action plans are not available, our approach still produces action models that are compliant with the input information. In this case, since learning is not constrained by actions, operators can be reformulated changing their semantics, in which case the comparison with a reference model turns out to be tricky.

%When example plans are available, our approach is also able to compute accurate action models from small sets of learning examples. Our experimental results report that \FAMA achieves {\em precision} and {\em recall} scores that are over 0.7 for ten out of the fourteen IPC domains, using only two plan traces (with ten actions each) and in little computation time (less than a second). Our experimental evaluation also shows that, in the particular setting of minimal input data, \FAMA outperforms \ARMS.

%On the other hand, this work also introduced a semantic version of the {\em precision} and {\em recall} metrics. These two metrics, widely used in ML, separately measure the soundness and completeness of the learned models, which facilitate the identification of model flaws. Our brand new semantic version of {\em precision} and {\em recall} do not require a GTM and evaluate \strips\ action models with respect to a given set of observations. In addition, when no reformulation occurs, these new metrics behave similarly to their syntactic counterparts.





\subsection*{Acknowledgment}
This work is supported by the Spanish MINECO project TIN2017-88476-C2-1-R. Diego Aineto is partially supported by the {\it FPU16/03184} and Sergio Jim\'enez by the {\it RYC15/18009}, both programs funded by the Spanish government.


%% References
%%
%% Following citation commands can be used in the body text:
%% Usage of \cite is as follows:
%%   \cite{key}         ==>>  [#]
%%   \cite[chap. 2]{key} ==>> [#, chap. 2]
%%

%% References with BibTeX database:

\bibliographystyle{elsarticle-num}
\bibliography{planlearnbibliography}

\input{appendix}

%% Authors are advised to use a BibTeX database file for their reference list.
%% The provided style file elsarticle-num.bst formats references in the required Procedia style

%% For references without a BibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}



\end{document}

%%
%% End of file `ecrc-template.tex'.
